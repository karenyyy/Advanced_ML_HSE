{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week3 Notes: Embedding Methods Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/16.png)\n",
    "\n",
    "\n",
    "回顾过去基于深度学习的 NLP 任务可以发现，几乎绝大多数都比较符合这三层概念。比如很多生成任务的 Seq2Seq 框架中不外乎都有一个 Encoder 和一个 Decoder。对应到这里，__Decoder 更像是一个 Task-specific Model，然后相应的将 Encoder 做一些细微调整，比如引入 Attention 机制等等__\n",
    "\n",
    "- Eg:\n",
    "    - 对于一些文本分类任务的结构，则 Encoder 模块与 Task-specific Model 模块的区分更为明显和清晰，Encoder 层可以作为一个相对比较通用的模块来使用, Encoder 负责提取文本特征，最后接上一些全连接层和 Softmax 层便可以当做 Task-specific Model 模块，便完成了一个文本分类任务\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVD+LSA -> PLSA -> LDA (introduce prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/1.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/2.png)\n",
    "\n",
    "\n",
    "### Here:\n",
    "\n",
    "$$PPMI = \\log (lift(or: interest)) = \\log \\frac{p(x, y)}{p(x)p(y)}$$\n",
    "\n",
    "![](../../images/week3/3.png)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/9.png)\n",
    "\n",
    "![](../../images/week3/10.png)\n",
    "\n",
    "![](../../images/week3/11.png)\n",
    "\n",
    "![](../../images/week3/12.png)\n",
    "\n",
    "![](../../images/week3/13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/17.png)\n",
    "\n",
    "![](../../images/week3/21.png)\n",
    "\n",
    "\n",
    "### NNLM\n",
    "\n",
    "![](../../images/week3/18.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/24.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### CBOW\n",
    "\n",
    "\n",
    "![](../../images/week3/19.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/26.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SKIP-GRAM\n",
    "\n",
    "\n",
    "![](../../images/week3/20.png)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/25.png)\n",
    "\n",
    "\n",
    "\n",
    "### Expansion of word2vec\n",
    "\n",
    "#### PV-DM (CBOW of paragraphs) &  PV-DBOW (Skip-Gram of paragraphs) \n",
    "\n",
    "\n",
    "![](../../images/week3/22.png)\n",
    "\n",
    "\n",
    "### Skip-Thoughts\n",
    "\n",
    "Skip-thoughts 直接在句子间进行预测，也就是将 Skip-gram 中以词为基本单位，替换成了以句子为基本单位，具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句\n",
    "\n",
    "\n",
    "对于句子的建模利用的 RNN 的 sequence 结构，预测上一个和下一个句子时候，也是利用的一个 sequence 的 RNN 来生成句子中的每一个词，所以这个结构本质上就是一个 Encoder-Decoder 框架，只不过和普通框架不一样的是，Skip-thoughts 有两个 Decoder\n",
    "\n",
    "- future works:\n",
    "    - 输入的 Encoder 可以引入 attention 机制, 从而让 Decoder 的输入不再只是依赖 Encoder 最后一个时刻的输出\n",
    "    - Encoder 和 Decoder 可以利用更深层的结构\n",
    "    - Decoder 也可以继续扩大，可以预测上下文中更多的句子\n",
    "    - RNN 也不是唯一的选择，诸如 CNN 以及 2017 年谷歌提出的 Transformer 结构也可以利用进来\n",
    "\n",
    "- major drawbacks\n",
    "    - Skip-thoughts 的 Decoder 效率太低\n",
    "    - 无法在大规模语料上很好的训练\n",
    "    \n",
    "    \n",
    "### Quick-Thoughts\n",
    "\n",
    "Skip-thoughts 的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例，并将这些句子对输入模型，让模型判断这些句子对是否是同一个上下文窗口中，很明显，这是一个分类任务\n",
    "    \n",
    "    \n",
    "![](../../images/week3/23.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/4.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/5.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/6.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/7.png)\n",
    "\n",
    "![](../../images/week3/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### GloVe Paper Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "![](../../images/week3/14.png)\n",
    "\n",
    "![](../../images/week3/15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CoVe (Contextualized Word Vectors, outputs of the MT-LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Learned in Translation: Contextualized Word Vectors (notes later)] 论文首先用一个 Encoder-Decoder 框架在机器翻译的训练语料上进行预训练，而后用训练好的模型，只取其中的 Embedding 层和 Encoder 层，同时在一个新的任务上设计一个 task-specific 模型，再将原先预训练好的 Embedding 层和 Encoder 层的输出作为这个 task-specific 模型的输入，最终在新的任务场景下进行训练.\n",
    "\n",
    "\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "![](../../images/week3/28.png)\n",
    "![](../../images/week3/29.png)\n",
    "\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "![](../../images/week3/30.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Attention Mechanism\n",
    "\n",
    "为了决定下一步翻译英语句子中的哪一部分，注意力机制需要从隐向量向前回溯。它使用状态向量来判别每一个隐向量的重要性，为了记录它的观察值，注意力机制会生成一个新的向量，我们可以称之为语境调整状态（context-sdjusted state）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/31.png)\n",
    "\n",
    "然后，生成器会根据语境调整状态来决定要生成哪个单词，接下来语境调整状态会回传到解码器中，让解码器对其翻译的结果有一个准确的感知。解码器一直重复这个过程，直至它完成所有翻译。这就是一个标准的基于注意力机制的编码器-解码器结构，它被用来学习像机器翻译一样的序列到序列任务。\n",
    "![](../../images/week3/32.png)\n",
    "\n",
    "当训练过程结束之后，将训练好的 LSTM 提取出来作为编码器用于机器翻译。我们将这个预训练的 LSTM 称作机器翻译 LSTM（MT-LSTM），并使用它生成新句子的隐向量。当我们把这些机器翻译隐向量用于其它的自然语言处理模型时，我们就把它们称作__语境向量(CoVe)__(CoVe 可以被用在任何将向量序列作为输入的模型中)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/27.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### CoVe Paper Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/33.png)\n",
    "\n",
    "![](../../images/week3/34.png)\n",
    "\n",
    "![](../../images/week3/35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### CoVe Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seq2seq rnn-based model (without attention):\n",
    "\n",
    "![](../../images/week3/36.jpg)\n",
    "![](../../images/week3/40.jpg)\n",
    "\n",
    "\n",
    "\n",
    "- with attention\n",
    "\n",
    "![](../../images/week3/41.jpg)\n",
    "\n",
    "在该模型中，定义了一个条件概率：\n",
    "\n",
    "![](../../images/week3/42.jpg)\n",
    "\n",
    "其中，$s_i$是decoder中RNN在在i时刻的隐状态\n",
    "\n",
    "![](../../images/week3/43.jpg)\n",
    "\n",
    "背景向量ci的计算方式，与传统的Seq2Seq模型直接累加的计算方式不一样，这里的ci是一个权重化（Weighted）之后的值:\n",
    "\n",
    "![](../../images/week3/44.jpg)\n",
    "\n",
    "$h_j$ 表示encoder端的第j个词的隐向量，$a_{ij}$表示encoder端的第j个词与decoder端的第i个词之间的权值，表示源端第j个词对目标端第i个词的影响程度\n",
    "\n",
    "$a_{ij}$的计算公式:\n",
    "\n",
    "![](../../images/week3/45.jpg)\n",
    "\n",
    "$e_{ij}$ 表示一个对齐模型，用于衡量encoder端的位置j个词，对于decoder端的位置i个词的对齐程度（影响程度）(i.e. decoder端生成位置i的词时，有多少程度受encoder端的位置j的词影响)\n",
    "\n",
    "对齐模型eij的计算方式有很多种，不同的计算方式，代表不同的Attention模型，最简单且最常用的的对齐模型是dot product乘积矩阵，即把target端的输出隐状态ht与source端的输出隐状态进行矩阵乘\n",
    "\n",
    "![](../../images/week3/46.jpg)\n",
    "\n",
    "\n",
    "![](../../images/week3/37.jpg)\n",
    "\n",
    "\n",
    "![](../../images/week3/38.jpg)\n",
    "\n",
    "\n",
    "权重 $\\alpha$ 是怎么来的呢？常见有三种方法：\n",
    "\n",
    "\n",
    "- $\\alpha_{0}^1=cos\\_sim(z_0, h_1)$\n",
    "- $\\alpha_0 =neural\\_network(z_0, h)$\n",
    "- $\\alpha_0 = h^TWz_0$\n",
    "\n",
    "\n",
    "思想就是根据当前解码“状态”判断输入序列的权重分布\n",
    "\n",
    "\n",
    "attention其实是以下的机制:\n",
    "\n",
    "![](../../images/week3/39.jpg)\n",
    "\n",
    "模型通过Q和K的匹配计算出权重，再结合V得到输出：\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(sim(Q, K))V$$\n",
    "\n",
    "\n",
    "- 模型分类\n",
    "    - Soft/Hard Attention\n",
    "        - soft attention：传统attention，可被嵌入到模型中去进行训练并传播梯度\n",
    "        - hard attention：不计算所有输出，依据概率对encoder的输出采样，在反向传播时需采用蒙特卡洛进行梯度估计\n",
    "    - Global/Local Attention\n",
    "        - global attention：传统attention，对所有encoder输出进行计算\n",
    "            - 传统的Attention model一样。所有的hidden state都被用于计算Context vector 的权重，即变长的对齐向量at，其长度等于encoder端输入句子的长度\n",
    "            ![](../../images/week3/47.jpg)\n",
    "            在t时刻，首先基于decoder的隐状态ht和源端的隐状态hs，计算一个变长的隐对齐权值向量$a_t$\n",
    "            ![](../../images/week3/48.jpg)\n",
    "            得到对齐向量$a_t$之后，就可以通过加权平均的方式，得到上下文向量$c_t$\n",
    "        - local attention：介于soft和hard之间，会预测一个位置并选取一个窗口进行计算\n",
    "            - ![](../../images/week3/49.jpg)\n",
    "            - Local Attention首先会为decoder端当前的词，预测一个source端对齐位置（aligned position）$p_t$，然后基于$p_t$选择一个窗口，用于计算背景向量$c_t$\n",
    "            ![](../../images/week3/50.jpg)\n",
    "            - S是encoder端句子长度，vp和wp是模型参数, 此时，对齐向量at的计算公式:\n",
    "            ![](../../images/week3/51.jpg)\n",
    "    - Self Attention\n",
    "        - ![](../../images/week3/50.png)\n",
    "        - 传统attention是计算Q和K之间的依赖关系，__而self attention则分别计算Q和K自身的依赖关系__\n",
    "        - Self Attention 分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系\n",
    "        - Self Attention 的具体计算方式如图所示:\n",
    "        ![](../../images/week3/52.jpg)\n",
    "       - 从All Attention的结构示意图可以发现，Encoder和decoder是层叠多了类似的__Multi-Head Attention__单元构成，而每一个Multi-Head Attention单元由多个结构相似的__Scaled Dot-Product Attention__单元组成\n",
    "        ![](../../images/week3/53.jpg)\n",
    "        - Self Attention也是在Scaled Dot-Product Attention单元里面实现的\n",
    "            - 首先把输入Input经过线性变换分别得到Q、K、V\n",
    "            - 然后把Q和K做dot Product相乘，得到输入Input词与词之间的依赖关系\n",
    "            - 然后经过尺度变换（scale）、掩码（mask）和softmax操作，得到最终的Self Attention矩阵 (尺度变换是为了防止输入值过大导致训练不稳定，mask则是为了保证时间的先后关系)\n",
    "            - 最后，把encoder端self Attention计算的结果加入到decoder做为k和V，结合decoder自身的输出做为q，得到encoder端的attention与decoder端attention之间的依赖关系\n",
    "    - Other Attention\n",
    "        - __Hierarchical Attention__构建了两个层次的Attention Mechanism，第一个层次是对句子中每个词的attention，即word attention；第二个层次是针对文档中每个句子的attention，即sentence attention\n",
    "         ![](../../images/week3/54.jpg)\n",
    "        - __Attention over Attention__\n",
    "            - 两个输入，一个Document和一个Query，分别用一个双向的RNN进行特征抽取，得到各自的隐状态h（doc）和h（query)\n",
    "            - 然后基于query和doc的隐状态进行dot product，得到query和doc的attention关联矩阵\n",
    "            - 然后按列（column）方向进行softmax操作，得到query-to-document的attention 值a（t）\n",
    "            - 按照行（row）方向进行softmax操作，得到document-to-query的attention值b（t）\n",
    "            - 再进行attention操作，即attention over attention得到最终query与document的关联矩阵\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Seq2Seq Attention TF source code (from scratch later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__tf.nn.seq2seq__文件共实现了5个seq2seq函数\n",
    "\n",
    "- __1. basic_rnn_seq2seq__: 最简单版本\n",
    "    - _输入和输出都是embedding的形式_;\n",
    "    - 最后一步的state vector作为decoder的initial state;\n",
    "    - encoder和decoder用相同的RNN cell， 但不共享权值参数\n",
    "- __2. tied_rnn_seq2seq__: 同1，但是encoder和decoder共享权值参数\n",
    "- __3. embedding_rnn_seq2seq__: 同1，但输入和输出改为id的形式，函数会在内部创建分别用于encoder和decoder的embedding matrix\n",
    "- __4. embedding_tied_rnn_seq2seq__: 同2，但输入和输出改为id形式，函数会在内部创建分别用于encoder和decoder的embedding matrix\n",
    "- __5. embedding_attention_seq2seq__: 同3，但多了attention机制\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### tf.nn.seq2seq.embedding_attention_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# T代表time_steps, 时序长度\n",
    "def embedding_attention_seq2seq(encoder_inputs,  # [T, batch_size], int32 id tensor list\n",
    "                                decoder_inputs,  # [T, batch_size], int32 id tensor list\n",
    "                                cell,\n",
    "                                num_encoder_symbols,\n",
    "                                num_decoder_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,      # attention的抽头数量\n",
    "                                output_projection=None, #decoder的投影矩阵\n",
    "                                feed_previous=False,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Encoder\n",
    "\n",
    "- 创建了一个embedding matrix\n",
    "- 计算encoder的output和state\n",
    "- 生成attention states，用于计算attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = rnn_cell.EmbeddingWrapper(      \n",
    "                                        cell, \n",
    "                                        embedding_classes=num_encoder_symbols, # 编码的符号数，即词表大小\n",
    "                                        embedding_size=embedding_size) # 词向量的维度\n",
    "\n",
    "encoder_outputs, encoder_state = rnn.rnn(\n",
    "                                        encoder_cell, \n",
    "                                        encoder_inputs, \n",
    "                                        dtype=dtype) #  [T，batch_size，size]\n",
    "\n",
    "top_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]    # T * [batch_size, 1, size]\n",
    "\n",
    "attention_states = array_ops.concat(1, top_states) # [batch_size,T,size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "上面的EmbeddingWrapper, 是RNNCell的前面加一层embedding，作为encoder_cell, input就可以是word的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingWrapper(RNNCell):\n",
    "    def __init__(self, cell, embedding_classes, embedding_size, initializer=None):\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "      #生成embedding矩阵[embedding_classes,embedding_size]\n",
    "      #inputs: [batch_size, 1]\n",
    "      #return : (output, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decoder\n",
    "\n",
    "- 生成decoder的cell，通过OutputProjectionWrapper类对输入参数中的cell实例包装实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decoder.\n",
    "    output_size = None\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "      output_size = num_decoder_symbols\n",
    "    if isinstance(feed_previous, bool):\n",
    "      return embedding_attention_decoder(\n",
    "          ...\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "上面的OutputProjectionWrapper将输出映射成想要的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OutputProjectionWrapper(RNNCell):\n",
    "    def __init__(self, cell, output_size): # output_size:映射后的size\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "      #init 返回一个带output projection的 rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_attention_decoder(decoder_inputs,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                num_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_size=None,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                update_embedding_for_previous=True,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False):\n",
    "    # 第一步创建了解码用的embedding\n",
    "    embedding = variable_scope.get_variable(\"embedding\",\n",
    "                                            [num_symbols, embedding_size])\n",
    "\n",
    "    # 第二步创建了一个循环函数loop_function，用于将上一步的输出映射到词表空间，输出一个word embedding作为下一步的输入\n",
    "    loop_function = _extract_argmax_and_embed(\n",
    "        embedding, output_projection,\n",
    "        update_embedding_for_previous) if feed_previous else None\n",
    "    emb_inp = [\n",
    "        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n",
    "    # T * [batch_size, embedding_size]\n",
    "    return attention_decoder(\n",
    "        emb_inp,\n",
    "        initial_state,\n",
    "        attention_states,\n",
    "        cell,\n",
    "        output_size=output_size,\n",
    "        num_heads=num_heads,\n",
    "        loop_function=loop_function,\n",
    "        initial_state_attention=initial_state_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs,    #T * [batch_size, input_size]\n",
    "                      initial_state,     #[batch_size, cell.states]\n",
    "                      attention_states,  #[batch_size, attn_length , attn_size]\n",
    "                      cell,\n",
    "                      output_size=None,\n",
    "                      num_heads=1,\n",
    "                      loop_function=None,\n",
    "                      dtype=None,\n",
    "                      scope=None,\n",
    "                      initial_state_attention=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__num_heads__:\n",
    "\n",
    "![](../../images/week3/37.png)\n",
    "\n",
    "attention就是对信息的加权求和，一个attention head对应了一种加权求和方式，这个参数定义了用多少个attention head去加权求和，所以公式三可以进一步表述为$\\sum^{num\\_heads}_{j=1}\\sum^{T_{A}}_{i=1}a_{i,j}h_{i}$\n",
    "\n",
    "- $W_{1}*h_{i}$用的是卷积的方式实现，返回的tensor的形状是[batch_size, attn_length, 1, attention_vec_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To calculate W1 * h_t we use a 1-by-1 convolution\n",
    "hidden = array_ops.reshape(attention_states, \n",
    "                           [-1, attn_length, 1, attn_size])\n",
    "hidden_features = []\n",
    "v = []\n",
    "attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "\n",
    "for a in xrange(num_heads):\n",
    "    k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "                                    [1, 1, attn_size, attention_vec_size])\n",
    "    hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "    v.append(\n",
    "          variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $W_{2}*d_{t}$，此项是通过下面的线性映射函数linear实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for a in xrange(num_heads):\n",
    "        with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "          # query对应当前隐层状态d_t\n",
    "          y = linear(query, attention_vec_size, True)\n",
    "          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "          # 计算u_t\n",
    "          s = math_ops.reduce_sum(\n",
    "              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "          a = nn_ops.softmax(s)\n",
    "          # 计算 attention-weighted vector d.\n",
    "          d = math_ops.reduce_sum(\n",
    "              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n",
    "              [1, 2])\n",
    "          ds.append(array_ops.reshape(d, [-1, attn_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### ELMo notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Goal: 一个预训练的词表示应该能够包含丰富的句法和语义信息，并且能够对多义词进行建模\n",
    "\n",
    "\n",
    "![](../../images/week3/55.jpg)\n",
    "\n",
    "\n",
    "ELMo 利用语言模型来获得一个上下文相关的预训练表示:\n",
    "\n",
    "![](../../images/week3/38.png)\n",
    "\n",
    "基本框架是一个双层的 Bi-LSTM，不过在第一层和第二层之间加入了一个残差结构（一般来说，残差结构能让训练过程更稳定)\n",
    "\n",
    "在 ELMo 中使用的是一个双向的 LSTM 语言模型，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然. ELMo 的基本框架是 2-stacked biLSTM + Residual 的结构, ELMo 的训练目标函数为:\n",
    "\n",
    "![](../../images/week3/39.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/56.jpg)\n",
    "\n",
    "\n",
    "在预训练好这个语言模型之后，ELMo 就是根据下面的公式来用作词表示，其实就是把这个双向语言模型的每一中间层进行一个求和:\n",
    "\n",
    "![](../../images/week3/57.jpg)\n",
    "\n",
    "__总结一下，不像传统的词向量，每一个词只对应一个词向量，ELMo 利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），再当成特征加入到具体的 NLP 有监督模型里__\n",
    "\n",
    "不过和普通 RNN 结构的不同之处在于，其主要改进在于输入层和输出层不再是 word，而是变为了一个 char-based CNN 结构，ELMo 在输入层和输出层考虑了使用同样的这种结构，该结构如下图示:\n",
    "\n",
    "![](../../images/week3/40.png)\n",
    "\n",
    "\n",
    "_Note^*:_\n",
    "\n",
    "输入层和输出层都使用了 CNN 结构\n",
    "\n",
    "\n",
    "在 CBOW 中的普通 Softmax 方法中，为了计算每个词的概率大小，使用的如下公式的计算方法:\n",
    "\n",
    "![](../../images/week3/41.png)\n",
    "\n",
    "现在我们假定 char-based CNN 模型是现成已有的，对于任意一个目标词都可以得到一个向量表示 CNN(tk) ，当前时刻的 LSTM 的输出向量为 h，那么便可以通过同样的方法得到目标词的概率大小:\n",
    "\n",
    "![](../../images/week3/42.png)\n",
    "\n",
    "\n",
    "这种先经过 CNN 得到词向量，然后再计算 Softmax 的方法叫做 CNN Softmax\n",
    "\n",
    "利用 CNN 解决有三点优势值得注意:\n",
    "\n",
    "-  CNN 能减少普通做 Softmax 时全连接层中的必须要有的 $|V|*h$ 的参数规模，只需保持 CNN 内部的参数大小即可。一般来说，CNN 中的参数规模都要比 $|V|*h$ 的参数规模小得多 (CNN Softmax 的好处就在于能够做到对于不同的词，映射参数都是共享的，这个共享便体现在使用的 CNN 中的参数都是同一套，从而大大减少参数的规模)\n",
    "\n",
    "- CNN 可以解决 OOV （Out-of-Vocabulary）问题，这个在翻译问题中尤其头疼\n",
    "\n",
    "- 在预测阶段，CNN 对于每一个词向量的计算可以预先做好，更能够减轻 inference 阶段的计算压力\n",
    "\n",
    "\n",
    "\n",
    "最终 ELMo 的主要结构便如下图（b）所示，可见输入层和输出层都是一个 CNN，中间使用 Bi-LSTM 框架:\n",
    "\n",
    "\n",
    "![](../../images/week3/43.png)\n",
    "\n",
    "$s_j$ 便是针对每一层的输出向量，利用一个 softmax 的参数来学习不同层的权值参数，因为不同任务需要的词语意义粒度也不一致，一般认为浅层的表征比较倾向于句法，而高层输出的向量比较倾向于语义信息。因此通过一个 softmax 的结构让任务自动去学习各层之间的权重\n",
    "\n",
    "![](../../images/week3/44.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/45.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### ELMo source code and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 将ELMo向量  $ELMo_k^{task}$ 与传统的词向量  $x_{k}$ 拼接成  $[x_{k};ELMo_k^{task}]$  后，输入到对应具体任务的RNN中。\n",
    "- 将ELMo向量放到模型输出部分，与具体任务RNN输出的  $h_{k}$ 拼接成 $[h_{k};ELMo_k^{task}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tf src code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Directly use by import from TF-Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/1'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-850bfcc5f106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Instantiate the elmo model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0melmo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mas_module_spec\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnative_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown module spec type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36mload_module_spec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mon\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mhandling\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompressed_module_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0mmodule_def_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_module_proto_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mmodule_def_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_def_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36mget_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \"\"\"\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       raise UnsupportedHandleError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_get_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    465\u001b[0m       raise UnsupportedHandleError(\n\u001b[1;32m    466\u001b[0m           self._create_unsupported_handle_error_msg(handle))\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_unsupported_handle_error_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36mget_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \"\"\"\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       raise UnsupportedHandleError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m_get_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0;32m--> 105\u001b[0;31m                                     self._lock_file_timeout_sec())\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lock_file_timeout_sec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0murl_opener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoggingHTTPRedirectHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_opener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_uncompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "# Instantiate the elmo model\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), \n",
    "                      signature=\"default\", \n",
    "                      as_dict=True)[\"default\"]\n",
    "\n",
    "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
    "embedding = layers.Lambda(ElmoEmbedding, output_shape=(1024,))(input_text)\n",
    "dense = layers.Dense(256, activation='relu')(embedding)\n",
    "pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ULMFiT是一种有效的NLP迁移学习方法，核心思想是通过精调预训练的语言模型完成其他NLP任务\n",
    "\n",
    "ULMFiT的过程分为三步:\n",
    "\n",
    "\n",
    "![](../../images/week3/47.png)\n",
    "![](../../images/week3/63.jpg)\n",
    "\n",
    "\n",
    "- ASGD(Averaged SGD):\n",
    "    - 是指先将模型训练到一定 epoch，然后再将其后的每一轮权值进行平均后，得到最终的权值\n",
    "    - 普通的 SGD 方法权值更新过程为：\n",
    "        - ![](../../images/week3/48.png)\n",
    "    - ASGD 则把它变成了:\n",
    "        - ![](../../images/week3/49.png)\n",
    "        - 其中 T 是一个阈值，而 K 是总共的迭代次数，把model迭代到第 T 次之后，对该参数在其后的第 T 轮到最后一轮之间的所有值求平均，\n",
    "\n",
    "\n",
    "- 两种fine-tuning方法：\n",
    "    - Discriminative fine-tuning\n",
    "        - 因为网络中不同层可以捕获不同类型的信息，因此在精调时也应该使用不同的learning rate。作者为每一层赋予一个学习率  $\\eta^{l}$ ，实验后发现，首先通过精调模型的最后一层L确定学习率  $\\eta^{L}$ ，再递推地选择上一层学习率进行精调的效果最好，递推公式为:  $\\eta^{l-1} =\\frac{ \\eta^{l}}{2.6}$\n",
    "    - Slanted triangular learning rates (STLR)\n",
    "        - 为了针对特定任务选择参数，理想情况下需要在训练开始时让参数快速收敛到一个合适的区域，之后进行精调。为了达到这种效果，作者提出STLR方法，即让LR在训练初期短暂递增，在之后下降。如上图的右上角所示\n",
    "        - ![](../../images/week3/46.png)\n",
    "        - parameters:\n",
    "            - T: number of training iterations\n",
    "            - cut_frac: fraction of iterations we increase the LR\n",
    "            - cut: the iteration when we switch from increasing to decreasing the LR\n",
    "            - p: the fraction of the number of iterations we have increased or will decrease the LR respectively\n",
    "            - ratio: specifies how much smaller the lowest LR $\\eta_{min}$ is from the max LR  $\\eta_{max}$\n",
    "            - $\\eta_{t}$  : the LR at iteration t\n",
    "            - $Note^*:$ in the paper, $cut\\_frac=1$, ration=32, $\\eta_{max} = 0.01$\n",
    "            \n",
    "            \n",
    "####  Target task classifier fine-tuning\n",
    "\n",
    "\n",
    "为了完成分类任务的精调，作者在最后一层添加了两个线性block:\n",
    "- 每个都有batch-norm和dropout\n",
    "- 使用ReLU作为中间层激活函数\n",
    "- 最后经过softmax输出分类的概率分布\n",
    "\n",
    "\n",
    "\n",
    "Details:\n",
    "\n",
    "- Concat pooling\n",
    "    - 第一个线性层的输入是最后一个隐层状态的池化。因为文本分类的关键信息可能在文本的任何地方，所以只是用最后时间步的输出是不够的。作者将最后时间步   $h_{T}$ 与尽可能多的时间步 $H= {h_{1},... , h_{T}}$ 池化后拼接起来，以  $h_{c} = [h_{T}, maxpool(H), meanpool(H)]$ 作为输入\n",
    "    \n",
    "- Gradual unfreezing\n",
    "    - 由于过度精调会导致模型遗忘之前预训练得到的信息，作者提出逐渐unfreeze网络层的方法，从最后一层开始unfreeze和精调，由后向前地unfreeze并精调所有层\n",
    "    \n",
    "- BPTT for Text Classification (BPT3C) \n",
    "    - 为了在large documents上进行模型精调，作者将文档分为固定长度为b的batches，并在每个batch训练时记录mean和max池化，梯度会被反向传播到对最终预测有贡献的batches\n",
    "    \n",
    "- Bidirectional language model\n",
    "    - 在paper中，分别独立地对前向和后向LM做了精调，并将两者的预测结果平均。两者结合后结果有0.5-0.7的提升。\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### GPT notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "OpenAI Transformer是一类可迁移到多种NLP任务的，基于Transformer的语言模型。它的基本思想同ULMFiT相同，都是在尽量不改变模型结构的情况下__将预训练的语言模型应用到各种任务__。不同的是，OpenAI Transformer主张用Transformer结构，而ULMFiT中使用的是基于RNN的语言模型\n",
    "\n",
    "\n",
    "![](../../images/week3/64.jpg)\n",
    "\n",
    "\n",
    "\n",
    "训练过程分为两步：\n",
    "\n",
    "- 1. Unsupervised pre-training\n",
    "\n",
    "__主要亮点在于利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构__\n",
    "\n",
    "\n",
    "![](../../images/week3/58.jpg)\n",
    "\n",
    "\n",
    "在具体 NLP 任务有监督微调时，与 ELMo 当成特征的做法不同，OpenAI GPT 不需要再重新对任务构建新的模型结构，而是直接在 Transformer 这个语言模型上的最后一层接上 softmax 作为任务输出层，然后再对这整个模型进行微调\n",
    "\n",
    "![](../../images/week3/59.jpg)\n",
    "\n",
    "- 2. Supervised fine-tuning\n",
    "\n",
    "\n",
    "有了预训练的语言模型之后，对于有标签的训练集 $C$ ，给定输入序列 $x^{1}, ..., x^{m}$ 和标签 $y$，可以通过语言模型得到  $h_{l}^{m}$ ，经过输出层后对  $y$  进行预测\n",
    "\n",
    "\n",
    "$$p(y|x^1, x^2, ..., x^m) = softmax(h_l^m W_y)$$\n",
    "\n",
    "\n",
    "$$L_2(C) = \\sum_{x,y} \\log p(y|x^1, x^2, ..., x^m)$$\n",
    "\n",
    "\n",
    "整个任务的目标函数为:\n",
    "\n",
    "$$L_3(C) = L_2(C)+ \\lambda L_1(C)$$\n",
    "\n",
    "Result:\n",
    "\n",
    "- 计算速度比循环神经网络更快，易于并行化\n",
    "- 实验结果显示Transformer的效果比ELMo和LSTM网络更好\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "\n",
    "从Wrod Embedding到OpenAI Transformer，NLP中的迁移学习__从最初使用word2vec、GLoVe进行字词的向量表示，到ELMo可以提供前几层的权重共享，再到ULMFiT和OpenAI Transformer的整个预训练模型的精调__，大大提高了NLP基本任务的效果。同时，多项研究也表明，以语言模型作为预训练模型，不仅可以捕捉到文字间的语法信息，更可以捕捉到语义信息，为后续的网络层提供高层次的抽象信息。另外，基于Transformer的模型在一些方面也展现出了优于RNN模型的效果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     10,
     33,
     44,
     52,
     69,
     91,
     104,
     111
    ]
   },
   "outputs": [],
   "source": [
    "# source code: github openAI finetune-transformer-lm/train.py\n",
    "\n",
    "# DROPOUT\n",
    "def dropout(x, pdrop, train):\n",
    "    if train and pdrop > 0:\n",
    "        x = tf.nn.dropout(x, \n",
    "                          keep_prob=1-pdrop)\n",
    "    return x\n",
    "\n",
    "# EMBEDDING\n",
    "def embed(X, we):\n",
    "    we = convert_gradient_to_tensor(we)\n",
    "    e = tf.gather(we, X)\n",
    "    h = tf.reduce_sum(e, 2)\n",
    "    return h\n",
    "\n",
    "# CONV1D\n",
    "def conv1d(x, scope, nf, rf, \n",
    "           w_init=tf.random_normal_initializer(stddev=0.02), \n",
    "           b_init=tf.constant_initializer(0), \n",
    "           pad='VALID', \n",
    "           train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [rf, nx, nf], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [nf], initializer=b_init)\n",
    "        if rf == 1: #faster 1x1 conv\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, shape_list(x)[:-1]+[nf]) # c = xw+b\n",
    "        else: #was used to train LM\n",
    "            c = tf.nn.conv1d(x, w, stride=1, padding=pad)+b\n",
    "        return c\n",
    "\n",
    "# FEED FORWARD NN\n",
    "def mlp(x, scope, n_state, train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        act = act_fns[afn]\n",
    "        h = act(conv1d(x, 'c_fc', n_state, 1, train=train))\n",
    "        h2 = conv1d(h, 'c_proj', nx, 1, train=train)\n",
    "        h2 = dropout(h2, resid_pdrop, train)\n",
    "        return h2\n",
    "    \n",
    "# MULTI-HEAD SELF ATTENTION \n",
    "## generate Attention weights\n",
    "def mask_attn_weights(w):\n",
    "    n = shape_list(w)[-1]\n",
    "    b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n",
    "    b = tf.reshape(b, [1, 1, n, n])\n",
    "    w = w*b + -1e9*(1-b)\n",
    "    return w\n",
    "## Attention Helper\n",
    "### Archtecture: matmul(dropout(softmax(masked(scale(matmul(Q,K))))), v), as in the self attention diagram 1 above in the attention section\n",
    "def _attn(q, k, v, train=False, scale=False):\n",
    "    w = tf.matmul(q, k)\n",
    "\n",
    "    if scale:\n",
    "        n_state = shape_list(v)[-1]\n",
    "        w = w*tf.rsqrt(tf.cast(n_state, tf.float32))\n",
    "\n",
    "    w = mask_attn_weights(w)\n",
    "    w = tf.nn.softmax(w)\n",
    "\n",
    "    w = dropout(w, attn_pdrop, train)\n",
    "\n",
    "    a = tf.matmul(w, v)\n",
    "    return a\n",
    "\n",
    "# Attention\n",
    "### Archtecture: matmul(dropout(softmax(masked(scale(matmul(Q,K))))), v), as in the self attention diagram 2 above in the attention section\n",
    "def split_heads(x, n, k=False):\n",
    "    if k:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 3, 1])\n",
    "    else:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n",
    "    \n",
    "def attn(x, scope, n_state, n_head, train=False, scale=False):\n",
    "    assert n_state%n_head==0\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3, 1, train=train) # faster 1x1 conv\n",
    "        q, k, v = tf.split(c, 3, 2)\n",
    "        q = split_heads(q, n_head)\n",
    "        k = split_heads(k, n_head, k=True)\n",
    "        v = split_heads(v, n_head)\n",
    "        a = _attn(q, k, v, train=train, scale=scale)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state, 1, train=train)\n",
    "        a = dropout(a, resid_pdrop, train)\n",
    "        return a\n",
    "\n",
    "# BLOCK\n",
    "# Architecture: masked multi self-attention --> layer norm --> feed forward --> layer norm --> output\n",
    "def block(x, scope, train=False, scale=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        a = attn(x, 'attn', nx, n_head, train=train, scale=scale)\n",
    "        n = norm(x+a, 'ln_1')\n",
    "        m = mlp(n, 'mlp', nx*4, train=train)\n",
    "        h = norm(n+m, 'ln_2')\n",
    "        return h\n",
    "    \n",
    "    \n",
    "    \n",
    "# MODEL\n",
    "## Logits\n",
    "def clf(x, ny, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), train=False):\n",
    "    with tf.variable_scope('clf'):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [nx, ny], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [ny], initializer=b_init)\n",
    "        return tf.matmul(x, w)+b\n",
    "    \n",
    "def model(X, M, Y, train=False, reuse=False):\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        # n_special=3，作者把数据集分为三份\n",
    "        # n_ctx 应该是 n_context\n",
    "        we = tf.get_variable(name=\"we\", \n",
    "                             shape=[n_vocab+n_special+n_ctx, n_embd], \n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        we = dropout(we, embd_pdrop, train)\n",
    "\n",
    "        X = tf.reshape(X, [-1, n_ctx, 2]) # [batch_size, n_context, 2]\n",
    "        M = tf.reshape(M, [-1, n_ctx])    # [batch_size, n_context]\n",
    "\n",
    "        # 1. Embedding\n",
    "        h = embed(X, we)\n",
    "\n",
    "        # 2. transformer block\n",
    "        for layer in range(n_layer):\n",
    "            h = block(h, 'h%d'%layer, train=train, scale=True)\n",
    "\n",
    "        # 3. language model loss\n",
    "        lm_h = tf.reshape(h[:, :-1], [-1, n_embd])\n",
    "        lm_logits = tf.matmul(lm_h, we, transpose_b=True)\n",
    "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, \n",
    "                                                                   labels=tf.reshape(X[:, 1:, 0], [-1]))\n",
    "        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])\n",
    "        lm_losses = tf.reduce_sum(lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)\n",
    "\n",
    "        # 4. classifier loss\n",
    "        clf_h = tf.reshape(h, [-1, n_embd])\n",
    "        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), \n",
    "                                             tf.float32), \n",
    "                                     1), \n",
    "                           tf.int32)\n",
    "        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32)*n_ctx+pool_idx)\n",
    "\n",
    "        clf_h = tf.reshape(clf_h, [-1, 2, n_embd])\n",
    "        if train and clf_pdrop > 0:\n",
    "            shape = shape_list(clf_h)\n",
    "            shape[1] = 1\n",
    "            clf_h = tf.nn.dropout(clf_h, 1-clf_pdrop, shape)\n",
    "        clf_h = tf.reshape(clf_h, [-1, n_embd])\n",
    "        clf_logits = clf(clf_h, 1, train=train)\n",
    "        clf_logits = tf.reshape(clf_logits, [-1, 2])\n",
    "\n",
    "        clf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clf_logits, labels=Y)\n",
    "        return clf_logits, clf_losses, lm_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Bert notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以$P(w_i| w_1, ...w_{i-1})$ 和 $P(w_i|w_{i+1}, ...w_n)$ 作为目标函数，独立训练处两个representation然后拼接，而BERT则是以 $P(w_i|w_1,  ...,w_{i-1}, w_{i+1},...,w_n)$ 作为目标函数训练LM\n",
    "\n",
    "这篇论文把预训练语言表示方法分为了基于特征的方法（代表 ELMo）和基于微调的方法（代表 OpenAI GPT）。而目前这两种方法在预训练时都是使用单向的语言模型来学习语言表示\n",
    "\n",
    "\n",
    "![](../../images/week3/60.jpg)\n",
    "\n",
    "\n",
    "这篇论文证明了使用双向的预训练效果更好。其实这篇论文方法的整体框架和 GPT 类似，是进一步的发展。具体的，BERT 是使用 Transformer 的编码器来作为语言模型，在语言模型预训练的时候，提出了两个新的目标任务（即masked语言模型 MLM 和预测下一个句子的任务)\n",
    "\n",
    "\n",
    "#### Method\n",
    "\n",
    "在语言模型上，BERT 使用的是 Transformer 编码器，并且设计了一个小一点的 base 结构和一个更大的网络结构\n",
    "\n",
    "![](../../images/week3/61.jpg)\n",
    "\n",
    "对比一下三种语言模型结构:\n",
    "\n",
    "- BERT 使用的是 Transformer 编码器，由于 self-attention 机制，所以模型上下层直接全部互相连接的。\n",
    "- OpenAI GPT 使用的是 Transformer 编码器，它是一个需要__从左到右的受限制__的 Transformer\n",
    "- ELMo 使用的是双向 LSTM，虽然是双向的，但是也只是在两个单向的 LSTM 的最高层进行简单的拼接。\n",
    "\n",
    "\n",
    "__所以只有 BERT 是真正在模型所有层中是双向的__\n",
    "\n",
    "![](../../images/week3/62.jpg)\n",
    "\n",
    "\n",
    "$Note^*$:\n",
    "\n",
    "- 而在模型的输入方面，BERT 做了更多的细节，如:\n",
    "    - 使用了 __WordPiece embedding__ 作为词向量\n",
    "    - 加入了位置向量和句子切分向量。此外，作者还在每一个文本输入前加入了一个 CLS 向量，后面会有这个向量作为具体的分类向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 23:42:34,891 : INFO : collecting all words and their counts\n",
      "2018-11-23 23:42:34,901 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-23 23:42:34,903 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-11-23 23:42:34,911 : INFO : Loading a fresh vocabulary\n",
      "2018-11-23 23:42:34,915 : INFO : effective_min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-11-23 23:42:34,921 : INFO : effective_min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-11-23 23:42:34,925 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-11-23 23:42:34,932 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-11-23 23:42:34,938 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-11-23 23:42:34,940 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-11-23 23:42:34,948 : INFO : resetting layer weights\n",
      "2018-11-23 23:42:34,972 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-23 23:42:35,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,035 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,054 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,061 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,070 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 99 effective words/s\n",
      "2018-11-23 23:42:35,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,088 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,113 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,125 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,127 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 7 effective words/s\n",
      "2018-11-23 23:42:35,130 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "# Gensim only requires that the input must provide sentences sequentially, \n",
    "# when iterated over. No need to keep everything in RAM: provide one sentence, process it, forget it, load another sentence\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 00:24:28,072 : INFO : loading projection weights from /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt\n",
      "2018-11-24 00:26:45,137 : INFO : loaded (399999, 100) matrix from /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f770ca3cd30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(fname='/home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt', binary=False)\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/home/karen/Downloads/data/glove.6B/glove.6B.100d.txt', 'r') as fread:\n",
    "    with open('/home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt', 'w') as fwrite:\n",
    "        fwrite.write('399999 100\\n')\n",
    "        for idx, line in enumerate(fread):\n",
    "            fwrite.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!rm /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 00:28:08,244 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14084136,  0.10044176, -0.17062186, -0.03504317, -0.03259846,\n",
       "        0.16226351,  0.00164017,  0.02982827, -0.04554551,  0.19863695,\n",
       "       -0.08848356, -0.03040672,  0.06940352,  0.03717477, -0.05495993,\n",
       "        0.05361199, -0.01390229, -0.01308273, -0.07149444,  0.08059222,\n",
       "       -0.16011068, -0.03823438,  0.07592923, -0.01951167, -0.00767571,\n",
       "       -0.14076705, -0.04336968, -0.19582428,  0.01398277,  0.07748415,\n",
       "       -0.11785013,  0.21659192, -0.01383613,  0.07638032, -0.10138992,\n",
       "       -0.1082482 , -0.03673607,  0.08005092, -0.05872075, -0.01275246,\n",
       "       -0.147268  ,  0.00342861, -0.00873196,  0.02516351,  0.06088596,\n",
       "       -0.05161836, -0.0100394 ,  0.03327952,  0.04757451,  0.04673779,\n",
       "       -0.04099397, -0.11782714, -0.05391094,  0.07558075,  0.00579424,\n",
       "       -0.30187368,  0.01918441, -0.0895591 ,  0.29233897,  0.04310788,\n",
       "       -0.00594   ,  0.01523573, -0.04347936, -0.01092671,  0.11162515,\n",
       "        0.01602999,  0.13726982,  0.06173506,  0.18407837,  0.16519998,\n",
       "        0.03331666, -0.04278062, -0.02724027, -0.04862705, -0.09478815,\n",
       "        0.13880351,  0.11250433, -0.09805897, -0.09296789, -0.17817003,\n",
       "        0.055572  ,  0.04316802, -0.00249795, -0.22557826, -0.25593367,\n",
       "       -0.04909405, -0.02562875, -0.08425928,  0.07797062, -0.1113262 ,\n",
       "        0.04604082,  0.04001042,  0.06554011,  0.08327927, -0.09876832,\n",
       "        0.07073379,  0.02987957, -0.05065782,  0.0969799 , -0.05943188],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.vectors_norm[word2vec.vocab['student'].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Sentence2Vec by word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_word2vec_matrix(text, word2vec):\n",
    "    word2vec_matrix=[]\n",
    "    count=0\n",
    "    for line in text:\n",
    "        word_lst=line.split()\n",
    "        current_word2vec=[]\n",
    "        for word in word_lst:\n",
    "            if word in word2vec.vocab:\n",
    "                # word2vec = token2idx\n",
    "                vec = word2vec.vectors_norm[word2vec.vocab[word].index]\n",
    "                if vec is not None:\n",
    "                    current_word2vec.append(vec)\n",
    "            else:\n",
    "                print(word)\n",
    "                count+=1\n",
    "                continue\n",
    "        # add up all the vector of each word to get the vector of a sentence \n",
    "        if np.array(current_word2vec).shape[0]!=0:\n",
    "            sentence_word2vec = list(np.array(current_word2vec).mean(axis=0))\n",
    "            word2vec_matrix.append(sentence_word2vec)\n",
    "        current_word2vec=[]\n",
    "    return word2vec_matrix, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = ['fantastic beasts and where to find them', \n",
    "        'fantastic beasts the crimes of grindelwald']\n",
    "word2vec_matrix, count = create_word2vec_matrix(text, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01363225,  0.0632612 ,  0.0532789 , -0.06067752, -0.03203793,\n",
       "         0.0573948 , -0.05758385,  0.02891669, -0.01005686, -0.04475319,\n",
       "         0.03142466, -0.00525038,  0.03889203, -0.01838762,  0.04951621,\n",
       "        -0.0357757 ,  0.04723538,  0.06406571, -0.110898  ,  0.08045464,\n",
       "         0.06799715, -0.03494107,  0.05513481, -0.04234928,  0.04503259,\n",
       "        -0.0038799 , -0.02963971, -0.0674924 ,  0.0407795 , -0.05591179,\n",
       "        -0.03720056,  0.01724851, -0.06198395, -0.00394624,  0.01963765,\n",
       "         0.04357598, -0.03199228,  0.04052764,  0.01646447, -0.06384147,\n",
       "        -0.06349035, -0.01205016, -0.03160828, -0.10188204, -0.02248716,\n",
       "         0.05475311, -0.00057904, -0.00529261,  0.00310575, -0.06026373,\n",
       "        -0.03455113,  0.0093182 ,  0.04103724,  0.22216587, -0.00798338,\n",
       "        -0.37076157,  0.0060763 , -0.02901358,  0.19122097,  0.05265542,\n",
       "        -0.0445339 ,  0.15990758, -0.07531472,  0.02351545,  0.13658723,\n",
       "        -0.00910989,  0.08402754,  0.06993966,  0.04774452, -0.07440751,\n",
       "         0.00432026, -0.1105385 ,  0.01313347, -0.07487839,  0.02296722,\n",
       "         0.0301899 , -0.0280746 , -0.01931024, -0.08416656,  0.03807382,\n",
       "         0.1088913 ,  0.05673665, -0.07269619,  0.04082172, -0.2123229 ,\n",
       "        -0.03421097,  0.00801395, -0.01705644, -0.07357238, -0.03133134,\n",
       "        -0.01436853, -0.05166963,  0.02597543, -0.01765687, -0.12825742,\n",
       "        -0.05672801, -0.09524342, -0.05342931,  0.0833168 ,  0.04327292],\n",
       "       [ 0.00253851, -0.00877398,  0.0734567 , -0.00424336,  0.00065437,\n",
       "         0.05516574, -0.07829381,  0.01089091, -0.05188168, -0.02742841,\n",
       "         0.02640177,  0.01450567, -0.00968969, -0.03183353,  0.04959001,\n",
       "         0.02107229,  0.01465111,  0.03763356, -0.07321229,  0.02437557,\n",
       "         0.0532227 , -0.02813247, -0.01158483, -0.04175212,  0.03454379,\n",
       "        -0.02026052,  0.03413947, -0.00275082, -0.01641666, -0.04765847,\n",
       "         0.02100672, -0.03880749, -0.03762351, -0.04919778,  0.00471069,\n",
       "        -0.00720015, -0.01010328,  0.0680422 ,  0.0268242 , -0.07889139,\n",
       "        -0.05303546,  0.02562189,  0.00818047, -0.0245726 ,  0.00355374,\n",
       "         0.08366998,  0.06239701, -0.02762469, -0.01287852, -0.00160172,\n",
       "        -0.00158122,  0.01933938,  0.03472779,  0.13059537, -0.0444909 ,\n",
       "        -0.20608287,  0.01566952,  0.01300212,  0.10512716,  0.02248251,\n",
       "        -0.01267021,  0.11147016, -0.04865355, -0.00529323,  0.08803131,\n",
       "         0.01013602,  0.01064846,  0.01560272,  0.03319718, -0.03983454,\n",
       "         0.04644413, -0.10616875, -0.00987621,  0.05064338,  0.01425096,\n",
       "         0.02025635, -0.02772122, -0.01244517, -0.07074878,  0.04453899,\n",
       "         0.09321102,  0.10945683, -0.02349626,  0.04435547, -0.16061288,\n",
       "        -0.00520629, -0.0616868 , -0.02149554,  0.00955985, -0.01845567,\n",
       "        -0.02264366, -0.02768158,  0.03380519,  0.06865028, -0.03667846,\n",
       "        -0.05988262, -0.10793936, -0.03674577,  0.06783447,  0.01057718]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(word2vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count # so all words are in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8124398589134216"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = word2vec_matrix[0]\n",
    "sentence2 = word2vec_matrix[1]\n",
    "from scipy import spatial\n",
    "\n",
    "# calculate the cosine similarity\n",
    "1 - spatial.distance.cosine(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('school', 0.7116979360580444),\n",
       " ('elementary', 0.7066525220870972),\n",
       " ('grades', 0.7055771350860596),\n",
       " ('schools', 0.6759970188140869),\n",
       " ('preschool', 0.6746401786804199),\n",
       " ('pupils', 0.6707763075828552),\n",
       " ('classes', 0.646867036819458),\n",
       " ('schooling', 0.6365389227867126),\n",
       " ('vocational', 0.6250067949295044),\n",
       " ('enrollment', 0.6218675374984741)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['kindergarten', 'college'], negative=['scientist'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Find the different word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### calculate the similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1993172"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity('apocalypse', 'disaster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
<<<<<<< HEAD
   "source": [
    "### [Word2Vec using Fasttext](https://pypi.org/project/fasttext/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### FastText?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "word2vec 和 GloVe 都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，__fastText 是利用带有监督标记的文本分类数据完成训练__\n",
    "\n",
    "本质上没有什么特殊的，__模型框架就是 CBOW__，只不过与普通的 CBOW 有两点不一样，分别是__输入数据和预测目标的不同__:\n",
    "\n",
    "- 在输入数据上，CBOW 输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，__而 fastText 为了利用更多的语序信息，将 bag-of-words 变成了 bag-of-features__，也就是输入 x 不再仅仅是一个词，还可以加上 bigram 或者是 trigram 的信息等等。\n",
    "\n",
    "\n",
    "![](../../images/week3/36.png)\n",
    "\n",
    "- 在预测目标上，CBOW 预测目标是语境中的一个词，而 __fastText 预测目标是当前这段输入文本的类别__，正因为需要这个文本类别，因此才说 fastText 是一个监督模型。\n",
    "\n",
    "而相同点在于，fastText 的网络结构和 CBOW 基本一致，同时在输出层的分类上也使用了 Hierachical Softmax 技巧来加速训练。\n",
    "\n",
    "这里的$x_{n,i}$便是语料当中第 n 篇文档的第 i 个词以及加上 N-gram 的特征信息。从这个损失函数便可以知道 __fastText 同样只有两个全连接层，分别是 A 和 B__，其中 A 便是最终可以获取的词向量信息"
=======
   "source": [
    "### [Sent2Vec using Fasttext](https://pypi.org/project/fasttext/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sent2vec represents sentences as an average of words DURING TRAINING, while __averaging word2vec vectors is a postprocessing step__\n",
    "\n",
    "__Even though sent2vec is still based on word (or n-gram) vectors, it tunes them during training to fit word-in-sentence co-occurrence data. This is not the same as fitting word-word co-occurrence data (as word2vec does) and can crucially improve the final representations of sentences__"
>>>>>>> 1587d5940474b9d285d86e287bb75e66443a0bd7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> 1587d5940474b9d285d86e287bb75e66443a0bd7
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
<<<<<<< HEAD
    "load_vectors('/home/karen/Downloads/data/wiki-news-300d-1M.vec')"
=======
    "# load_vectors('/home/karen/Downloads/data/wiki-news-300d-1M.vec')"
>>>>>>> 1587d5940474b9d285d86e287bb75e66443a0bd7
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.000991184264421463,\n",
       " -0.001029246486723423,\n",
       " 0.0006811252678744495,\n",
       " 0.00011803481174865738,\n",
       " -0.0006219972274266183,\n",
       " 0.001100853318348527,\n",
       " 0.00014544253644999117,\n",
       " -0.0008069276809692383,\n",
       " 0.0008861601236276329,\n",
       " 0.0005640610470436513,\n",
       " 0.00018239919154439121,\n",
       " 0.002081118058413267,\n",
       " 0.0011119148693978786,\n",
       " -8.017678737815004e-06,\n",
       " 0.0019191585015505552,\n",
       " 0.002273016609251499,\n",
       " 0.0004999999655410647,\n",
       " 0.0008727581007406116,\n",
       " 0.0017401062650606036,\n",
       " 0.00136960344389081,\n",
       " -0.00010665664740372449,\n",
       " 0.0006562909111380577,\n",
       " -0.0005103441653773189,\n",
       " 0.0006105859065428376,\n",
       " -0.001563229481689632,\n",
       " 0.0024708015844225883,\n",
       " -7.241084676934406e-05,\n",
       " 0.00035496175405569375,\n",
       " -0.0008005818235687912,\n",
       " 0.001430544420145452,\n",
       " 0.0005475004436448216,\n",
       " -0.000570164353121072,\n",
       " 0.00010769572691060603,\n",
       " -2.468213642714545e-05,\n",
       " 0.0015175550943240523,\n",
       " 0.0003394550003577024,\n",
       " -0.0006585742812603712,\n",
       " -0.0010379229206591845,\n",
       " 0.0002934975200332701,\n",
       " -0.0004705099854618311,\n",
       " -0.0005588103667832911,\n",
       " -0.0001224353618454188,\n",
       " 0.00018472163355909288,\n",
       " -0.0008804462268017232,\n",
       " -0.000540959823410958,\n",
       " 0.0007997071370482445,\n",
       " -0.000813662598375231,\n",
       " -0.0012080087326467037,\n",
       " 0.00136748724617064,\n",
       " 0.0007802385371178389,\n",
       " 0.0003648688143584877,\n",
       " -3.339976683491841e-05,\n",
       " -0.00031441979808732867,\n",
       " -0.0003039219882339239,\n",
       " -0.0005072257481515408,\n",
       " 0.0010179569944739342,\n",
       " -0.0009995679138228297,\n",
       " -0.0003505126223899424,\n",
       " 0.0003821019490715116,\n",
       " 0.0011511098127812147,\n",
       " -0.0005719128530472517,\n",
       " -0.0006986728403717279,\n",
       " 0.0005401912494562566,\n",
       " -0.0013313741656020284,\n",
       " 0.00041330300155095756,\n",
       " -0.0006128869135864079,\n",
       " 0.0012367976596578956,\n",
       " -0.001043042866513133,\n",
       " 0.000432994042057544,\n",
       " -0.0024143403861671686,\n",
       " -0.00034675822826102376,\n",
       " -0.00010233062494080514,\n",
       " 0.001215107273310423,\n",
       " 0.0012985324719920754,\n",
       " -6.912585376994684e-05,\n",
       " -2.201867027906701e-05,\n",
       " -0.0011087505845353007,\n",
       " -0.0006296449573710561,\n",
       " 0.0004131598980166018,\n",
       " 0.0010789132211357355,\n",
       " 0.001263558049686253,\n",
       " 0.00032109773019328713,\n",
       " 0.0006544655188918114,\n",
       " -0.0001419403706677258,\n",
       " -0.00021072222443763167,\n",
       " 0.00044179518590681255,\n",
       " 0.0007763911853544414,\n",
       " 0.0009751742472872138,\n",
       " -0.0004021052736788988,\n",
       " -0.0007130807498469949,\n",
       " -0.001583990640938282,\n",
       " -0.0005740714841522276,\n",
       " -0.0015615387819707394,\n",
       " -0.0025593223981559277,\n",
       " 0.00018674305465538055,\n",
       " 0.0009086879435926676,\n",
       " 0.0008893464109860361,\n",
       " 0.00039883964927867055,\n",
       " -0.0007123534451238811,\n",
       " 0.0025303619913756847]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('toy_data/second.txt', 'model')\n",
    "model['fantastic']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CBOW model\n",
    "model = fasttext.cbow('toy_data/second.txt', 'model')"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 1587d5940474b9d285d86e287bb75e66443a0bd7
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.000991184264421463,\n",
       " -0.001029246486723423,\n",
       " 0.0006811252678744495,\n",
       " 0.00011803481174865738,\n",
       " -0.0006219972274266183,\n",
       " 0.001100853318348527,\n",
       " 0.00014544253644999117,\n",
       " -0.0008069276809692383,\n",
       " 0.0008861601236276329,\n",
       " 0.0005640610470436513,\n",
       " 0.00018239919154439121,\n",
       " 0.002081118058413267,\n",
       " 0.0011119148693978786,\n",
       " -8.017678737815004e-06,\n",
       " 0.0019191585015505552,\n",
       " 0.002273016609251499,\n",
       " 0.0004999999655410647,\n",
       " 0.0008727581007406116,\n",
       " 0.0017401062650606036,\n",
       " 0.00136960344389081,\n",
       " -0.00010665664740372449,\n",
       " 0.0006562909111380577,\n",
       " -0.0005103441653773189,\n",
       " 0.0006105859065428376,\n",
       " -0.001563229481689632,\n",
       " 0.0024708015844225883,\n",
       " -7.241084676934406e-05,\n",
       " 0.00035496175405569375,\n",
       " -0.0008005818235687912,\n",
       " 0.001430544420145452,\n",
       " 0.0005475004436448216,\n",
       " -0.000570164353121072,\n",
       " 0.00010769572691060603,\n",
       " -2.468213642714545e-05,\n",
       " 0.0015175550943240523,\n",
       " 0.0003394550003577024,\n",
       " -0.0006585742812603712,\n",
       " -0.0010379229206591845,\n",
       " 0.0002934975200332701,\n",
       " -0.0004705099854618311,\n",
       " -0.0005588103667832911,\n",
       " -0.0001224353618454188,\n",
       " 0.00018472163355909288,\n",
       " -0.0008804462268017232,\n",
       " -0.000540959823410958,\n",
       " 0.0007997071370482445,\n",
       " -0.000813662598375231,\n",
       " -0.0012080087326467037,\n",
       " 0.00136748724617064,\n",
       " 0.0007802385371178389,\n",
       " 0.0003648688143584877,\n",
       " -3.339976683491841e-05,\n",
       " -0.00031441979808732867,\n",
       " -0.0003039219882339239,\n",
       " -0.0005072257481515408,\n",
       " 0.0010179569944739342,\n",
       " -0.0009995679138228297,\n",
       " -0.0003505126223899424,\n",
       " 0.0003821019490715116,\n",
       " 0.0011511098127812147,\n",
       " -0.0005719128530472517,\n",
       " -0.0006986728403717279,\n",
       " 0.0005401912494562566,\n",
       " -0.0013313741656020284,\n",
       " 0.00041330300155095756,\n",
       " -0.0006128869135864079,\n",
       " 0.0012367976596578956,\n",
       " -0.001043042866513133,\n",
       " 0.000432994042057544,\n",
       " -0.0024143403861671686,\n",
       " -0.00034675822826102376,\n",
       " -0.00010233062494080514,\n",
       " 0.001215107273310423,\n",
       " 0.0012985324719920754,\n",
       " -6.912585376994684e-05,\n",
       " -2.201867027906701e-05,\n",
       " -0.0011087505845353007,\n",
       " -0.0006296449573710561,\n",
       " 0.0004131598980166018,\n",
       " 0.0010789132211357355,\n",
       " 0.001263558049686253,\n",
       " 0.00032109773019328713,\n",
       " 0.0006544655188918114,\n",
       " -0.0001419403706677258,\n",
       " -0.00021072222443763167,\n",
       " 0.00044179518590681255,\n",
       " 0.0007763911853544414,\n",
       " 0.0009751742472872138,\n",
       " -0.0004021052736788988,\n",
       " -0.0007130807498469949,\n",
       " -0.001583990640938282,\n",
       " -0.0005740714841522276,\n",
       " -0.0015615387819707394,\n",
       " -0.0025593223981559277,\n",
       " 0.00018674305465538055,\n",
       " 0.0009086879435926676,\n",
       " 0.0008893464109860361,\n",
       " 0.00039883964927867055,\n",
       " -0.0007123534451238811,\n",
       " 0.0025303619913756847]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CBOW model\n",
    "model = fasttext.cbow('toy_data/second.txt', 'model')\n",
    "model['fantastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = fasttext.load_model('model.bin')\n",
    "# model['fantastic']"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! sent2vec/src/fasttext print-sentence-vectors model.bin < text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sent2Vec using Starspace](https://github.com/facebookresearch/StarSpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2c6a822e3314>:81: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Starting Training Over 4459 Sentences.\n",
      "Training Observation #10: Loss = 12.83597\n",
      "Training Observation #20: Loss = 0.00036747786\n",
      "Training Observation #30: Loss = 0.017874205\n",
      "Training Observation #40: Loss = 0.1322336\n",
      "Training Observation #50: Loss = 0.0891401\n",
      "Training Observation #60: Loss = 0.0013315583\n",
      "Training Observation #70: Loss = 2.9756798e-06\n",
      "Training Observation #80: Loss = 0.00089186867\n",
      "Training Observation #90: Loss = 0.007103161\n",
      "Training Observation #100: Loss = 2.522557\n",
      "Training Observation #110: Loss = 0.10210744\n",
      "Training Observation #120: Loss = 0.3243814\n",
      "Training Observation #130: Loss = 0.7241575\n",
      "Training Observation #140: Loss = 0.0012320678\n",
      "Training Observation #150: Loss = 0.0108655095\n",
      "Training Observation #160: Loss = 0.012075206\n",
      "Training Observation #170: Loss = 0.0051761228\n",
      "Training Observation #180: Loss = 0.03767232\n",
      "Training Observation #190: Loss = 0.0008419928\n",
      "Training Observation #200: Loss = 0.036374208\n",
      "Training Observation #210: Loss = 0.018221281\n",
      "Training Observation #220: Loss = 0.00028485243\n",
      "Training Observation #230: Loss = 0.06835014\n",
      "Training Observation #240: Loss = 1.0068004\n",
      "Training Observation #250: Loss = 0.02255757\n",
      "Training Observation #260: Loss = 0.0019438778\n",
      "Training Observation #270: Loss = 0.11785612\n",
      "Training Observation #280: Loss = 0.0013219296\n",
      "Training Observation #290: Loss = 1.1935585e-06\n",
      "Training Observation #300: Loss = 0.12436022\n",
      "Training Observation #310: Loss = 0.00040547355\n",
      "Training Observation #320: Loss = 0.015276301\n",
      "Training Observation #330: Loss = 6.6564846\n",
      "Training Observation #340: Loss = 0.018993717\n",
      "Training Observation #350: Loss = 0.014083611\n",
      "Training Observation #360: Loss = 0.0008072286\n",
      "Training Observation #370: Loss = 0.0107799815\n",
      "Training Observation #380: Loss = 0.008574913\n",
      "Training Observation #390: Loss = 0.0023727624\n",
      "Training Observation #400: Loss = 0.0059139812\n",
      "Training Observation #410: Loss = 4.049254\n",
      "Training Observation #420: Loss = 0.038739104\n",
      "Training Observation #430: Loss = 3.1808417\n",
      "Training Observation #440: Loss = 4.926406e-05\n",
      "Training Observation #450: Loss = 5.521544\n",
      "Training Observation #460: Loss = 0.00710985\n",
      "Training Observation #470: Loss = 2.0455751\n",
      "Training Observation #480: Loss = 7.8590245\n",
      "Training Observation #490: Loss = 0.00977158\n",
      "Training Observation #500: Loss = 6.9037914\n",
      "Training Observation #510: Loss = 1.5676032e-05\n",
      "Training Observation #520: Loss = 4.889942\n",
      "Training Observation #530: Loss = 0.0015665988\n",
      "Training Observation #540: Loss = 0.042012636\n",
      "Training Observation #550: Loss = 0.024860436\n",
      "Training Observation #560: Loss = 3.515129\n",
      "Training Observation #570: Loss = 0.007790206\n",
      "Training Observation #580: Loss = 6.854002e-05\n",
      "Training Observation #590: Loss = 0.0009163599\n",
      "Training Observation #600: Loss = 2.5105507\n",
      "Training Observation #610: Loss = 2.4479612e-05\n",
      "Training Observation #620: Loss = 0.015267419\n",
      "Training Observation #630: Loss = 0.009593348\n",
      "Training Observation #640: Loss = 0.08259696\n",
      "Training Observation #650: Loss = 0.020136258\n",
      "Training Observation #660: Loss = 1.4227403\n",
      "Training Observation #670: Loss = 0.09208563\n",
      "Training Observation #680: Loss = 7.02384\n",
      "Training Observation #690: Loss = 4.6549144\n",
      "Training Observation #700: Loss = 0.060743324\n",
      "Training Observation #710: Loss = 0.00013194572\n",
      "Training Observation #720: Loss = 0.001847928\n",
      "Training Observation #730: Loss = 1.5711595\n",
      "Training Observation #740: Loss = 5.1167517e-06\n",
      "Training Observation #750: Loss = 0.022635223\n",
      "Training Observation #760: Loss = 0.0049313996\n",
      "Training Observation #770: Loss = 0.06182465\n",
      "Training Observation #780: Loss = 0.27767777\n",
      "Training Observation #790: Loss = 0.5683476\n",
      "Training Observation #800: Loss = 0.00043888492\n",
      "Training Observation #810: Loss = 0.0007820625\n",
      "Training Observation #820: Loss = 0.017586801\n",
      "Training Observation #830: Loss = 0.0027241036\n",
      "Training Observation #840: Loss = 0.0073425043\n",
      "Training Observation #850: Loss = 0.2262907\n",
      "Training Observation #860: Loss = 6.329936\n",
      "Training Observation #870: Loss = 0.0008231554\n",
      "Training Observation #880: Loss = 0.0010927587\n",
      "Training Observation #890: Loss = 4.450685\n",
      "Training Observation #900: Loss = 3.1203132\n",
      "Training Observation #910: Loss = 0.9683882\n",
      "Training Observation #920: Loss = 0.11517322\n",
      "Training Observation #930: Loss = 0.0001589594\n",
      "Training Observation #940: Loss = 0.00036356275\n",
      "Training Observation #950: Loss = 4.428695\n",
      "Training Observation #960: Loss = 1.8751357e-06\n",
      "Training Observation #970: Loss = 0.56484026\n",
      "Training Observation #980: Loss = 4.1719213e-05\n",
      "Training Observation #990: Loss = 0.12951154\n",
      "Training Observation #1000: Loss = 4.5389977\n",
      "Training Observation #1010: Loss = 0.04485746\n",
      "Training Observation #1020: Loss = 0.00062377297\n",
      "Training Observation #1030: Loss = 5.2320656e-06\n",
      "Training Observation #1040: Loss = 0.010418015\n",
      "Training Observation #1050: Loss = 0.004826072\n",
      "Training Observation #1060: Loss = 0.000114982075\n",
      "Training Observation #1070: Loss = 0.0234188\n",
      "Training Observation #1080: Loss = 1.0278125\n",
      "Training Observation #1090: Loss = 2.8948846\n",
      "Training Observation #1100: Loss = 0.0009573599\n",
      "Training Observation #1110: Loss = 0.00039693931\n",
      "Training Observation #1120: Loss = 0.1375273\n",
      "Training Observation #1130: Loss = 0.356089\n",
      "Training Observation #1140: Loss = 0.010751816\n",
      "Training Observation #1150: Loss = 0.0050143395\n",
      "Training Observation #1160: Loss = 0.018728817\n",
      "Training Observation #1170: Loss = 0.0010127317\n",
      "Training Observation #1180: Loss = 5.7436586e-07\n",
      "Training Observation #1190: Loss = 0.03553387\n",
      "Training Observation #1200: Loss = 0.5693738\n",
      "Training Observation #1210: Loss = 0.0016681601\n",
      "Training Observation #1220: Loss = 0.00042327665\n",
      "Training Observation #1230: Loss = 5.8355818e-05\n",
      "Training Observation #1240: Loss = 0.00083527627\n",
      "Training Observation #1250: Loss = 0.0010290076\n",
      "Training Observation #1260: Loss = 0.00075661956\n",
      "Training Observation #1270: Loss = 7.6674347\n",
      "Training Observation #1280: Loss = 0.00017410805\n",
      "Training Observation #1290: Loss = 0.00033005266\n",
      "Training Observation #1300: Loss = 0.0002062842\n",
      "Training Observation #1310: Loss = 0.4988019\n",
      "Training Observation #1320: Loss = 0.006687494\n",
      "Training Observation #1330: Loss = 0.0043020872\n",
      "Training Observation #1340: Loss = 0.00012284167\n",
      "Training Observation #1350: Loss = 0.00043661668\n",
      "Training Observation #1360: Loss = 0.000248876\n",
      "Training Observation #1370: Loss = 8.451655\n",
      "Training Observation #1380: Loss = 0.00024946762\n",
      "Training Observation #1390: Loss = 0.0007919484\n",
      "Training Observation #1400: Loss = 0.00062078174\n",
      "Training Observation #1410: Loss = 5.516938\n",
      "Training Observation #1420: Loss = 2.9663897\n",
      "Training Observation #1430: Loss = 0.0003468246\n",
      "Training Observation #1440: Loss = 3.2560194\n",
      "Training Observation #1450: Loss = 0.00017624287\n",
      "Training Observation #1460: Loss = 0.0010867767\n",
      "Training Observation #1470: Loss = 0.03183364\n",
      "Training Observation #1480: Loss = 0.45820296\n",
      "Training Observation #1490: Loss = 0.00011423451\n",
      "Training Observation #1500: Loss = 4.8725276\n",
      "Training Observation #1510: Loss = 5.09648e-05\n",
      "Training Observation #1520: Loss = 0.03192837\n",
      "Training Observation #1530: Loss = 0.00010403774\n",
      "Training Observation #1540: Loss = 3.2132306e-05\n",
      "Training Observation #1550: Loss = 0.0510729\n",
      "Training Observation #1560: Loss = 0.001676368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #1570: Loss = 0.23155643\n",
      "Training Observation #1580: Loss = 0.00027795325\n",
      "Training Observation #1590: Loss = 4.7545896\n",
      "Training Observation #1600: Loss = 5.601075e-05\n",
      "Training Observation #1610: Loss = 0.00028800723\n",
      "Training Observation #1620: Loss = 5.981204e-05\n",
      "Training Observation #1630: Loss = 0.20729166\n",
      "Training Observation #1640: Loss = 1.18306225e-05\n",
      "Training Observation #1650: Loss = 4.0363047e-05\n",
      "Training Observation #1660: Loss = 8.367646\n",
      "Training Observation #1670: Loss = 2.1697133\n",
      "Training Observation #1680: Loss = 0.0004692959\n",
      "Training Observation #1690: Loss = 0.21425524\n",
      "Training Observation #1700: Loss = 1.32483\n",
      "Training Observation #1710: Loss = 4.8706897e-05\n",
      "Training Observation #1720: Loss = 0.0008537966\n",
      "Training Observation #1730: Loss = 2.3041952\n",
      "Training Observation #1740: Loss = 0.00014471836\n",
      "Training Observation #1750: Loss = 0.1383138\n",
      "Training Observation #1760: Loss = 0.0042888285\n",
      "Training Observation #1770: Loss = 0.022666944\n",
      "Training Observation #1780: Loss = 0.13900557\n",
      "Training Observation #1790: Loss = 0.0004509503\n",
      "Training Observation #1800: Loss = 0.000933145\n",
      "Training Observation #1810: Loss = 3.9955175\n",
      "Training Observation #1820: Loss = 7.428463e-05\n",
      "Training Observation #1830: Loss = 6.4743924\n",
      "Training Observation #1840: Loss = 5.5858617\n",
      "Training Observation #1850: Loss = 2.683272e-05\n",
      "Training Observation #1860: Loss = 0.0008394911\n",
      "Training Observation #1870: Loss = 4.122535e-05\n",
      "Training Observation #1880: Loss = 12.943296\n",
      "Training Observation #1890: Loss = 1.4877846\n",
      "Training Observation #1900: Loss = 2.6812717e-05\n",
      "Training Observation #1910: Loss = 0.00041702535\n",
      "Training Observation #1920: Loss = 0.009758903\n",
      "Training Observation #1930: Loss = 0.0018543244\n",
      "Training Observation #1940: Loss = 0.000825906\n",
      "Training Observation #1950: Loss = 0.0012984205\n",
      "Training Observation #1960: Loss = 0.00083833624\n",
      "Training Observation #1970: Loss = 0.6683789\n",
      "Training Observation #1980: Loss = 5.6840277\n",
      "Training Observation #1990: Loss = 2.6125002e-05\n",
      "Training Observation #2000: Loss = 0.00018778336\n",
      "Training Observation #2010: Loss = 0.011816997\n",
      "Training Observation #2020: Loss = 0.0041122297\n",
      "Training Observation #2030: Loss = 2.6838834\n",
      "Training Observation #2040: Loss = 0.0009130339\n",
      "Training Observation #2050: Loss = 7.966196\n",
      "Training Observation #2060: Loss = 0.0006662758\n",
      "Training Observation #2070: Loss = 0.01731772\n",
      "Training Observation #2080: Loss = 0.0023503418\n",
      "Training Observation #2090: Loss = 4.2157445\n",
      "Training Observation #2100: Loss = 0.0060417396\n",
      "Training Observation #2110: Loss = 9.311978\n",
      "Training Observation #2120: Loss = 0.08842871\n",
      "Training Observation #2130: Loss = 4.1621697e-06\n",
      "Training Observation #2140: Loss = 0.013235861\n",
      "Training Observation #2150: Loss = 6.020026\n",
      "Training Observation #2160: Loss = 4.975566e-06\n",
      "Training Observation #2170: Loss = 0.0016112423\n",
      "Training Observation #2180: Loss = 0.0008295352\n",
      "Training Observation #2190: Loss = 2.9364252\n",
      "Training Observation #2200: Loss = 6.009645\n",
      "Training Observation #2210: Loss = 0.20709269\n",
      "Training Observation #2220: Loss = 3.906823\n",
      "Training Observation #2230: Loss = 0.00018246204\n",
      "Training Observation #2240: Loss = 0.00012079956\n",
      "Training Observation #2250: Loss = 4.0590872e-05\n",
      "Training Observation #2260: Loss = 2.6955483\n",
      "Training Observation #2270: Loss = 0.0033104992\n",
      "Training Observation #2280: Loss = 0.00016198013\n",
      "Training Observation #2290: Loss = 0.0016647232\n",
      "Training Observation #2300: Loss = 0.0010670425\n",
      "Training Observation #2310: Loss = 1.5713694\n",
      "Training Observation #2320: Loss = 1.2095017e-05\n",
      "Training Observation #2330: Loss = 6.9833336e-06\n",
      "Training Observation #2340: Loss = 2.738894\n",
      "Training Observation #2350: Loss = 9.9799414e-05\n",
      "Training Observation #2360: Loss = 0.0019065572\n",
      "Training Observation #2370: Loss = 0.0060472363\n",
      "Training Observation #2380: Loss = 2.9796603\n",
      "Training Observation #2390: Loss = 0.004988723\n",
      "Training Observation #2400: Loss = 0.00044671644\n",
      "Training Observation #2410: Loss = 0.0004479275\n",
      "Training Observation #2420: Loss = 3.07029e-05\n",
      "Training Observation #2430: Loss = 0.0003267837\n",
      "Training Observation #2440: Loss = 0.0050377958\n",
      "Training Observation #2450: Loss = 0.0050290334\n",
      "Training Observation #2460: Loss = 0.098247744\n",
      "Training Observation #2470: Loss = 4.7559486e-05\n",
      "Training Observation #2480: Loss = 0.6279479\n",
      "Training Observation #2490: Loss = 0.018398734\n",
      "Training Observation #2500: Loss = 0.00024238441\n",
      "Training Observation #2510: Loss = 0.16419229\n",
      "Training Observation #2520: Loss = 0.0063596754\n",
      "Training Observation #2530: Loss = 6.971257e-06\n",
      "Training Observation #2540: Loss = 0.6213671\n",
      "Training Observation #2550: Loss = 0.007888898\n",
      "Training Observation #2560: Loss = 0.00019243619\n",
      "Training Observation #2570: Loss = 0.38577676\n",
      "Training Observation #2580: Loss = 0.06472641\n",
      "Training Observation #2590: Loss = 0.0063231518\n",
      "Training Observation #2600: Loss = 2.2050228e-05\n",
      "Training Observation #2610: Loss = 0.05263435\n",
      "Training Observation #2620: Loss = 0.004491146\n",
      "Training Observation #2630: Loss = 0.020597806\n",
      "Training Observation #2640: Loss = 1.6883129\n",
      "Training Observation #2650: Loss = 0.054580014\n",
      "Training Observation #2660: Loss = 0.48895836\n",
      "Training Observation #2670: Loss = 0.10949553\n",
      "Training Observation #2680: Loss = 0.0017249119\n",
      "Training Observation #2690: Loss = 0.036462586\n",
      "Training Observation #2700: Loss = 0.028036471\n",
      "Training Observation #2710: Loss = 0.00055380963\n",
      "Training Observation #2720: Loss = 0.00017816873\n",
      "Training Observation #2730: Loss = 0.55087364\n",
      "Training Observation #2740: Loss = 0.0074890223\n",
      "Training Observation #2750: Loss = 0.48709846\n",
      "Training Observation #2760: Loss = 0.31648973\n",
      "Training Observation #2770: Loss = 0.14120851\n",
      "Training Observation #2780: Loss = 0.3373715\n",
      "Training Observation #2790: Loss = 2.575482\n",
      "Training Observation #2800: Loss = 0.0055998154\n",
      "Training Observation #2810: Loss = 0.0025351348\n",
      "Training Observation #2820: Loss = 0.012775381\n",
      "Training Observation #2830: Loss = 0.27625632\n",
      "Training Observation #2840: Loss = 4.7102\n",
      "Training Observation #2850: Loss = 0.0023710735\n",
      "Training Observation #2860: Loss = 0.106114045\n",
      "Training Observation #2870: Loss = 0.0049088853\n",
      "Training Observation #2880: Loss = 1.8151978e-07\n",
      "Training Observation #2890: Loss = 0.003963956\n",
      "Training Observation #2900: Loss = 0.016388359\n",
      "Training Observation #2910: Loss = 0.07591569\n",
      "Training Observation #2920: Loss = 0.0058226865\n",
      "Training Observation #2930: Loss = 0.001305698\n",
      "Training Observation #2940: Loss = 0.1976055\n",
      "Training Observation #2950: Loss = 0.06628454\n",
      "Training Observation #2960: Loss = 0.00019768006\n",
      "Training Observation #2970: Loss = 0.5359801\n",
      "Training Observation #2980: Loss = 0.0016019901\n",
      "Training Observation #2990: Loss = 0.00025843983\n",
      "Training Observation #3000: Loss = 0.08343232\n",
      "Training Observation #3010: Loss = 0.00066822045\n",
      "Training Observation #3020: Loss = 1.6541978\n",
      "Training Observation #3030: Loss = 0.005574709\n",
      "Training Observation #3040: Loss = 5.581846e-07\n",
      "Training Observation #3050: Loss = 0.00012220442\n",
      "Training Observation #3060: Loss = 1.1937019e-06\n",
      "Training Observation #3070: Loss = 0.00012361843\n",
      "Training Observation #3080: Loss = 0.0076419157\n",
      "Training Observation #3090: Loss = 0.005341984\n",
      "Training Observation #3100: Loss = 0.8267073\n",
      "Training Observation #3110: Loss = 5.1293716\n",
      "Training Observation #3120: Loss = 0.00013407813\n",
      "Training Observation #3130: Loss = 0.0066444413\n",
      "Training Observation #3140: Loss = 0.0010284212\n",
      "Training Observation #3150: Loss = 0.00049784075\n",
      "Training Observation #3160: Loss = 7.891656e-06\n",
      "Training Observation #3170: Loss = 0.00072069623\n",
      "Training Observation #3180: Loss = 1.7555588e-07\n",
      "Training Observation #3190: Loss = 0.5334617\n",
      "Training Observation #3200: Loss = 0.008252261\n",
      "Training Observation #3210: Loss = 0.008559004\n",
      "Training Observation #3220: Loss = 2.5876143\n",
      "Training Observation #3230: Loss = 1.4013131\n",
      "Training Observation #3240: Loss = 3.7977593\n",
      "Training Observation #3250: Loss = 0.00016243212\n",
      "Training Observation #3260: Loss = 1.7530838e-06\n",
      "Training Observation #3270: Loss = 0.0058139977\n",
      "Training Observation #3280: Loss = 2.3824574e-05\n",
      "Training Observation #3290: Loss = 0.0024056288\n",
      "Training Observation #3300: Loss = 0.0041972846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #3310: Loss = 0.014718291\n",
      "Training Observation #3320: Loss = 0.084168226\n",
      "Training Observation #3330: Loss = 0.0061082263\n",
      "Training Observation #3340: Loss = 0.02248032\n",
      "Training Observation #3350: Loss = 0.009690013\n",
      "Training Observation #3360: Loss = 0.0006971913\n",
      "Training Observation #3370: Loss = 0.106514975\n",
      "Training Observation #3380: Loss = 0.0053786226\n",
      "Training Observation #3390: Loss = 0.0024281957\n",
      "Training Observation #3400: Loss = 0.0019351918\n",
      "Training Observation #3410: Loss = 0.0020079715\n",
      "Training Observation #3420: Loss = 7.877569e-05\n",
      "Training Observation #3430: Loss = 0.057005472\n",
      "Training Observation #3440: Loss = 0.0074972487\n",
      "Training Observation #3450: Loss = 0.1799813\n",
      "Training Observation #3460: Loss = 0.002305804\n",
      "Training Observation #3470: Loss = 0.1676617\n",
      "Training Observation #3480: Loss = 5.1204665e-06\n",
      "Training Observation #3490: Loss = 0.009339727\n",
      "Training Observation #3500: Loss = 2.559186\n",
      "Training Observation #3510: Loss = 2.5785896e-05\n",
      "Training Observation #3520: Loss = 0.03571123\n",
      "Training Observation #3530: Loss = 0.0022931849\n",
      "Training Observation #3540: Loss = 1.3641851\n",
      "Training Observation #3550: Loss = 0.27508187\n",
      "Training Observation #3560: Loss = 1.9318277e-05\n",
      "Training Observation #3570: Loss = 0.005249234\n",
      "Training Observation #3580: Loss = 0.28927457\n",
      "Training Observation #3590: Loss = 0.02216272\n",
      "Training Observation #3600: Loss = 1.31797e-07\n",
      "Training Observation #3610: Loss = 0.00020824438\n",
      "Training Observation #3620: Loss = 9.9263234e-05\n",
      "Training Observation #3630: Loss = 0.0013061808\n",
      "Training Observation #3640: Loss = 0.0029934642\n",
      "Training Observation #3650: Loss = 0.0008858849\n",
      "Training Observation #3660: Loss = 0.0009738388\n",
      "Training Observation #3670: Loss = 0.00063284277\n",
      "Training Observation #3680: Loss = 2.5589368e-06\n",
      "Training Observation #3690: Loss = 0.00048034207\n",
      "Training Observation #3700: Loss = 0.014494015\n",
      "Training Observation #3710: Loss = 0.0024688032\n",
      "Training Observation #3720: Loss = 0.0033550456\n",
      "Training Observation #3730: Loss = 0.5120262\n",
      "Training Observation #3740: Loss = 0.37374708\n",
      "Training Observation #3750: Loss = 0.0011856856\n",
      "Training Observation #3760: Loss = 4.569486\n",
      "Training Observation #3770: Loss = 0.00084326474\n",
      "Training Observation #3780: Loss = 2.7987692\n",
      "Training Observation #3790: Loss = 0.1009001\n",
      "Training Observation #3800: Loss = 0.00027454706\n",
      "Training Observation #3810: Loss = 0.016911307\n",
      "Training Observation #3820: Loss = 1.5214325e-05\n",
      "Training Observation #3830: Loss = 0.7391878\n",
      "Training Observation #3840: Loss = 0.0004906171\n",
      "Training Observation #3850: Loss = 0.014305618\n",
      "Training Observation #3860: Loss = 0.009820453\n",
      "Training Observation #3870: Loss = 0.0069423425\n",
      "Training Observation #3880: Loss = 0.17667413\n",
      "Training Observation #3890: Loss = 9.202172e-05\n",
      "Training Observation #3900: Loss = 4.7679464e-06\n",
      "Training Observation #3910: Loss = 2.950886\n",
      "Training Observation #3920: Loss = 0.12105178\n",
      "Training Observation #3930: Loss = 4.4571114\n",
      "Training Observation #3940: Loss = 0.00064242556\n",
      "Training Observation #3950: Loss = 0.008321595\n",
      "Training Observation #3960: Loss = 2.1430133e-05\n",
      "Training Observation #3970: Loss = 0.0015100056\n",
      "Training Observation #3980: Loss = 0.005554242\n",
      "Training Observation #3990: Loss = 0.0008605046\n",
      "Training Observation #4000: Loss = 0.0007255297\n",
      "Training Observation #4010: Loss = 0.13980256\n",
      "Training Observation #4020: Loss = 0.08060011\n",
      "Training Observation #4030: Loss = 0.0050095264\n",
      "Training Observation #4040: Loss = 0.005963135\n",
      "Training Observation #4050: Loss = 0.22159725\n",
      "Training Observation #4060: Loss = 0.9730949\n",
      "Training Observation #4070: Loss = 0.003437046\n",
      "Training Observation #4080: Loss = 4.646165e-05\n",
      "Training Observation #4090: Loss = 4.25155\n",
      "Training Observation #4100: Loss = 0.0015966224\n",
      "Training Observation #4110: Loss = 1.5376741\n",
      "Training Observation #4120: Loss = 0.0009859392\n",
      "Training Observation #4130: Loss = 0.0036204674\n",
      "Training Observation #4140: Loss = 8.385119\n",
      "Training Observation #4150: Loss = 1.619474e-05\n",
      "Training Observation #4160: Loss = 0.0095103765\n",
      "Training Observation #4170: Loss = 4.9373984\n",
      "Training Observation #4180: Loss = 0.19886906\n",
      "Training Observation #4190: Loss = 0.020356162\n",
      "Training Observation #4200: Loss = 0.003963433\n",
      "Training Observation #4210: Loss = 0.00024470373\n",
      "Training Observation #4220: Loss = 0.43490928\n",
      "Training Observation #4230: Loss = 0.07295917\n",
      "Training Observation #4240: Loss = 2.1568506\n",
      "Training Observation #4250: Loss = 0.108072594\n",
      "Training Observation #4260: Loss = 1.2471127e-06\n",
      "Training Observation #4270: Loss = 4.9677812e-05\n",
      "Training Observation #4280: Loss = 2.6459637e-05\n",
      "Training Observation #4290: Loss = 8.234376e-05\n",
      "Training Observation #4300: Loss = 0.11955583\n",
      "Training Observation #4310: Loss = 8.786737\n",
      "Training Observation #4320: Loss = 0.11010691\n",
      "Training Observation #4330: Loss = 0.04232809\n",
      "Training Observation #4340: Loss = 0.0015611079\n",
      "Training Observation #4350: Loss = 0.010474167\n",
      "Training Observation #4360: Loss = 0.000116763724\n",
      "Training Observation #4370: Loss = 6.4285183\n",
      "Training Observation #4380: Loss = 0.0001247364\n",
      "Training Observation #4390: Loss = 5.025926e-05\n",
      "Training Observation #4400: Loss = 0.00066448504\n",
      "Training Observation #4410: Loss = 0.026123138\n",
      "Training Observation #4420: Loss = 0.002733834\n",
      "Training Observation #4430: Loss = 4.657316\n",
      "Training Observation #4440: Loss = 0.00035335144\n",
      "Training Observation #4450: Loss = 8.117642e-05\n",
      "Getting Test Set Accuracy For 1115 Sentences.\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy: 0.8493273542600897\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Working with Bag of Words\n",
    "#---------------------------------------\n",
    "#\n",
    "# In this example, we will download and preprocess the ham/spam\n",
    "#  text data.  We will then use a one-hot-encoding to make a\n",
    "#  bag of words set of features to use in logistic regression.\n",
    "#\n",
    "# We will use these one-hot-vectors for logistic regression to\n",
    "#  predict if a text is spam or ham.\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Check if data was downloaded, otherwise download it and save for future use\n",
    "save_file_name = os.path.join('/home/karen/workspace/Coursera_Assignments/NLP/miscellaneous/datasets','temp_spam_data.csv')\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, 'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    \n",
    "    # And write to csv\n",
    "    with open(save_file_name, 'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "\n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "\n",
    "# Relabel 'spam' as 1, 'ham' as 0\n",
    "target = [1 if x=='spam' else 0 for x in target]\n",
    "\n",
    "# Normalize text\n",
    "# Lower case\n",
    "texts = [x.lower() for x in texts]\n",
    "\n",
    "# Remove punctuation\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "# Remove numbers\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "# Trim extra whitespace\n",
    "texts = [' '.join(x.split()) for x in texts]\n",
    "\n",
    "# Plot histogram of text lengths\n",
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "\n",
    "# Choose max text word length at 25\n",
    "sentence_size = 25\n",
    "min_word_freq = 3\n",
    "\n",
    "# Setup vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "\n",
    "# Have to fit transform to get length of unique words.\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "# Split up data set into train/test\n",
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]\n",
    "\n",
    "# Setup Index Matrix for one-hot-encoding\n",
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
    "\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "\n",
    "# Text-Vocab Embedding\n",
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)\n",
    "\n",
    "# Declare model operations\n",
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output,\n",
    "                                                              labels=y_target))\n",
    "\n",
    "# Prediction operation\n",
    "prediction = tf.sigmoid(model_output)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Start Logistic Regression\n",
    "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    \n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    if (ix+1)%10==0:\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + str(temp_loss))\n",
    "        \n",
    "    # Keep trailing average of past 50 observations accuracy\n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    # Get True/False if prediction is accurate\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))\n",
    "\n",
    "# Get test set accuracy\n",
    "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\n",
    "test_acc_all = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    \n",
    "    if (ix+1)%50==0:\n",
    "        print('Test Observation #' + str(ix+1))    \n",
    "    \n",
    "    # Keep trailing average of past 50 observations accuracy\n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    # Get True/False if prediction is accurate\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "\n",
    "print('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n",
    "\n",
    "# Plot training accuracy over time\n",
    "plt.plot(range(len(train_acc_avg)), train_acc_avg, 'k-', label='Train Accuracy')\n",
    "plt.title('Avg Training Acc Over Past 50 Generations')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Generation # 500. Train Loss (Test Loss): 0.87 (0.85). Train Acc (Test Acc): 0.38 (0.39)\n",
      "Generation # 1000. Train Loss (Test Loss): 0.70 (0.69). Train Acc (Test Acc): 0.58 (0.63)\n",
      "Generation # 1500. Train Loss (Test Loss): 0.61 (0.60). Train Acc (Test Acc): 0.66 (0.71)\n",
      "Generation # 2000. Train Loss (Test Loss): 0.59 (0.55). Train Acc (Test Acc): 0.74 (0.77)\n",
      "Generation # 2500. Train Loss (Test Loss): 0.55 (0.51). Train Acc (Test Acc): 0.78 (0.80)\n",
      "Generation # 3000. Train Loss (Test Loss): 0.58 (0.49). Train Acc (Test Acc): 0.76 (0.81)\n",
      "Generation # 3500. Train Loss (Test Loss): 0.50 (0.48). Train Acc (Test Acc): 0.83 (0.82)\n",
      "Generation # 4000. Train Loss (Test Loss): 0.47 (0.47). Train Acc (Test Acc): 0.82 (0.83)\n",
      "Generation # 4500. Train Loss (Test Loss): 0.48 (0.46). Train Acc (Test Acc): 0.82 (0.83)\n",
      "Generation # 5000. Train Loss (Test Loss): 0.45 (0.46). Train Acc (Test Acc): 0.85 (0.83)\n",
      "Generation # 5500. Train Loss (Test Loss): 0.49 (0.46). Train Acc (Test Acc): 0.81 (0.83)\n",
      "Generation # 6000. Train Loss (Test Loss): 0.43 (0.45). Train Acc (Test Acc): 0.86 (0.84)\n",
      "Generation # 6500. Train Loss (Test Loss): 0.38 (0.45). Train Acc (Test Acc): 0.87 (0.84)\n",
      "Generation # 7000. Train Loss (Test Loss): 0.41 (0.45). Train Acc (Test Acc): 0.87 (0.85)\n",
      "Generation # 7500. Train Loss (Test Loss): 0.40 (0.45). Train Acc (Test Acc): 0.86 (0.85)\n",
      "Generation # 8000. Train Loss (Test Loss): 0.43 (0.45). Train Acc (Test Acc): 0.86 (0.85)\n",
      "Generation # 8500. Train Loss (Test Loss): 0.38 (0.45). Train Acc (Test Acc): 0.88 (0.85)\n",
      "Generation # 9000. Train Loss (Test Loss): 0.38 (0.45). Train Acc (Test Acc): 0.88 (0.85)\n",
      "Generation # 9500. Train Loss (Test Loss): 0.42 (0.45). Train Acc (Test Acc): 0.84 (0.85)\n",
      "Generation # 10000. Train Loss (Test Loss): 0.45 (0.45). Train Acc (Test Acc): 0.84 (0.85)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "batch_size = 200\n",
    "max_features = 100\n",
    "\n",
    "# Define tokenizer\n",
    "def tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Create TF-IDF of texts\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)\n",
    "sparse_tfidf_texts = tfidf.fit_transform(texts)\n",
    "\n",
    "# Split up data set into train/test\n",
    "train_indices = np.random.choice(sparse_tfidf_texts.shape[0], round(0.8*sparse_tfidf_texts.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(sparse_tfidf_texts.shape[0])) - set(train_indices)))\n",
    "texts_train = sparse_tfidf_texts[train_indices]\n",
    "texts_test = sparse_tfidf_texts[test_indices]\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n",
    "\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[max_features,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[None, max_features], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Declare logistic model (sigmoid in loss function)\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.0025)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Start Logistic Regression\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "for i in range(10000):\n",
    "    rand_index = np.random.choice(texts_train.shape[0], size=batch_size)\n",
    "    rand_x = texts_train[rand_index].todense()\n",
    "    rand_y = np.transpose([target_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i+1)%100==0:\n",
    "        i_data.append(i+1)\n",
    "        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        train_loss.append(train_loss_temp)\n",
    "        \n",
    "        test_loss_temp = sess.run(loss, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n",
    "        test_loss.append(test_loss_temp)\n",
    "        \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        train_acc.append(train_acc_temp)\n",
    "    \n",
    "        test_acc_temp = sess.run(accuracy, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i+1)%500==0:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(i_data, train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
>>>>>>> 1587d5940474b9d285d86e287bb75e66443a0bd7
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import codecs, random\n",
    "\n",
    "PATH = '/home/karen/workspace/Datasci_Fall_18/ds200/Labs/others/CharLSTM/'\n",
    "TRAIN_SET = '/home/karen/Downloads/data/training.1600000.processed.noemoticon.csv'\n",
    "VALID_PERC = 0.05\n",
    "\n",
    "def reshape_lines(lines):\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        print(l)\n",
    "        split = l.split(',')\n",
    "        data.append((split[0][1:], split[-1][:-2]))\n",
    "    return data\n",
    "\n",
    "def shuffle_datasets(valid_perc=VALID_PERC):\n",
    "    with codecs.open(TRAIN_SET, 'r', 'latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        lines = [l.encode('utf-8') for l in lines]\n",
    "        lines_train = lines[:int(len(lines) * (1 - valid_perc))]\n",
    "        lines_valid = lines[int(len(lines) * (1 - valid_perc)):]\n",
    "    save_csv(PATH + 'datasets/valid_set.csv', reshape_lines(lines_valid))\n",
    "\n",
    "shuffle_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GroupLens](https://grouplens.org/datasets/hetrec-2011/)\n",
    "\n",
    "[Sentiment140](http://help.sentiment140.com/for-students/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](https://www.zhihu.com/question/50043438)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
