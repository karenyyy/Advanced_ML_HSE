{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Language Representation Models Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/16.png)\n",
    "\n",
    "\n",
    "回顾过去基于深度学习的 NLP 任务可以发现，几乎绝大多数都比较符合这三层概念。比如很多生成任务的 Seq2Seq 框架中不外乎都有一个 Encoder 和一个 Decoder。对应到这里，__Decoder 更像是一个 Task-specific Model，然后相应的将 Encoder 做一些细微调整，比如引入 Attention 机制等等__\n",
    "\n",
    "- Eg:\n",
    "    - 对于一些文本分类任务的结构，则 Encoder 模块与 Task-specific Model 模块的区分更为明显和清晰，Encoder 层可以作为一个相对比较通用的模块来使用, Encoder 负责提取文本特征，最后接上一些全连接层和 Softmax 层便可以当做 Task-specific Model 模块，便完成了一个文本分类任务\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVD+LSA -> PLSA -> LDA (introduce prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/1.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/2.png)\n",
    "\n",
    "\n",
    "### Here:\n",
    "\n",
    "$$PPMI = \\log (lift(or: interest)) = \\log \\frac{p(x, y)}{p(x)p(y)}$$\n",
    "\n",
    "![](../../images/week3/3.png)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/9.png)\n",
    "\n",
    "![](../../images/week3/10.png)\n",
    "\n",
    "![](../../images/week3/11.png)\n",
    "\n",
    "![](../../images/week3/12.png)\n",
    "\n",
    "![](../../images/week3/13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/17.png)\n",
    "\n",
    "![](../../images/week3/21.png)\n",
    "\n",
    "\n",
    "### NNLM\n",
    "\n",
    "![](../../images/week3/18.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/24.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### CBOW\n",
    "\n",
    "\n",
    "![](../../images/week3/19.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/26.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SKIP-GRAM\n",
    "\n",
    "\n",
    "![](../../images/week3/20.png)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/25.png)\n",
    "\n",
    "\n",
    "\n",
    "### Expansion of word2vec\n",
    "\n",
    "#### PV-DM (CBOW of paragraphs) &  PV-DBOW (Skip-Gram of paragraphs) \n",
    "\n",
    "\n",
    "![](../../images/week3/22.png)\n",
    "\n",
    "\n",
    "### Skip-Thoughts\n",
    "\n",
    "Skip-thoughts 直接在句子间进行预测，也就是__将 Skip-gram 中以词为基本单位，替换成了以句子为基本单位__，具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句\n",
    "\n",
    "\n",
    "对于句子的建模利用的 RNN 的 sequence 结构，预测上一个和下一个句子时候，也是利用的一个 sequence 的 RNN 来生成句子中的每一个词，所以这个结构本质上就是一个 Encoder-Decoder 框架，只不过和普通框架不一样的是，Skip-thoughts 有两个 Decoder\n",
    "\n",
    "- future works:\n",
    "    - 输入的 Encoder 可以引入 attention 机制, 从而让 Decoder 的输入不再只是依赖 Encoder 最后一个时刻的输出\n",
    "    - Encoder 和 Decoder 可以利用更深层的结构\n",
    "    - Decoder 也可以继续扩大，可以预测上下文中更多的句子\n",
    "    - RNN 也不是唯一的选择，诸如 CNN 以及 2017 年谷歌提出的 Transformer 结构也可以利用进来\n",
    "\n",
    "- major drawbacks\n",
    "    - Skip-thoughts 的 Decoder 效率太低\n",
    "    - 无法在大规模语料上很好的训练\n",
    "    \n",
    "    \n",
    "### Quick-Thoughts\n",
    "\n",
    "Skip-thoughts 的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例，并将这些句子对输入模型，让模型判断这些句子对是否是同一个上下文窗口中，很明显，这是一个分类任务\n",
    "    \n",
    "    \n",
    "![](../../images/week3/23.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/4.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/5.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/6.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/7.png)\n",
    "\n",
    "![](../../images/week3/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### GloVe Paper Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "![](../../images/week3/14.png)\n",
    "\n",
    "![](../../images/week3/15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CoVe (Contextualized Word Vectors, outputs of the MT-LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Learned in Translation: Contextualized Word Vectors (notes later)] 论文首先用一个 Encoder-Decoder 框架在机器翻译的训练语料上进行预训练，而后用训练好的模型，只取其中的 Embedding 层和 Encoder 层，同时在一个新的任务上设计一个 task-specific 模型，再将原先预训练好的 Embedding 层和 Encoder 层的输出作为这个 task-specific 模型的输入，最终在新的任务场景下进行训练.\n",
    "\n",
    "\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "![](../../images/week3/28.png)\n",
    "![](../../images/week3/29.png)\n",
    "\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "![](../../images/week3/30.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Attention Mechanism\n",
    "\n",
    "为了决定下一步翻译英语句子中的哪一部分，注意力机制需要从隐向量向前回溯。它使用状态向量来判别每一个隐向量的重要性，为了记录它的观察值，注意力机制会生成一个新的向量，我们可以称之为语境调整状态（context-sdjusted state）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/31.png)\n",
    "\n",
    "然后，生成器会根据语境调整状态来决定要生成哪个单词，接下来语境调整状态会回传到解码器中，让解码器对其翻译的结果有一个准确的感知。解码器一直重复这个过程，直至它完成所有翻译。这就是一个标准的基于注意力机制的编码器-解码器结构，它被用来学习像机器翻译一样的序列到序列任务。\n",
    "![](../../images/week3/32.png)\n",
    "\n",
    "当训练过程结束之后，将训练好的 LSTM 提取出来作为编码器用于机器翻译。我们将这个预训练的 LSTM 称作机器翻译 LSTM（MT-LSTM），并使用它生成新句子的隐向量。当我们把这些机器翻译隐向量用于其它的自然语言处理模型时，我们就把它们称作__语境向量(CoVe)__(CoVe 可以被用在任何将向量序列作为输入的模型中)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/week3/27.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### CoVe Paper Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/week3/33.png)\n",
    "\n",
    "![](../../images/week3/34.png)\n",
    "\n",
    "![](../../images/week3/35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### CoVe Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Attention notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Seq2seq rnn-based model (without attention):\n",
    "\n",
    "![](../../images/week3/36.jpg)\n",
    "![](../../images/week3/40.jpg)\n",
    "\n",
    "\n",
    "\n",
    "- with attention\n",
    "\n",
    "![](../../images/week3/41.jpg)\n",
    "\n",
    "在该模型中，定义了一个条件概率：\n",
    "\n",
    "![](../../images/week3/42.jpg)\n",
    "\n",
    "其中，$s_i$是decoder中RNN在在i时刻的隐状态\n",
    "\n",
    "![](../../images/week3/43.jpg)\n",
    "\n",
    "背景向量ci的计算方式，与传统的Seq2Seq模型直接累加的计算方式不一样，这里的ci是一个权重化（Weighted）之后的值:\n",
    "\n",
    "![](../../images/week3/44.jpg)\n",
    "\n",
    "$h_j$ 表示encoder端的第j个词的隐向量，$a_{ij}$表示encoder端的第j个词与decoder端的第i个词之间的权值，表示源端第j个词对目标端第i个词的影响程度\n",
    "\n",
    "$a_{ij}$的计算公式:\n",
    "\n",
    "![](../../images/week3/45.jpg)\n",
    "![](../../images/week3/52.png)\n",
    "\n",
    "\n",
    "$e_{ij}$ 表示一个对齐模型，用于衡量encoder端的位置j个词，对于decoder端的位置i个词的对齐程度（影响程度）(i.e. decoder端生成位置i的词时，有多少程度受encoder端的位置j的词影响)\n",
    "\n",
    "对齐模型eij的计算方式有很多种，不同的计算方式，代表不同的Attention模型，最简单且最常用的的对齐模型是dot product乘积矩阵，即把target端的输出隐状态ht与source端的输出隐状态进行矩阵乘\n",
    "\n",
    "![](../../images/week3/46.jpg)\n",
    "\n",
    "\n",
    "![](../../images/week3/37.jpg)\n",
    "\n",
    "\n",
    "![](../../images/week3/38.jpg)\n",
    "\n",
    "\n",
    "权重 $\\alpha$ 是怎么来的呢？常见有三种方法：\n",
    "\n",
    "\n",
    "- $\\alpha_{0}^1=cos\\_sim(z_0, h_1)$\n",
    "- $\\alpha_0 =neural\\_network(z_0, h)$\n",
    "- $\\alpha_0 = h^TWz_0$\n",
    "\n",
    "\n",
    "思想就是根据当前解码“状态”判断输入序列的权重分布\n",
    "\n",
    "\n",
    "attention其实是以下的机制:\n",
    "\n",
    "![](../../images/week3/39.jpg)\n",
    "\n",
    "模型通过Q和K的匹配计算出权重，再结合V得到输出：\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(sim(Q, K))V$$\n",
    "\n",
    "\n",
    "- 模型分类\n",
    "    - Soft/Hard Attention\n",
    "        - soft attention：传统attention，可被嵌入到模型中去进行训练并传播梯度\n",
    "        - hard attention：不计算所有输出，依据概率对encoder的输出采样，在反向传播时需采用蒙特卡洛进行梯度估计\n",
    "    - Global/Local Attention\n",
    "        - global attention：传统attention，对所有encoder输出进行计算\n",
    "            - 传统的Attention model一样。所有的hidden state都被用于计算Context vector 的权重，即变长的对齐向量at，其长度等于encoder端输入句子的长度\n",
    "            ![](../../images/week3/47.jpg)\n",
    "            在t时刻，首先基于decoder的隐状态ht和源端的隐状态hs，计算一个变长的隐对齐权值向量$a_t$\n",
    "            ![](../../images/week3/48.jpg)\n",
    "            得到对齐向量$a_t$之后，就可以通过加权平均的方式，得到上下文向量$c_t$\n",
    "        - local attention：介于soft和hard之间，会预测一个位置并选取一个窗口进行计算\n",
    "            - ![](../../images/week3/49.jpg)\n",
    "            - Local Attention首先会为decoder端当前的词，预测一个source端对齐位置（aligned position）$p_t$，然后基于$p_t$选择一个窗口，用于计算背景向量$c_t$\n",
    "            ![](../../images/week3/50.jpg)\n",
    "            - S是encoder端句子长度，vp和wp是模型参数, 此时，对齐向量at的计算公式:\n",
    "            ![](../../images/week3/51.jpg)\n",
    "    - Self Attention\n",
    "        - ![](../../images/week3/50.png)\n",
    "        - Self-attention 中的multiple-heads mechanism便是将这样的操作分别进行多次，让句子的表征充分学习到不同的侧重点，最终将这些多头学习出来的表征 concat 到一起，然后再同一个全连接网络，便可以得到这个句子最终 Self-attention 下新的表示, 其中的每一个头的操作过程用公式表示如下，需要注意的是 softmax 是针对矩阵的 row 方向进行操作得到的。所以，说白了，这个公式表示的意思就是针对 V 进行加权求和，加权权值通过 Q 和 K 的点乘得到:\n",
    "        - ![](../../images/week3/51.png)\n",
    "        - 这里给出一例，下图只是两个 head 学习到的交融模式，如果多达 16 个 head，这样的交融模式还要重复16次 (而相应的在 ELMo 与 GPT 中，它们并没有用上这种交融模式，也就是它们本质上还是一个单向的模型，ELMo 稍微好一点，将两个单向模型的信息 concat起 来。GPT 则只用了单向模型，这是因为它没有用上 Transformer Encoder、只用了 Decdoer 的天生基因决定的):\n",
    "        - ![](../../images/week3/55.png)\n",
    "        - 传统attention是计算Q和K之间的依赖关系，__而self attention则分别计算Q和K自身的依赖关系__\n",
    "        - Self Attention 分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系\n",
    "        - Self Attention 的具体计算方式如图所示:\n",
    "        ![](../../images/week3/52.jpg)\n",
    "       - 从All Attention的结构示意图可以发现，Encoder和decoder是层叠多了类似的__Multi-Head Attention__单元构成，而每一个Multi-Head Attention单元由多个结构相似的__Scaled Dot-Product Attention__单元组成\n",
    "        ![](../../images/week3/53.jpg)\n",
    "        - Self Attention也是在Scaled Dot-Product Attention单元里面实现的\n",
    "            - 首先把输入Input经过线性变换分别得到Q、K、V\n",
    "            - 然后把Q和K做dot Product相乘，得到输入Input词与词之间的依赖关系\n",
    "            - 然后经过尺度变换（scale）、掩码（mask）和softmax操作，得到最终的Self Attention矩阵 (尺度变换是为了防止输入值过大导致训练不稳定，mask则是为了保证时间的先后关系)\n",
    "            - 最后，把encoder端self Attention计算的结果加入到decoder做为k和V，结合decoder自身的输出做为q，得到encoder端的attention与decoder端attention之间的依赖关系\n",
    "    - Other Attention\n",
    "        - __Hierarchical Attention__构建了两个层次的Attention Mechanism，第一个层次是对句子中每个词的attention，即word attention；第二个层次是针对文档中每个句子的attention，即sentence attention\n",
    "         ![](../../images/week3/54.jpg)\n",
    "        - __Attention over Attention__\n",
    "            - 两个输入，一个Document和一个Query，分别用一个双向的RNN进行特征抽取，得到各自的隐状态h（doc）和h（query)\n",
    "            - 然后基于query和doc的隐状态进行dot product，得到query和doc的attention关联矩阵\n",
    "            - 然后按列（column）方向进行softmax操作，得到query-to-document的attention 值a（t）\n",
    "            - 按照行（row）方向进行softmax操作，得到document-to-query的attention值b（t）\n",
    "            - 再进行attention操作，即attention over attention得到最终query与document的关联矩阵\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "OpenAI Transformer是一类可迁移到多种NLP任务的，基于Transformer的语言模型。它的基本思想同ULMFiT相同，都是在尽量不改变模型结构的情况下__将预训练的语言模型应用到各种任务__。不同的是，OpenAI Transformer主张用Transformer结构，而ULMFiT中使用的是基于RNN的语言模型\n",
    "\n",
    "\n",
    "![](../../images/week3/64.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Besides _Self Attention_, 在 Transformer 的 Encoder 中，还有一些其他设计，比如:\n",
    "- 加入 position embedding（因为 Transformer 的 Encoder 中不是时序输入词序列，因此 __position embedding 也是主要位置信息__);\n",
    "- Residual 结构，使得模型的训练过程更为平稳;\n",
    "- normalization 层\n",
    "- feed forward 层（本质上是一个两层的全连接网络，中间加一个 ReLu 的激活函数）\n",
    "\n",
    "Decoder 的结构与此类似，只不过在进行 decode 的时候，会__将 Encoder 这边的输出作为 Decoder 中 Self-attention 时的 K 和 V__\n",
    "\n",
    "![](../../images/week3/53.png)\n",
    "\n",
    "\n",
    "对于 decode 过程，具体来看，大致过程如下:\n",
    "\n",
    "![](../../images/week3/54.png)\n",
    "\n",
    "![](../../images/week3/1.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Seq2Seq Attention TF source code (from scratch later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__tf.nn.seq2seq__文件共实现了5个seq2seq函数\n",
    "\n",
    "- __1. basic_rnn_seq2seq__: 最简单版本\n",
    "    - _输入和输出都是embedding的形式_;\n",
    "    - 最后一步的state vector作为decoder的initial state;\n",
    "    - encoder和decoder用相同的RNN cell， 但不共享权值参数\n",
    "- __2. tied_rnn_seq2seq__: 同1，但是encoder和decoder共享权值参数\n",
    "- __3. embedding_rnn_seq2seq__: 同1，但输入和输出改为id的形式，函数会在内部创建分别用于encoder和decoder的embedding matrix\n",
    "- __4. embedding_tied_rnn_seq2seq__: 同2，但输入和输出改为id形式，函数会在内部创建分别用于encoder和decoder的embedding matrix\n",
    "- __5. embedding_attention_seq2seq__: 同3，但多了attention机制\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tf.nn.seq2seq.embedding_attention_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# T代表time_steps, 时序长度\n",
    "def embedding_attention_seq2seq(encoder_inputs,  # [T, batch_size], int32 id tensor list\n",
    "                                decoder_inputs,  # [T, batch_size], int32 id tensor list\n",
    "                                cell,\n",
    "                                num_encoder_symbols,\n",
    "                                num_decoder_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,      # attention的抽头数量\n",
    "                                output_projection=None, #decoder的投影矩阵\n",
    "                                feed_previous=False,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Encoder\n",
    "\n",
    "- 创建了一个embedding matrix\n",
    "- 计算encoder的output和state\n",
    "- 生成attention states，用于计算attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = rnn_cell.EmbeddingWrapper(      \n",
    "                                        cell, \n",
    "                                        embedding_classes=num_encoder_symbols, # 编码的符号数，即词表大小\n",
    "                                        embedding_size=embedding_size) # 词向量的维度\n",
    "\n",
    "encoder_outputs, encoder_state = rnn.rnn(\n",
    "                                        encoder_cell, \n",
    "                                        encoder_inputs, \n",
    "                                        dtype=dtype) #  [T，batch_size，size]\n",
    "\n",
    "top_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]    # T * [batch_size, 1, size]\n",
    "\n",
    "attention_states = array_ops.concat(1, top_states) # [batch_size,T,size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "上面的EmbeddingWrapper, 是RNNCell的前面加一层embedding，作为encoder_cell, input就可以是word的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingWrapper(RNNCell):\n",
    "    def __init__(self, cell, embedding_classes, embedding_size, initializer=None):\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "      #生成embedding矩阵[embedding_classes,embedding_size]\n",
    "      #inputs: [batch_size, 1]\n",
    "      #return : (output, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decoder\n",
    "\n",
    "- 生成decoder的cell，通过OutputProjectionWrapper类对输入参数中的cell实例包装实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decoder.\n",
    "    output_size = None\n",
    "    if output_projection is None:\n",
    "      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "      output_size = num_decoder_symbols\n",
    "    if isinstance(feed_previous, bool):\n",
    "      return embedding_attention_decoder(\n",
    "          ...\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "上面的OutputProjectionWrapper将输出映射成想要的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OutputProjectionWrapper(RNNCell):\n",
    "    def __init__(self, cell, output_size): # output_size:映射后的size\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "      #init 返回一个带output projection的 rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_attention_decoder(decoder_inputs,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                num_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_size=None,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                update_embedding_for_previous=True,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False):\n",
    "    # 第一步创建了解码用的embedding\n",
    "    embedding = variable_scope.get_variable(\"embedding\",\n",
    "                                            [num_symbols, embedding_size])\n",
    "\n",
    "    # 第二步创建了一个循环函数loop_function，用于将上一步的输出映射到词表空间，输出一个word embedding作为下一步的输入\n",
    "    loop_function = _extract_argmax_and_embed(\n",
    "        embedding, output_projection,\n",
    "        update_embedding_for_previous) if feed_previous else None\n",
    "    emb_inp = [\n",
    "        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n",
    "    # T * [batch_size, embedding_size]\n",
    "    return attention_decoder(\n",
    "        emb_inp,\n",
    "        initial_state,\n",
    "        attention_states,\n",
    "        cell,\n",
    "        output_size=output_size,\n",
    "        num_heads=num_heads,\n",
    "        loop_function=loop_function,\n",
    "        initial_state_attention=initial_state_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs,    #T * [batch_size, input_size]\n",
    "                      initial_state,     #[batch_size, cell.states]\n",
    "                      attention_states,  #[batch_size, attn_length , attn_size]\n",
    "                      cell,\n",
    "                      output_size=None,\n",
    "                      num_heads=1,\n",
    "                      loop_function=None,\n",
    "                      dtype=None,\n",
    "                      scope=None,\n",
    "                      initial_state_attention=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__num_heads__:\n",
    "\n",
    "![](../../images/week3/37.png)\n",
    "\n",
    "attention就是对信息的加权求和，一个attention head对应了一种加权求和方式，这个参数定义了用多少个attention head去加权求和，所以公式三可以进一步表述为$\\sum^{num\\_heads}_{j=1}\\sum^{T_{A}}_{i=1}a_{i,j}h_{i}$\n",
    "\n",
    "- $W_{1}*h_{i}$用的是卷积的方式实现，返回的tensor的形状是[batch_size, attn_length, 1, attention_vec_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To calculate W1 * h_t we use a 1-by-1 convolution\n",
    "hidden = array_ops.reshape(attention_states, \n",
    "                           [-1, attn_length, 1, attn_size])\n",
    "hidden_features = []\n",
    "v = []\n",
    "attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "\n",
    "for a in xrange(num_heads):\n",
    "    k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "                                    [1, 1, attn_size, attention_vec_size])\n",
    "    hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "    v.append(\n",
    "          variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $W_{2}*d_{t}$，此项是通过下面的线性映射函数linear实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for a in xrange(num_heads):\n",
    "        with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "          # query对应当前隐层状态d_t\n",
    "          y = linear(query, attention_vec_size, True)\n",
    "          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "          # 计算u_t\n",
    "          s = math_ops.reduce_sum(\n",
    "              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "          a = nn_ops.softmax(s)\n",
    "          # 计算 attention-weighted vector d.\n",
    "          d = math_ops.reduce_sum(\n",
    "              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n",
    "              [1, 2])\n",
    "          ds.append(array_ops.reshape(d, [-1, attn_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ELMo notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Goal: 一个预训练的词表示应该能够包含丰富的句法和语义信息，并且能够对多义词进行建模\n",
    "\n",
    "\n",
    "![](../../images/week3/55.jpg)\n",
    "\n",
    "\n",
    "ELMo 利用语言模型来获得一个上下文相关的预训练表示:\n",
    "\n",
    "![](../../images/week3/38.png)\n",
    "\n",
    "基本框架是一个双层的 Bi-LSTM，不过在第一层和第二层之间加入了一个残差结构（一般来说，残差结构能让训练过程更稳定)\n",
    "\n",
    "在 ELMo 中使用的是一个双向的 LSTM 语言模型，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然. ELMo 的__基本框架是 2-stacked biLSTM + Residual 的结构__, ELMo 的训练目标函数为:\n",
    "\n",
    "![](../../images/week3/39.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/56.jpg)\n",
    "\n",
    "\n",
    "在预训练好这个语言模型之后，ELMo 就是根据下面的公式来用作词表示，其实就是把这个双向语言模型的每一中间层进行一个求和:\n",
    "\n",
    "![](../../images/week3/57.jpg)\n",
    "\n",
    "__总结一下，不像传统的词向量，每一个词只对应一个词向量，ELMo 利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），再当成特征加入到具体的 NLP 有监督模型里__\n",
    "\n",
    "不过和普通 RNN 结构的不同之处在于，其主要改进在于输入层和输出层不再是 word，而是变为了一个 char-based CNN 结构，ELMo 在输入层和输出层考虑了使用同样的这种结构，该结构如下图示:\n",
    "\n",
    "![](../../images/week3/40.png)\n",
    "\n",
    "\n",
    "_$Note^*$:_\n",
    "\n",
    "输入层和输出层都使用了 CNN 结构\n",
    "\n",
    "\n",
    "在 CBOW 中的普通 Softmax 方法中，为了计算每个词的概率大小，使用的如下公式的计算方法:\n",
    "\n",
    "![](../../images/week3/41.png)\n",
    "\n",
    "现在我们假定 char-based CNN 模型是现成已有的，对于任意一个目标词都可以得到一个向量表示 CNN(tk) ，当前时刻的 LSTM 的输出向量为 h，那么便可以通过同样的方法得到目标词的概率大小:\n",
    "\n",
    "![](../../images/week3/42.png)\n",
    "\n",
    "\n",
    "这种先经过 CNN 得到词向量，然后再计算 Softmax 的方法叫做 CNN Softmax\n",
    "\n",
    "利用 CNN 解决有三点优势值得注意:\n",
    "\n",
    "-  CNN 能减少普通做 Softmax 时全连接层中的必须要有的 $|V|*h$ 的参数规模，只需保持 CNN 内部的参数大小即可。一般来说，CNN 中的参数规模都要比 $|V|*h$ 的参数规模小得多 (CNN Softmax 的好处就在于能够做到对于不同的词，映射参数都是共享的，这个共享便体现在使用的 CNN 中的参数都是同一套，从而大大减少参数的规模)\n",
    "\n",
    "- CNN 可以解决 OOV （Out-of-Vocabulary）问题，这个在翻译问题中尤其头疼\n",
    "\n",
    "- 在预测阶段，CNN 对于每一个词向量的计算可以预先做好，更能够减轻 inference 阶段的计算压力\n",
    "\n",
    "\n",
    "\n",
    "最终 ELMo 的主要结构便如下图（b）所示，可见输入层和输出层都是一个 CNN，中间使用 Bi-LSTM 框架:\n",
    "\n",
    "\n",
    "![](../../images/week3/43.png)\n",
    "\n",
    "$s_j$ 便是针对每一层的输出向量，利用一个 softmax 的参数来学习不同层的权值参数，因为不同任务需要的词语意义粒度也不一致，一般认为浅层的表征比较倾向于句法，而高层输出的向量比较倾向于语义信息。因此通过一个 softmax 的结构让任务自动去学习各层之间的权重\n",
    "\n",
    "![](../../images/week3/44.png)\n",
    "\n",
    "\n",
    "![](../../images/week3/45.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMo source code and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将ELMo向量  $ELMo_k^{task}$ 与传统的词向量  $x_{k}$ 拼接成  $[x_{k};ELMo_k^{task}]$  后，输入到对应具体任务的RNN中。\n",
    "- 将ELMo向量放到模型输出部分，与具体任务RNN输出的  $h_{k}$ 拼接成 $[h_{k};ELMo_k^{task}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tf src code](https://github.com/allenai/bilm-tf/tree/master/bilm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Directly use by import from TF-Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/1'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-850bfcc5f106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Instantiate the elmo model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0melmo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mas_module_spec\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnative_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown module spec type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36mload_module_spec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mon\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mhandling\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompressed_module_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0mmodule_def_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_module_proto_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mmodule_def_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_def_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36mget_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \"\"\"\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       raise UnsupportedHandleError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_get_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    465\u001b[0m       raise UnsupportedHandleError(\n\u001b[1;32m    466\u001b[0m           self._create_unsupported_handle_error_msg(handle))\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_unsupported_handle_error_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36mget_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \"\"\"\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       raise UnsupportedHandleError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m_get_module_path\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0;32m--> 105\u001b[0;31m                                     self._lock_file_timeout_sec())\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lock_file_timeout_sec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0murl_opener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoggingHTTPRedirectHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_opener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_uncompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "# Instantiate the elmo model\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), \n",
    "                      signature=\"default\", \n",
    "                      as_dict=True)[\"default\"]\n",
    "\n",
    "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
    "embedding = layers.Lambda(ElmoEmbedding, output_shape=(1024,))(input_text)\n",
    "dense = layers.Dense(256, activation='relu')(embedding)\n",
    "pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ULMFiT是一种有效的NLP迁移学习方法，核心思想是通过精调预训练的语言模型完成其他NLP任务\n",
    "\n",
    "ULMFiT的过程分为三步:\n",
    "\n",
    "\n",
    "![](../../images/week3/47.png)\n",
    "![](../../images/week3/63.jpg)\n",
    "\n",
    "\n",
    "- ASGD(Averaged SGD):\n",
    "    - 是指先将模型训练到一定 epoch，然后再将其后的每一轮权值进行平均后，得到最终的权值\n",
    "    - 普通的 SGD 方法权值更新过程为：\n",
    "        - ![](../../images/week3/48.png)\n",
    "    - ASGD 则把它变成了:\n",
    "        - ![](../../images/week3/49.png)\n",
    "        - 其中 T 是一个阈值，而 K 是总共的迭代次数，把model迭代到第 T 次之后，对该参数在其后的第 T 轮到最后一轮之间的所有值求平均，\n",
    "\n",
    "\n",
    "- 两种fine-tuning方法：\n",
    "    - Discriminative fine-tuning\n",
    "        - 因为网络中不同层可以捕获不同类型的信息，因此在精调时也应该使用不同的learning rate。作者为每一层赋予一个学习率  $\\eta^{l}$ ，实验后发现，首先通过精调模型的最后一层L确定学习率  $\\eta^{L}$ ，再递推地选择上一层学习率进行精调的效果最好，递推公式为:  $\\eta^{l-1} =\\frac{ \\eta^{l}}{2.6}$\n",
    "    - Slanted triangular learning rates (STLR)\n",
    "        - 为了针对特定任务选择参数，理想情况下需要在训练开始时让参数快速收敛到一个合适的区域，之后进行精调。为了达到这种效果，作者提出STLR方法，即让LR在训练初期短暂递增，在之后下降。如上图的右上角所示\n",
    "        - ![](../../images/week3/46.png)\n",
    "        - parameters:\n",
    "            - T: number of training iterations\n",
    "            - cut_frac: fraction of iterations we increase the LR\n",
    "            - cut: the iteration when we switch from increasing to decreasing the LR\n",
    "            - p: the fraction of the number of iterations we have increased or will decrease the LR respectively\n",
    "            - ratio: specifies how much smaller the lowest LR $\\eta_{min}$ is from the max LR  $\\eta_{max}$\n",
    "            - $\\eta_{t}$  : the LR at iteration t\n",
    "            - $Note^*:$ in the paper, $cut\\_frac=1$, ration=32, $\\eta_{max} = 0.01$\n",
    "            \n",
    "            \n",
    "####  Target task classifier fine-tuning\n",
    "\n",
    "\n",
    "为了完成分类任务的精调，作者在最后一层添加了两个线性block:\n",
    "- 每个都有batch-norm和dropout\n",
    "- 使用ReLU作为中间层激活函数\n",
    "- 最后经过softmax输出分类的概率分布\n",
    "\n",
    "\n",
    "\n",
    "Details:\n",
    "\n",
    "- Concat pooling\n",
    "    - 第一个线性层的输入是最后一个隐层状态的池化。因为文本分类的关键信息可能在文本的任何地方，所以只是用最后时间步的输出是不够的。作者将最后时间步   $h_{T}$ 与尽可能多的时间步 $H= {h_{1},... , h_{T}}$ 池化后拼接起来，以  $h_{c} = [h_{T}, maxpool(H), meanpool(H)]$ 作为输入\n",
    "    \n",
    "- Gradual unfreezing\n",
    "    - 由于过度精调会导致模型遗忘之前预训练得到的信息，作者提出逐渐unfreeze网络层的方法，从最后一层开始unfreeze和精调，由后向前地unfreeze并精调所有层\n",
    "    \n",
    "- BPTT for Text Classification (BPT3C) \n",
    "    - 为了在large documents上进行模型精调，作者将文档分为固定长度为b的batches，并在每个batch训练时记录mean和max池化，梯度会被反向传播到对最终预测有贡献的batches\n",
    "    \n",
    "- Bidirectional language model\n",
    "    - 在paper中，分别独立地对前向和后向LM做了精调，并将两者的预测结果平均。两者结合后结果有0.5-0.7的提升。\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### OpenAI GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### GPT notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2018 年早些时候谷歌的 Generating Wikipedia by Summarizing Long Sequences，GPT 名称中的 Generative 便是源自这篇文章，二者都有用到生成式方法来训练模型，也就是生成式 Decoder。\n",
    "\n",
    "\n",
    "\n",
    "训练过程分为两步：\n",
    "\n",
    "- 1. Unsupervised pre-training\n",
    "\n",
    "__主要亮点在于利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构__\n",
    "\n",
    "\n",
    "![](../../images/week3/58.jpg)\n",
    "\n",
    "\n",
    "在具体 NLP 任务有监督微调时，与 ELMo 当成特征的做法不同，OpenAI GPT 不需要再重新对任务构建新的模型结构，而是直接在 Transformer 这个语言模型上的最后一层接上 softmax 作为任务输出层，然后再对这整个模型进行微调\n",
    "\n",
    "![](../../images/week3/59.jpg)\n",
    "\n",
    "- 2. Supervised fine-tuning\n",
    "\n",
    "\n",
    "有了预训练的语言模型之后，对于有标签的训练集 $C$ ，给定输入序列 $x^{1}, ..., x^{m}$ 和标签 $y$，可以通过语言模型得到  $h_{l}^{m}$ ，经过输出层后对  $y$  进行预测\n",
    "\n",
    "\n",
    "$$p(y|x^1, x^2, ..., x^m) = softmax(h_l^m W_y)$$\n",
    "\n",
    "\n",
    "$$L_2(C) = \\sum_{x,y} \\log p(y|x^1, x^2, ..., x^m)$$\n",
    "\n",
    "\n",
    "整个任务的目标函数为:\n",
    "\n",
    "$$L_3(C) = L_2(C)+ \\lambda L_1(C)$$\n",
    "\n",
    "其中 $L_2$ 是 task-specific 的目标函数， $L_1$ 则是语言模型的目标函数\n",
    "\n",
    "Result:\n",
    "\n",
    "- 计算速度比循环神经网络更快，易于并行化\n",
    "- 实验结果显示Transformer的效果比ELMo和LSTM网络更好\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "\n",
    "从Wrod Embedding到OpenAI Transformer，NLP中的迁移学习__从最初使用word2vec、GLoVe进行字词的向量表示，到ELMo可以提供前几层的权重共享，再到ULMFiT和OpenAI Transformer的整个预训练模型的精调__，大大提高了NLP基本任务的效果。同时，多项研究也表明，以语言模型作为预训练模型，不仅可以捕捉到文字间的语法信息，更可以捕捉到语义信息，为后续的网络层提供高层次的抽象信息。另外，基于Transformer的模型在一些方面也展现出了优于RNN模型的效果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### GPT Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     10,
     33,
     44,
     52,
     69,
     91,
     104,
     111
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# source code: github openAI finetune-transformer-lm/train.py\n",
    "\n",
    "# DROPOUT\n",
    "def dropout(x, pdrop, train):\n",
    "    if train and pdrop > 0:\n",
    "        x = tf.nn.dropout(x, \n",
    "                          keep_prob=1-pdrop)\n",
    "    return x\n",
    "\n",
    "# EMBEDDING\n",
    "def embed(X, we):\n",
    "    we = convert_gradient_to_tensor(we)\n",
    "    e = tf.gather(we, X)\n",
    "    h = tf.reduce_sum(e, 2)\n",
    "    return h\n",
    "\n",
    "# CONV1D\n",
    "def conv1d(x, scope, nf, rf, \n",
    "           w_init=tf.random_normal_initializer(stddev=0.02), \n",
    "           b_init=tf.constant_initializer(0), \n",
    "           pad='VALID', \n",
    "           train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [rf, nx, nf], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [nf], initializer=b_init)\n",
    "        if rf == 1: #faster 1x1 conv\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, shape_list(x)[:-1]+[nf]) # c = xw+b\n",
    "        else: #was used to train LM\n",
    "            c = tf.nn.conv1d(x, w, stride=1, padding=pad)+b\n",
    "        return c\n",
    "\n",
    "# FEED FORWARD NN\n",
    "def mlp(x, scope, n_state, train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        act = act_fns[afn]\n",
    "        h = act(conv1d(x, 'c_fc', n_state, 1, train=train))\n",
    "        h2 = conv1d(h, 'c_proj', nx, 1, train=train)\n",
    "        h2 = dropout(h2, resid_pdrop, train)\n",
    "        return h2\n",
    "    \n",
    "# MULTI-HEAD SELF ATTENTION \n",
    "## generate Attention weights\n",
    "def mask_attn_weights(w):\n",
    "    n = shape_list(w)[-1]\n",
    "    b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n",
    "    b = tf.reshape(b, [1, 1, n, n])\n",
    "    w = w*b + -1e9*(1-b)\n",
    "    return w\n",
    "## Attention Helper\n",
    "### Archtecture: matmul(dropout(softmax(masked(scale(matmul(Q,K))))), v), as in the self attention diagram 1 above in the attention section\n",
    "def _attn(q, k, v, train=False, scale=False):\n",
    "    w = tf.matmul(q, k)\n",
    "\n",
    "    if scale:\n",
    "        n_state = shape_list(v)[-1]\n",
    "        w = w*tf.rsqrt(tf.cast(n_state, tf.float32))\n",
    "\n",
    "    w = mask_attn_weights(w)\n",
    "    w = tf.nn.softmax(w)\n",
    "\n",
    "    w = dropout(w, attn_pdrop, train)\n",
    "\n",
    "    a = tf.matmul(w, v)\n",
    "    return a\n",
    "\n",
    "# Attention\n",
    "### Archtecture: matmul(dropout(softmax(masked(scale(matmul(Q,K))))), v), as in the self attention diagram 2 above in the attention section\n",
    "def split_heads(x, n, k=False):\n",
    "    if k:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 3, 1])\n",
    "    else:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n",
    "    \n",
    "def attn(x, scope, n_state, n_head, train=False, scale=False):\n",
    "    assert n_state%n_head==0\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3, 1, train=train) # faster 1x1 conv\n",
    "        q, k, v = tf.split(c, 3, 2)\n",
    "        q = split_heads(q, n_head)\n",
    "        k = split_heads(k, n_head, k=True)\n",
    "        v = split_heads(v, n_head)\n",
    "        a = _attn(q, k, v, train=train, scale=scale)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state, 1, train=train)\n",
    "        a = dropout(a, resid_pdrop, train)\n",
    "        return a\n",
    "\n",
    "# BLOCK\n",
    "# Architecture: masked multi self-attention --> layer norm --> feed forward --> layer norm --> output\n",
    "def block(x, scope, train=False, scale=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        a = attn(x, 'attn', nx, n_head, train=train, scale=scale)\n",
    "        n = norm(x+a, 'ln_1')\n",
    "        m = mlp(n, 'mlp', nx*4, train=train)\n",
    "        h = norm(n+m, 'ln_2')\n",
    "        return h\n",
    "    \n",
    "    \n",
    "    \n",
    "# MODEL\n",
    "## Logits\n",
    "def clf(x, ny, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), train=False):\n",
    "    with tf.variable_scope('clf'):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [nx, ny], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [ny], initializer=b_init)\n",
    "        return tf.matmul(x, w)+b\n",
    "    \n",
    "def model(X, M, Y, train=False, reuse=False):\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        # n_special=3，作者把数据集分为三份\n",
    "        # n_ctx 应该是 n_context\n",
    "        we = tf.get_variable(name=\"we\", \n",
    "                             shape=[n_vocab+n_special+n_ctx, n_embd], \n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        we = dropout(we, embd_pdrop, train)\n",
    "\n",
    "        X = tf.reshape(X, [-1, n_ctx, 2]) # [batch_size, n_context, 2]\n",
    "        M = tf.reshape(M, [-1, n_ctx])    # [batch_size, n_context]\n",
    "\n",
    "        # 1. Embedding\n",
    "        h = embed(X, we)\n",
    "\n",
    "        # 2. transformer block\n",
    "        for layer in range(n_layer):\n",
    "            h = block(h, 'h%d'%layer, train=train, scale=True)\n",
    "\n",
    "        # 3. language model loss\n",
    "        lm_h = tf.reshape(h[:, :-1], [-1, n_embd])\n",
    "        lm_logits = tf.matmul(lm_h, we, transpose_b=True)\n",
    "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, \n",
    "                                                                   labels=tf.reshape(X[:, 1:, 0], [-1]))\n",
    "        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])\n",
    "        lm_losses = tf.reduce_sum(lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)\n",
    "\n",
    "        # 4. classifier loss\n",
    "        clf_h = tf.reshape(h, [-1, n_embd])\n",
    "        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), \n",
    "                                             tf.float32), \n",
    "                                     1), \n",
    "                           tf.int32)\n",
    "        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32)*n_ctx+pool_idx)\n",
    "\n",
    "        clf_h = tf.reshape(clf_h, [-1, 2, n_embd])\n",
    "        if train and clf_pdrop > 0:\n",
    "            shape = shape_list(clf_h)\n",
    "            shape[1] = 1\n",
    "            clf_h = tf.nn.dropout(clf_h, 1-clf_pdrop, shape)\n",
    "        clf_h = tf.reshape(clf_h, [-1, n_embd])\n",
    "        clf_logits = clf(clf_h, 1, train=train)\n",
    "        clf_logits = tf.reshape(clf_logits, [-1, 2])\n",
    "\n",
    "        clf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clf_logits, labels=Y)\n",
    "        return clf_logits, clf_losses, lm_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "对比ELMo，虽然都是“双向”，但目标函数其实是不同的:\n",
    "\n",
    "- ELMo是分别以$P(w_i| w_1, ...w_{i-1})$ 和 $P(w_i|w_{i+1}, ...w_n)$ 作为目标函数，独立训练处两个representation然后拼接\n",
    "- BERT则是以 $P(w_i|w_1,  ...,w_{i-1}, w_{i+1},...,w_n)$ 作为目标函数训练LM\n",
    "\n",
    "这篇论文把预训练语言表示方法分为了基于特征的方法（代表 ELMo）和基于微调的方法（代表 OpenAI GPT）。而目前这两种方法在预训练时都是使用单向的语言模型来学习语言表示\n",
    "\n",
    "\n",
    "![](../../images/week3/60.jpg)\n",
    "\n",
    "\n",
    "这篇论文证明了使用双向的预训练效果更好。其实这篇论文方法的整体框架和 GPT 类似，是进一步的发展。具体的，BERT 是使用 Transformer 的编码器来作为语言模型，在语言模型预训练的时候，提出了两个新的目标任务（即masked语言模型 MLM 和预测下一个句子的任务)\n",
    "\n",
    "\n",
    "#### Method\n",
    "\n",
    "在语言模型上，BERT 使用的是 Transformer 编码器，并且设计了一个小一点的 base 结构和一个更大的网络结构\n",
    "\n",
    "![](../../images/week3/61.jpg)\n",
    "\n",
    "对比一下三种语言模型结构:\n",
    "\n",
    "- BERT 使用的是 Transformer 编码器，由于 self-attention 机制，所以模型上下层直接全部互相连接的。\n",
    "- OpenAI GPT 使用的是 Transformer 编码器，它是一个需要__从左到右的受限制__的 Transformer\n",
    "- ELMo 使用的是双向 LSTM，虽然是双向的，但是也只是在两个单向的 LSTM 的最高层进行简单的拼接。\n",
    "\n",
    "\n",
    "__所以只有 BERT 是真正在模型所有层中是双向的__\n",
    "\n",
    "![](../../images/week3/62.jpg)\n",
    "\n",
    "\n",
    "$Note^*$:\n",
    "\n",
    "- 而在模型的输入方面，BERT 做了更多的细节，如:\n",
    "    - 使用了 __WordPiece embedding__ 作为词向量\n",
    "    - 加入了位置向量和句子切分向量。此外，作者还在每一个文本输入前加入了一个 CLS 向量，后面会有这个向量作为具体的分类向量\n",
    "    \n",
    "#### Masked-LM\n",
    "然而，使用双向 Transformer 会有一个问题。正如上面的分析，即便对于 Base 版 BERT 来说，经过 12 个 block，每个 block 内部都有 12 个多头注意力机制，到最后一层的输出，序列中每个位置上对应的词向量信息，早已融合了输入序列中所有词的信息。\n",
    "\n",
    "而普通的语言模型中，是通过某个词的上下文语境预测当前词的概率。如果直接把这个套用到 Transformer 的 Encoder 中，会发现待预测的输出和序列输入已经糅合在一块了，说白了就是 Encoder 的输入已经包含了正确的监督信息了，相当于给模型泄题了，如此__普通语言模型的目标函数无法直接套用__。\n",
    "\n",
    "> 那么，如何解决 Self-attention 中带来了表征性能卓越的双向机制，却又同时带来信息泄露的这一问题？ \n",
    "\n",
    "BERT 的作者很快联想到了，如果把原来要预测整个句子的输出，改为只预测这个句子中的某个词，并且把输入中这个词所在位置挖空, 输入序列依然和普通Transformer保持一致，只不过把挖掉的一个词用\"[MASK]\"替换;\n",
    "\n",
    "- Transformer 的 Encoder 部分按正常进行；\n",
    "\n",
    "- 输出层在被挖掉的词位置，接一个分类层做词典大小上的分类问题，得到被 mask 掉的词概率大小。\n",
    "\n",
    "- 正是因为加了 mask，因此 BERT 才把这种方法叫做 Masked-LM\n",
    "\n",
    "![](../../images/week3/56.png)\n",
    "    \n",
    "    \n",
    "这就直接把普通语言模型中的生成问题（正如 GPT 中把它当做一个生成问题一样，虽然其本质上也是一个序列生成问题），变为一个简单的分类问题，并且也直接解决了 Encoder 中多层 Self-attention 的双向机制带来的泄密问题（单层 Self-attention 是真双向，但不会带来泄密问题，只有多层累加的 Self-attention 才会带来泄密问题），使得语言模型中的真双向机制变为现实。\n",
    "\n",
    "\n",
    "BERT 针对如何做“[MASK]”，做了一些更深入的研究，它做了如下处理：\n",
    "\n",
    "- 选取语料中所有词的 15% 进行随机 mask；\n",
    "\n",
    "- 选中的词在 80% 的概率下被真实 mask；\n",
    "\n",
    "- 选中的词在 10% 的概率下不做 mask，而被随机替换成其他一个词；\n",
    "\n",
    "- 选中的词在 10% 的概率下不做 mask，仍然保留原来真实的词。\n",
    "\n",
    "\n",
    "\n",
    "#### Next Sentence Prediction\n",
    "\n",
    "除了用上 Mask-LM 的方法使得双向 Transformer 下的语言模型成为现实，BERT 还利用和借鉴了 Skip-thoughts 方法中的句子预测问题，来学习句子级别的语义关系。具体做法则是将两个句子组合成一个序列，然后让模型预测这两个句子是否为先后近邻的两个句子，也就是会把\"Next Sentence Prediction\"问题建模__成为一个二分类问题__\n",
    "\n",
    "\n",
    "\n",
    "在预训练阶段，因为有两个任务需要训练：Mask-LM 和 Next Sentence Prediction，因此 BERT 的预训练过程实质上是一个 Multi-task Learning\n",
    "\n",
    "BERT的损失函数由两部分组成，第一部分是来自 Mask-LM 的单词级别分类任务，另一部分是句子级别的分类任务。通过这两个任务的联合学习，可以使得 __BERT 学习到的表征既有 token 级别信息，同时也包含了句子级别的语义信息__。具体损失函数如下:\n",
    "\n",
    "\n",
    "![](../../images/week3/57.png)\n",
    "\n",
    "\n",
    "- 其中 $\\theta$ 是 BERT 中 Encoder 部分的参数;\n",
    "- $\\theta_1$ 是 Mask-LM 任务中在 Encoder 上所接的输出层中的参数;\n",
    "- $\\theta_2$ 则是 Next Sentence Prediction 任务中在 Encoder 接上的分类器参数\n",
    "\n",
    "\n",
    "因此，在Masked-LM的损失函数中，如果被 mask 的词集合为 M，因为它是一个词典大小 $|V|$ 上的__多分类问题__，那么具体说来有:\n",
    "\n",
    "![](../../images/week3/58.png)\n",
    "\n",
    "\n",
    "在Next sentence prediction任务中，也是一个分类问题的损失函数:\n",
    "\n",
    "![](../../images/week3/59.png)\n",
    "\n",
    "两个任务联合学习的损失函数是:\n",
    "\n",
    "![](../../images/week3/60.png)\n",
    "\n",
    "\n",
    "BERT 还利用了一系列策略，使得模型更易于训练，比如对于学习率的 warm-up 策略（和上文提到的 ULMFiT 以及 Transformer 中用到的技巧类似），使用的激活函数不再是普通的 ReLu，而是 __GeLu__，也是用了 dropout 等常见的训练技巧\n",
    "\n",
    "\n",
    "在输入层方面，思路和 GPT 基本类似，如果输入只有一个句子，则直接在句子前后添加句子的起始标记位和句子的结束符号。在 BERT 中，起始标记都用“[CLS]”来表示，结束标记符用\"[SEP]\"表示，对于两个句子的输入情况，除了起始标记和结束标记之外，两个句子间通过\"[SEP]\"来进行区分。\n",
    "\n",
    "除了这些之外，BERT 还用两个表示当前是句子 A 或句子 B 的向量来进行表示。对于句子 A 来说，每一词都会添加一个同样的表示当前句子为句子 A 的向量，如果有句子 B 的话，句子 B 中的每个词也会添加一个表示当前句子为句子 B 的向量\n",
    "\n",
    "\n",
    "![](../../images/week3/61.png)\n",
    "\n",
    "\n",
    "除了输入层要尽量做到通用之外，根据不同任务设计不同的输出层也变得尤为重要，BERT 主要针对四类任务考虑和设计了一些非常易于移植的输出层，这四类任务分别是:\n",
    "- 单个序列文本分类任务\n",
    "- 两个序列文本分类任务\n",
    "- 阅读理解任务和序列标注任务\n",
    "- 句子或答案选择任务\n",
    "\n",
    "对于单序列文本分类任务和序列对的文本分类任务使用框架基本一致，只要输入层按照上面的方法做好表示即可。\n",
    "\n",
    "这两个分类任务都是__利用 Encoder 最后一层的第一个时刻“[CLS]”对应的输出作为分类器的输入__，这个时刻可以得到输入序列的全局表征，并且因为监督信号从这个位置反馈给模型，因而实际上在 finetune 阶段也可以使得这一表征尽量倾向于全局的表征。\n",
    "\n",
    "在finetune 阶段，在 BERT Encoder 基础上，这些分类任务因为只需要接一个全连接层，因此增加的参数只有 $H × K$ ，其中 $H$ 是 Encoder 输出层中隐状态的维度，$K$ 是分类类别个数\n",
    "\n",
    "对于 SQuAD 1.1 任务来说，需要在给定段落中找到正确答案所在区间，这段区间通过一个起始符与终止符来进行标记，因此只需__预测输入序列中哪个 token 所在位置是起始符或终止符即可__\n",
    "\n",
    "\n",
    "![](../../images/week3/62.png)\n",
    "\n",
    "\n",
    "Summary of BERT:\n",
    "\n",
    "- 1. 双向功能是 Transformer Encoder 自带的，因此借鉴 Transformer；\n",
    "\n",
    "- 2. Masked-LM 借鉴语言模型、CBOW 以及 Cloze问题；\n",
    "\n",
    "- 3. Next Sentence Prediction 借鉴 Skip-gram、Skip-thoughts 和 Quick-thoughts 等工作；\n",
    "\n",
    "- 4. 对输入层和输出层的改造，借鉴了 T-DMCA 以及 GPT 的做法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### [BERT Code](https://github.com/brightmart/bert_language_understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 23:42:34,891 : INFO : collecting all words and their counts\n",
      "2018-11-23 23:42:34,901 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-23 23:42:34,903 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-11-23 23:42:34,911 : INFO : Loading a fresh vocabulary\n",
      "2018-11-23 23:42:34,915 : INFO : effective_min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-11-23 23:42:34,921 : INFO : effective_min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-11-23 23:42:34,925 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-11-23 23:42:34,932 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-11-23 23:42:34,938 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-11-23 23:42:34,940 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-11-23 23:42:34,948 : INFO : resetting layer weights\n",
      "2018-11-23 23:42:34,972 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-23 23:42:35,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,035 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,054 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,061 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,070 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 99 effective words/s\n",
      "2018-11-23 23:42:35,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,088 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,113 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-23 23:42:35,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-23 23:42:35,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-23 23:42:35,125 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-11-23 23:42:35,127 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 7 effective words/s\n",
      "2018-11-23 23:42:35,130 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "# Gensim only requires that the input must provide sentences sequentially, \n",
    "# when iterated over. No need to keep everything in RAM: provide one sentence, process it, forget it, load another sentence\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 00:24:28,072 : INFO : loading projection weights from /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt\n",
      "2018-11-24 00:26:45,137 : INFO : loaded (399999, 100) matrix from /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f770ca3cd30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(fname='/home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt', binary=False)\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/karen/Downloads/data/glove.6B/glove.6B.100d.txt', 'r') as fread:\n",
    "    with open('/home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt', 'w') as fwrite:\n",
    "        fwrite.write('399999 100\\n')\n",
    "        for idx, line in enumerate(fread):\n",
    "            fwrite.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/karen/Downloads/data/glove.6B/glove.6B.100d.tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 00:28:08,244 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14084136,  0.10044176, -0.17062186, -0.03504317, -0.03259846,\n",
       "        0.16226351,  0.00164017,  0.02982827, -0.04554551,  0.19863695,\n",
       "       -0.08848356, -0.03040672,  0.06940352,  0.03717477, -0.05495993,\n",
       "        0.05361199, -0.01390229, -0.01308273, -0.07149444,  0.08059222,\n",
       "       -0.16011068, -0.03823438,  0.07592923, -0.01951167, -0.00767571,\n",
       "       -0.14076705, -0.04336968, -0.19582428,  0.01398277,  0.07748415,\n",
       "       -0.11785013,  0.21659192, -0.01383613,  0.07638032, -0.10138992,\n",
       "       -0.1082482 , -0.03673607,  0.08005092, -0.05872075, -0.01275246,\n",
       "       -0.147268  ,  0.00342861, -0.00873196,  0.02516351,  0.06088596,\n",
       "       -0.05161836, -0.0100394 ,  0.03327952,  0.04757451,  0.04673779,\n",
       "       -0.04099397, -0.11782714, -0.05391094,  0.07558075,  0.00579424,\n",
       "       -0.30187368,  0.01918441, -0.0895591 ,  0.29233897,  0.04310788,\n",
       "       -0.00594   ,  0.01523573, -0.04347936, -0.01092671,  0.11162515,\n",
       "        0.01602999,  0.13726982,  0.06173506,  0.18407837,  0.16519998,\n",
       "        0.03331666, -0.04278062, -0.02724027, -0.04862705, -0.09478815,\n",
       "        0.13880351,  0.11250433, -0.09805897, -0.09296789, -0.17817003,\n",
       "        0.055572  ,  0.04316802, -0.00249795, -0.22557826, -0.25593367,\n",
       "       -0.04909405, -0.02562875, -0.08425928,  0.07797062, -0.1113262 ,\n",
       "        0.04604082,  0.04001042,  0.06554011,  0.08327927, -0.09876832,\n",
       "        0.07073379,  0.02987957, -0.05065782,  0.0969799 , -0.05943188],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.vectors_norm[word2vec.vocab['student'].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Sentence2Vec by word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_word2vec_matrix(text, word2vec):\n",
    "    word2vec_matrix=[]\n",
    "    count=0\n",
    "    for line in text:\n",
    "        word_lst=line.split()\n",
    "        current_word2vec=[]\n",
    "        for word in word_lst:\n",
    "            if word in word2vec.vocab:\n",
    "                # word2vec = token2idx\n",
    "                vec = word2vec.vectors_norm[word2vec.vocab[word].index]\n",
    "                if vec is not None:\n",
    "                    current_word2vec.append(vec)\n",
    "            else:\n",
    "                print(word)\n",
    "                count+=1\n",
    "                continue\n",
    "        # add up all the vector of each word to get the vector of a sentence \n",
    "        if np.array(current_word2vec).shape[0]!=0:\n",
    "            sentence_word2vec = list(np.array(current_word2vec).mean(axis=0))\n",
    "            word2vec_matrix.append(sentence_word2vec)\n",
    "        current_word2vec=[]\n",
    "    return word2vec_matrix, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = ['fantastic beasts and where to find them', \n",
    "        'fantastic beasts the crimes of grindelwald']\n",
    "word2vec_matrix, count = create_word2vec_matrix(text, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01363225,  0.0632612 ,  0.0532789 , -0.06067752, -0.03203793,\n",
       "         0.0573948 , -0.05758385,  0.02891669, -0.01005686, -0.04475319,\n",
       "         0.03142466, -0.00525038,  0.03889203, -0.01838762,  0.04951621,\n",
       "        -0.0357757 ,  0.04723538,  0.06406571, -0.110898  ,  0.08045464,\n",
       "         0.06799715, -0.03494107,  0.05513481, -0.04234928,  0.04503259,\n",
       "        -0.0038799 , -0.02963971, -0.0674924 ,  0.0407795 , -0.05591179,\n",
       "        -0.03720056,  0.01724851, -0.06198395, -0.00394624,  0.01963765,\n",
       "         0.04357598, -0.03199228,  0.04052764,  0.01646447, -0.06384147,\n",
       "        -0.06349035, -0.01205016, -0.03160828, -0.10188204, -0.02248716,\n",
       "         0.05475311, -0.00057904, -0.00529261,  0.00310575, -0.06026373,\n",
       "        -0.03455113,  0.0093182 ,  0.04103724,  0.22216587, -0.00798338,\n",
       "        -0.37076157,  0.0060763 , -0.02901358,  0.19122097,  0.05265542,\n",
       "        -0.0445339 ,  0.15990758, -0.07531472,  0.02351545,  0.13658723,\n",
       "        -0.00910989,  0.08402754,  0.06993966,  0.04774452, -0.07440751,\n",
       "         0.00432026, -0.1105385 ,  0.01313347, -0.07487839,  0.02296722,\n",
       "         0.0301899 , -0.0280746 , -0.01931024, -0.08416656,  0.03807382,\n",
       "         0.1088913 ,  0.05673665, -0.07269619,  0.04082172, -0.2123229 ,\n",
       "        -0.03421097,  0.00801395, -0.01705644, -0.07357238, -0.03133134,\n",
       "        -0.01436853, -0.05166963,  0.02597543, -0.01765687, -0.12825742,\n",
       "        -0.05672801, -0.09524342, -0.05342931,  0.0833168 ,  0.04327292],\n",
       "       [ 0.00253851, -0.00877398,  0.0734567 , -0.00424336,  0.00065437,\n",
       "         0.05516574, -0.07829381,  0.01089091, -0.05188168, -0.02742841,\n",
       "         0.02640177,  0.01450567, -0.00968969, -0.03183353,  0.04959001,\n",
       "         0.02107229,  0.01465111,  0.03763356, -0.07321229,  0.02437557,\n",
       "         0.0532227 , -0.02813247, -0.01158483, -0.04175212,  0.03454379,\n",
       "        -0.02026052,  0.03413947, -0.00275082, -0.01641666, -0.04765847,\n",
       "         0.02100672, -0.03880749, -0.03762351, -0.04919778,  0.00471069,\n",
       "        -0.00720015, -0.01010328,  0.0680422 ,  0.0268242 , -0.07889139,\n",
       "        -0.05303546,  0.02562189,  0.00818047, -0.0245726 ,  0.00355374,\n",
       "         0.08366998,  0.06239701, -0.02762469, -0.01287852, -0.00160172,\n",
       "        -0.00158122,  0.01933938,  0.03472779,  0.13059537, -0.0444909 ,\n",
       "        -0.20608287,  0.01566952,  0.01300212,  0.10512716,  0.02248251,\n",
       "        -0.01267021,  0.11147016, -0.04865355, -0.00529323,  0.08803131,\n",
       "         0.01013602,  0.01064846,  0.01560272,  0.03319718, -0.03983454,\n",
       "         0.04644413, -0.10616875, -0.00987621,  0.05064338,  0.01425096,\n",
       "         0.02025635, -0.02772122, -0.01244517, -0.07074878,  0.04453899,\n",
       "         0.09321102,  0.10945683, -0.02349626,  0.04435547, -0.16061288,\n",
       "        -0.00520629, -0.0616868 , -0.02149554,  0.00955985, -0.01845567,\n",
       "        -0.02264366, -0.02768158,  0.03380519,  0.06865028, -0.03667846,\n",
       "        -0.05988262, -0.10793936, -0.03674577,  0.06783447,  0.01057718]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(word2vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count # so all words are in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8124398589134216"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = word2vec_matrix[0]\n",
    "sentence2 = word2vec_matrix[1]\n",
    "from scipy import spatial\n",
    "\n",
    "# calculate the cosine similarity\n",
    "1 - spatial.distance.cosine(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('school', 0.7116979360580444),\n",
       " ('elementary', 0.7066525220870972),\n",
       " ('grades', 0.7055771350860596),\n",
       " ('schools', 0.6759970188140869),\n",
       " ('preschool', 0.6746401786804199),\n",
       " ('pupils', 0.6707763075828552),\n",
       " ('classes', 0.646867036819458),\n",
       " ('schooling', 0.6365389227867126),\n",
       " ('vocational', 0.6250067949295044),\n",
       " ('enrollment', 0.6218675374984741)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['kindergarten', 'college'], negative=['scientist'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Find the different word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### calculate the similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1993172"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity('apocalypse', 'disaster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### [Word2Vec using Fasttext](https://pypi.org/project/fasttext/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### FastText?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "word2vec 和 GloVe 都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，__fastText 是利用带有监督标记的文本分类数据完成训练__\n",
    "\n",
    "本质上没有什么特殊的，__模型框架就是 CBOW__，只不过与普通的 CBOW 有两点不一样，分别是__输入数据和预测目标的不同__:\n",
    "\n",
    "- 在输入数据上，CBOW 输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，__而 fastText 为了利用更多的语序信息，将 bag-of-words 变成了 bag-of-features__，也就是输入 x 不再仅仅是一个词，还可以加上 bigram 或者是 trigram 的信息等等。\n",
    "\n",
    "\n",
    "![](../../images/week3/36.png)\n",
    "\n",
    "- 在预测目标上，CBOW 预测目标是语境中的一个词，而 __fastText 预测目标是当前这段输入文本的类别__，正因为需要这个文本类别，因此才说 fastText 是一个监督模型。\n",
    "\n",
    "而相同点在于，fastText 的网络结构和 CBOW 基本一致，同时在输出层的分类上也使用了 Hierachical Softmax 技巧来加速训练。\n",
    "\n",
    "这里的$x_{n,i}$便是语料当中第 n 篇文档的第 i 个词以及加上 N-gram 的特征信息。从这个损失函数便可以知道 __fastText 同样只有两个全连接层，分别是 A 和 B__，其中 A 便是最终可以获取的词向量信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "load_vectors('/home/karen/Downloads/data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.000991184264421463,\n",
       " -0.001029246486723423,\n",
       " 0.0006811252678744495,\n",
       " 0.00011803481174865738,\n",
       " -0.0006219972274266183,\n",
       " 0.001100853318348527,\n",
       " 0.00014544253644999117,\n",
       " -0.0008069276809692383,\n",
       " 0.0008861601236276329,\n",
       " 0.0005640610470436513,\n",
       " 0.00018239919154439121,\n",
       " 0.002081118058413267,\n",
       " 0.0011119148693978786,\n",
       " -8.017678737815004e-06,\n",
       " 0.0019191585015505552,\n",
       " 0.002273016609251499,\n",
       " 0.0004999999655410647,\n",
       " 0.0008727581007406116,\n",
       " 0.0017401062650606036,\n",
       " 0.00136960344389081,\n",
       " -0.00010665664740372449,\n",
       " 0.0006562909111380577,\n",
       " -0.0005103441653773189,\n",
       " 0.0006105859065428376,\n",
       " -0.001563229481689632,\n",
       " 0.0024708015844225883,\n",
       " -7.241084676934406e-05,\n",
       " 0.00035496175405569375,\n",
       " -0.0008005818235687912,\n",
       " 0.001430544420145452,\n",
       " 0.0005475004436448216,\n",
       " -0.000570164353121072,\n",
       " 0.00010769572691060603,\n",
       " -2.468213642714545e-05,\n",
       " 0.0015175550943240523,\n",
       " 0.0003394550003577024,\n",
       " -0.0006585742812603712,\n",
       " -0.0010379229206591845,\n",
       " 0.0002934975200332701,\n",
       " -0.0004705099854618311,\n",
       " -0.0005588103667832911,\n",
       " -0.0001224353618454188,\n",
       " 0.00018472163355909288,\n",
       " -0.0008804462268017232,\n",
       " -0.000540959823410958,\n",
       " 0.0007997071370482445,\n",
       " -0.000813662598375231,\n",
       " -0.0012080087326467037,\n",
       " 0.00136748724617064,\n",
       " 0.0007802385371178389,\n",
       " 0.0003648688143584877,\n",
       " -3.339976683491841e-05,\n",
       " -0.00031441979808732867,\n",
       " -0.0003039219882339239,\n",
       " -0.0005072257481515408,\n",
       " 0.0010179569944739342,\n",
       " -0.0009995679138228297,\n",
       " -0.0003505126223899424,\n",
       " 0.0003821019490715116,\n",
       " 0.0011511098127812147,\n",
       " -0.0005719128530472517,\n",
       " -0.0006986728403717279,\n",
       " 0.0005401912494562566,\n",
       " -0.0013313741656020284,\n",
       " 0.00041330300155095756,\n",
       " -0.0006128869135864079,\n",
       " 0.0012367976596578956,\n",
       " -0.001043042866513133,\n",
       " 0.000432994042057544,\n",
       " -0.0024143403861671686,\n",
       " -0.00034675822826102376,\n",
       " -0.00010233062494080514,\n",
       " 0.001215107273310423,\n",
       " 0.0012985324719920754,\n",
       " -6.912585376994684e-05,\n",
       " -2.201867027906701e-05,\n",
       " -0.0011087505845353007,\n",
       " -0.0006296449573710561,\n",
       " 0.0004131598980166018,\n",
       " 0.0010789132211357355,\n",
       " 0.001263558049686253,\n",
       " 0.00032109773019328713,\n",
       " 0.0006544655188918114,\n",
       " -0.0001419403706677258,\n",
       " -0.00021072222443763167,\n",
       " 0.00044179518590681255,\n",
       " 0.0007763911853544414,\n",
       " 0.0009751742472872138,\n",
       " -0.0004021052736788988,\n",
       " -0.0007130807498469949,\n",
       " -0.001583990640938282,\n",
       " -0.0005740714841522276,\n",
       " -0.0015615387819707394,\n",
       " -0.0025593223981559277,\n",
       " 0.00018674305465538055,\n",
       " 0.0009086879435926676,\n",
       " 0.0008893464109860361,\n",
       " 0.00039883964927867055,\n",
       " -0.0007123534451238811,\n",
       " 0.0025303619913756847]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('toy_data/second.txt', 'model')\n",
    "model['fantastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.000991184264421463,\n",
       " -0.001029246486723423,\n",
       " 0.0006811252678744495,\n",
       " 0.00011803481174865738,\n",
       " -0.0006219972274266183,\n",
       " 0.001100853318348527,\n",
       " 0.00014544253644999117,\n",
       " -0.0008069276809692383,\n",
       " 0.0008861601236276329,\n",
       " 0.0005640610470436513,\n",
       " 0.00018239919154439121,\n",
       " 0.002081118058413267,\n",
       " 0.0011119148693978786,\n",
       " -8.017678737815004e-06,\n",
       " 0.0019191585015505552,\n",
       " 0.002273016609251499,\n",
       " 0.0004999999655410647,\n",
       " 0.0008727581007406116,\n",
       " 0.0017401062650606036,\n",
       " 0.00136960344389081,\n",
       " -0.00010665664740372449,\n",
       " 0.0006562909111380577,\n",
       " -0.0005103441653773189,\n",
       " 0.0006105859065428376,\n",
       " -0.001563229481689632,\n",
       " 0.0024708015844225883,\n",
       " -7.241084676934406e-05,\n",
       " 0.00035496175405569375,\n",
       " -0.0008005818235687912,\n",
       " 0.001430544420145452,\n",
       " 0.0005475004436448216,\n",
       " -0.000570164353121072,\n",
       " 0.00010769572691060603,\n",
       " -2.468213642714545e-05,\n",
       " 0.0015175550943240523,\n",
       " 0.0003394550003577024,\n",
       " -0.0006585742812603712,\n",
       " -0.0010379229206591845,\n",
       " 0.0002934975200332701,\n",
       " -0.0004705099854618311,\n",
       " -0.0005588103667832911,\n",
       " -0.0001224353618454188,\n",
       " 0.00018472163355909288,\n",
       " -0.0008804462268017232,\n",
       " -0.000540959823410958,\n",
       " 0.0007997071370482445,\n",
       " -0.000813662598375231,\n",
       " -0.0012080087326467037,\n",
       " 0.00136748724617064,\n",
       " 0.0007802385371178389,\n",
       " 0.0003648688143584877,\n",
       " -3.339976683491841e-05,\n",
       " -0.00031441979808732867,\n",
       " -0.0003039219882339239,\n",
       " -0.0005072257481515408,\n",
       " 0.0010179569944739342,\n",
       " -0.0009995679138228297,\n",
       " -0.0003505126223899424,\n",
       " 0.0003821019490715116,\n",
       " 0.0011511098127812147,\n",
       " -0.0005719128530472517,\n",
       " -0.0006986728403717279,\n",
       " 0.0005401912494562566,\n",
       " -0.0013313741656020284,\n",
       " 0.00041330300155095756,\n",
       " -0.0006128869135864079,\n",
       " 0.0012367976596578956,\n",
       " -0.001043042866513133,\n",
       " 0.000432994042057544,\n",
       " -0.0024143403861671686,\n",
       " -0.00034675822826102376,\n",
       " -0.00010233062494080514,\n",
       " 0.001215107273310423,\n",
       " 0.0012985324719920754,\n",
       " -6.912585376994684e-05,\n",
       " -2.201867027906701e-05,\n",
       " -0.0011087505845353007,\n",
       " -0.0006296449573710561,\n",
       " 0.0004131598980166018,\n",
       " 0.0010789132211357355,\n",
       " 0.001263558049686253,\n",
       " 0.00032109773019328713,\n",
       " 0.0006544655188918114,\n",
       " -0.0001419403706677258,\n",
       " -0.00021072222443763167,\n",
       " 0.00044179518590681255,\n",
       " 0.0007763911853544414,\n",
       " 0.0009751742472872138,\n",
       " -0.0004021052736788988,\n",
       " -0.0007130807498469949,\n",
       " -0.001583990640938282,\n",
       " -0.0005740714841522276,\n",
       " -0.0015615387819707394,\n",
       " -0.0025593223981559277,\n",
       " 0.00018674305465538055,\n",
       " 0.0009086879435926676,\n",
       " 0.0008893464109860361,\n",
       " 0.00039883964927867055,\n",
       " -0.0007123534451238811,\n",
       " 0.0025303619913756847]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CBOW model\n",
    "model = fasttext.cbow('toy_data/second.txt', 'model')\n",
    "model['fantastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = fasttext.load_model('model.bin')\n",
    "# model['fantastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
