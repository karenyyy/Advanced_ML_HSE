{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(\n",
    "    '/home/karen/Downloads/data/GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'banana' in wv_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.54492188e-02,   4.71191406e-02,  -6.93359375e-02,\n",
       "         3.02734375e-01,  -1.87500000e-01,  -3.19824219e-02,\n",
       "         2.95410156e-02,  -2.05078125e-01,  -9.03320312e-02,\n",
       "         2.98828125e-01,   2.10937500e-01,  -6.88476562e-02,\n",
       "         4.76074219e-02,   5.83496094e-02,  -2.75390625e-01,\n",
       "         2.42187500e-01,  -3.22265625e-01,   4.73632812e-02,\n",
       "        -1.44531250e-01,   9.37500000e-02,   6.74438477e-03,\n",
       "         2.17773438e-01,   2.31445312e-01,   1.87500000e-01,\n",
       "         8.49609375e-02,  -5.39550781e-02,  -2.65625000e-01,\n",
       "         2.23388672e-02,   2.24609375e-01,   4.12109375e-01,\n",
       "        -2.30712891e-02,  -1.67968750e-01,   1.01928711e-02,\n",
       "         2.10937500e-01,   1.14135742e-02,   2.50000000e-01,\n",
       "         8.64257812e-02,  -2.16796875e-01,   8.93554688e-02,\n",
       "         1.50390625e-01,  -2.03125000e-01,  -2.30468750e-01,\n",
       "         1.42578125e-01,   1.34765625e-01,  -1.13769531e-01,\n",
       "        -1.80664062e-01,   3.58886719e-02,  -1.69921875e-01,\n",
       "         1.55273438e-01,   2.39257812e-01,  -2.30468750e-01,\n",
       "        -9.96093750e-02,   1.82617188e-01,  -1.81640625e-01,\n",
       "        -1.46484375e-01,  -7.03125000e-02,   9.37500000e-02,\n",
       "        -2.67578125e-01,   2.17285156e-02,  -1.45507812e-01,\n",
       "        -2.38037109e-02,   1.53320312e-01,  -9.81445312e-02,\n",
       "        -1.69677734e-02,   5.00488281e-02,  -3.08593750e-01,\n",
       "        -3.00781250e-01,  -3.20312500e-01,   1.18652344e-01,\n",
       "        -1.28906250e-01,   1.79687500e-01,   8.74023438e-02,\n",
       "        -1.48925781e-02,  -9.71679688e-02,  -5.62500000e-01,\n",
       "         2.50244141e-02,  -2.08984375e-01,  -2.33398438e-01,\n",
       "        -9.86328125e-02,  -2.22167969e-02,  -1.13281250e-01,\n",
       "         1.17675781e-01,  -1.56250000e-01,  -1.03515625e-01,\n",
       "         6.22558594e-02,  -2.04101562e-01,   2.75878906e-02,\n",
       "         2.57812500e-01,  -1.89453125e-01,  -6.29882812e-02,\n",
       "        -2.94921875e-01,   1.64062500e-01,   2.21252441e-04,\n",
       "        -1.40625000e-01,  -2.45361328e-02,  -6.73828125e-02,\n",
       "        -4.76074219e-02,   2.57568359e-02,   2.92968750e-01,\n",
       "        -4.41894531e-02,  -2.26562500e-01,  -3.07617188e-02,\n",
       "         3.66210938e-02,   1.00585938e-01,  -8.10546875e-02,\n",
       "         7.76367188e-02,   1.94335938e-01,  -1.09863281e-01,\n",
       "        -2.59765625e-01,  -1.41601562e-01,  -2.67578125e-01,\n",
       "        -2.39257812e-01,  -2.34375000e-01,   6.59179688e-02,\n",
       "        -2.96875000e-01,  -1.22680664e-02,   1.44531250e-01,\n",
       "        -2.38281250e-01,   2.38037109e-02,   2.81982422e-02,\n",
       "        -5.12695312e-02,  -1.06445312e-01,  -1.22070312e-01,\n",
       "        -2.33398438e-01,  -1.75781250e-01,   1.24023438e-01,\n",
       "         1.94091797e-02,  -1.08886719e-01,   1.19628906e-01,\n",
       "         3.73046875e-01,  -2.48046875e-01,  -1.38671875e-01,\n",
       "        -1.33789062e-01,  -2.65625000e-01,  -2.71484375e-01,\n",
       "         2.67578125e-01,  -5.22460938e-02,   4.68750000e-02,\n",
       "         1.42578125e-01,  -2.57812500e-01,  -1.46484375e-01,\n",
       "        -4.02343750e-01,  -7.95898438e-02,   3.39355469e-02,\n",
       "         1.06445312e-01,   8.64257812e-02,   1.00585938e-01,\n",
       "         6.39648438e-02,  -1.69921875e-01,   2.67578125e-01,\n",
       "         4.85839844e-02,  -1.90429688e-01,  -3.22265625e-01,\n",
       "         1.03027344e-01,   4.19921875e-02,  -6.37817383e-03,\n",
       "        -1.96289062e-01,  -1.04003906e-01,   1.04492188e-01,\n",
       "        -1.68945312e-01,  -3.14453125e-01,   2.16674805e-03,\n",
       "         1.27929688e-01,   9.17968750e-02,  -1.25976562e-01,\n",
       "        -2.12097168e-03,   1.94335938e-01,  -1.42578125e-01,\n",
       "        -1.56250000e-01,  -5.44433594e-02,  -9.13085938e-02,\n",
       "         1.52343750e-01,  -2.55126953e-02,  -2.38281250e-01,\n",
       "        -4.14062500e-01,  -1.99218750e-01,   1.73828125e-01,\n",
       "        -8.25195312e-02,   6.88476562e-02,   2.69531250e-01,\n",
       "        -9.17968750e-02,  -8.10546875e-02,  -2.45117188e-01,\n",
       "         2.53906250e-01,  -1.96838379e-03,   5.05371094e-02,\n",
       "         4.66308594e-02,   2.63671875e-02,   2.23632812e-01,\n",
       "        -1.89453125e-01,   1.04003906e-01,  -2.80761719e-02,\n",
       "         4.00390625e-02,  -2.07031250e-01,   4.06250000e-01,\n",
       "         4.49218750e-01,  -3.32031250e-02,  -2.24609375e-01,\n",
       "        -1.28906250e-01,  -2.86865234e-02,   2.08984375e-01,\n",
       "        -2.18750000e-01,   6.07910156e-02,   1.25000000e-01,\n",
       "         4.37011719e-02,   7.17773438e-02,   3.14941406e-02,\n",
       "         1.23535156e-01,  -8.23974609e-03,  -4.32128906e-02,\n",
       "        -2.31445312e-01,   7.37304688e-02,  -1.51367188e-01,\n",
       "         1.52343750e-01,   2.08007812e-01,  -1.63085938e-01,\n",
       "        -2.20703125e-01,  -2.96875000e-01,   1.46484375e-01,\n",
       "         3.16406250e-01,   9.47265625e-02,  -7.03125000e-02,\n",
       "        -2.06054688e-01,  -3.06396484e-02,  -1.22070312e-01,\n",
       "        -2.47070312e-01,   9.13085938e-02,   1.22558594e-01,\n",
       "         4.10156250e-01,   1.22558594e-01,   3.80859375e-02,\n",
       "        -8.64257812e-02,  -4.27246094e-02,   4.12597656e-02,\n",
       "        -1.30859375e-01,  -1.11328125e-01,  -1.15966797e-02,\n",
       "        -1.60156250e-01,   2.63671875e-01,   5.37109375e-02,\n",
       "         3.11279297e-02,  -2.53906250e-01,   6.12792969e-02,\n",
       "         5.02929688e-02,   3.68652344e-02,  -2.83203125e-01,\n",
       "        -3.63281250e-01,   2.78320312e-02,   1.28906250e-01,\n",
       "        -1.85546875e-01,  -1.03515625e-01,  -1.29882812e-01,\n",
       "         3.69140625e-01,  -1.31835938e-01,  -4.22363281e-02,\n",
       "        -3.98437500e-01,   1.08642578e-02,  -1.95312500e-01,\n",
       "        -1.49414062e-01,  -4.95605469e-02,  -7.91015625e-02,\n",
       "         2.50000000e-01,  -2.05078125e-01,   2.65625000e-01,\n",
       "        -1.19628906e-02,  -1.37695312e-01,  -1.73828125e-01,\n",
       "         3.35937500e-01,  -1.28906250e-01,  -2.55859375e-01,\n",
       "        -1.89453125e-01,   1.61132812e-01,   2.23632812e-01,\n",
       "        -1.08886719e-01,  -8.05664062e-03,   1.82617188e-01,\n",
       "        -1.90429688e-01,  -1.69677734e-02,  -6.83593750e-02,\n",
       "         3.04687500e-01,   1.79687500e-01,   1.65039062e-01,\n",
       "         5.85937500e-02,  -1.02050781e-01,   6.68945312e-02,\n",
       "        -7.61718750e-02,  -3.33984375e-01,   6.34765625e-02,\n",
       "        -9.76562500e-02,   1.71875000e-01,  -1.77734375e-01,\n",
       "        -8.25195312e-02,   3.85742188e-02,  -2.94921875e-01,\n",
       "         5.52368164e-03,   5.44433594e-02,  -9.47265625e-02,\n",
       "         1.26953125e-01,   1.17187500e-01,   1.77734375e-01], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings['banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings['banana'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.24995956e-02,  5.55013008e-02,  3.41796875e-02,  1.51407883e-01,\n",
       "       -1.11328125e-01,  1.13037109e-01,  5.83699532e-02, -5.94685860e-02,\n",
       "        9.75748673e-02,  7.69449845e-02, -1.67480469e-01, -2.70919800e-02,\n",
       "        4.00187187e-02, -1.50553389e-02, -8.15429688e-02,  1.97875977e-01,\n",
       "        7.72298202e-02,  2.17285156e-02, -9.79003906e-02, -2.04264317e-02,\n",
       "        3.58072924e-03,  6.20117188e-02,  9.55403671e-02, -4.55322266e-02,\n",
       "       -2.22981777e-02,  5.96516915e-02, -6.29882812e-02,  1.28417969e-01,\n",
       "        6.22253418e-02,  1.47949219e-01, -1.52994795e-02,  5.33854179e-02,\n",
       "        3.66210938e-03,  8.13382491e-02,  9.93245468e-02,  3.77171822e-02,\n",
       "       -1.20951332e-01,  3.87369804e-02,  1.59179688e-01, -7.99560547e-03,\n",
       "       -4.86653633e-02,  2.12402344e-02,  8.26822892e-02,  2.76692715e-02,\n",
       "       -1.69270840e-02, -4.89298487e-03,  2.85034180e-02,  8.44726562e-02,\n",
       "       -4.02628593e-02,  6.36393204e-02,  5.72102852e-02,  9.60286427e-03,\n",
       "        2.32747402e-02, -4.47591133e-02,  6.16861992e-02,  2.77303066e-02,\n",
       "        3.31217460e-02, -3.65193677e-03,  2.62451172e-02, -7.94270858e-02,\n",
       "       -1.66015625e-02,  1.02864586e-01,  8.40250682e-03, -1.09130859e-01,\n",
       "        6.59179688e-03, -1.83512364e-02, -3.85742188e-02,  5.11881523e-02,\n",
       "       -7.10652694e-02,  1.25325518e-02,  3.54003906e-02, -4.02018242e-02,\n",
       "        5.65592460e-02, -8.12581405e-02,  2.27864590e-02,  1.08133955e-02,\n",
       "        8.80533829e-02,  1.00179039e-01,  3.14941406e-02,  3.12825531e-01,\n",
       "        5.92041016e-02, -1.20035805e-01,  7.04752579e-02, -2.11588535e-02,\n",
       "       -1.52669266e-01, -8.57518539e-02, -9.68017578e-02,  1.41601562e-01,\n",
       "        9.76562500e-02,  5.75358085e-02,  9.26106796e-02, -6.57246932e-02,\n",
       "       -1.31362915e-01, -7.29166642e-02, -4.22363281e-02, -6.85221329e-02,\n",
       "        1.28255203e-01,  5.11881523e-02, -2.03450527e-05,  9.85514298e-02,\n",
       "        1.86360683e-02, -1.23291016e-02,  4.52067070e-02,  5.20833321e-02,\n",
       "       -1.54622391e-01, -1.70898438e-02, -8.44319686e-02, -1.24023438e-01,\n",
       "       -8.17057267e-02, -9.53776017e-02,  1.78222656e-02, -1.63167313e-01,\n",
       "       -7.61311874e-02,  1.40543625e-01,  1.26546221e-02,  2.88899732e-03,\n",
       "        1.26627609e-01, -1.36718750e-02,  1.50716141e-01,  1.31022140e-01,\n",
       "        1.11338301e-02, -2.06235256e-02, -1.73502609e-01,  9.55810547e-02,\n",
       "       -1.07340492e-01, -1.54215498e-02, -3.86555982e-03,  2.70182285e-02,\n",
       "        5.29785156e-02,  2.19675694e-02, -1.30615234e-01, -1.29231766e-01,\n",
       "       -8.67513046e-02, -9.94466171e-02,  1.32985428e-01, -7.34659806e-02,\n",
       "        1.25651047e-01,  9.42789689e-02,  1.70898438e-03,  2.05688477e-02,\n",
       "        2.01334640e-01,  3.64583321e-02,  1.13444008e-01, -1.06730141e-01,\n",
       "        6.59179688e-03, -8.95182323e-03, -4.76481132e-02, -1.22599281e-01,\n",
       "        2.75065098e-02, -8.96809921e-02, -1.13118486e-02,  6.23372383e-02,\n",
       "       -9.63541642e-02,  6.39088973e-02, -4.31721993e-02,  8.56119767e-02,\n",
       "        9.08610001e-02, -1.70898438e-01,  1.71610508e-02, -8.02408829e-02,\n",
       "       -5.09440117e-02,  6.08723946e-02,  9.07109603e-02, -5.61523438e-02,\n",
       "        1.34023027e-02, -6.16861992e-02,  1.18896484e-01, -5.02115898e-02,\n",
       "       -9.44010448e-03,  6.79524764e-02, -2.48209629e-02, -6.77490234e-02,\n",
       "       -4.35384130e-03, -1.85709640e-01,  1.66829431e-03, -1.31347656e-01,\n",
       "        9.74934921e-02, -8.22753906e-02, -3.49934888e-03,  1.04166670e-02,\n",
       "       -6.10351562e-02,  5.41127510e-02, -2.74963379e-02, -5.24088554e-02,\n",
       "        7.57802352e-02,  5.56640625e-02, -8.51440430e-02,  6.11979179e-02,\n",
       "        1.62923172e-01,  1.51041672e-01,  4.17531319e-02,  2.44140625e-02,\n",
       "        7.68127441e-02,  3.28979492e-02, -1.43025713e-02, -3.21044922e-02,\n",
       "       -4.08528633e-02,  2.56551113e-02,  5.84411621e-02, -2.96223965e-02,\n",
       "        8.54288712e-02,  5.68033867e-02, -1.65852860e-01,  1.08968101e-01,\n",
       "       -1.29801437e-01,  8.76871720e-02, -8.15022811e-02,  5.24495430e-02,\n",
       "        9.74934921e-02,  4.65494804e-02,  3.61531563e-02,  9.26106796e-02,\n",
       "       -1.05794275e-03, -6.42903661e-03, -2.58789062e-01,  4.76074219e-02,\n",
       "        1.32080078e-01, -3.93066406e-02, -8.69547501e-02,  3.20231132e-02,\n",
       "       -9.19596329e-02, -5.75154610e-02, -1.23602547e-01, -7.03125000e-02,\n",
       "        2.02473953e-01,  9.76562500e-03,  1.26627609e-01, -1.91243493e-03,\n",
       "       -7.76163721e-03,  1.23291016e-01, -2.19726562e-02, -3.00292969e-02,\n",
       "        6.12182617e-02,  1.02701820e-01,  3.19824219e-02, -1.83919277e-02,\n",
       "       -2.49023438e-02, -2.51057949e-02, -4.93977852e-02,  7.04421997e-02,\n",
       "       -1.01969399e-01, -6.37410507e-02, -1.08642578e-02, -6.61621094e-02,\n",
       "       -5.28971367e-02, -5.92346191e-02,  8.43912736e-02,  7.33642578e-02,\n",
       "        9.11458358e-02, -2.21761074e-02, -9.27734375e-02, -9.19698104e-02,\n",
       "        7.19197616e-02,  1.25081375e-01, -3.37931328e-02, -1.94905605e-02,\n",
       "        1.04695641e-01,  2.17692051e-02, -1.27929688e-01, -7.46256486e-02,\n",
       "        6.20524101e-02, -2.19726562e-02, -6.68945312e-02,  2.16674805e-02,\n",
       "        5.33040352e-02,  2.41210938e-01,  8.72395858e-02, -1.08235680e-01,\n",
       "       -1.71712235e-01, -1.27209976e-01, -7.24283839e-03,  1.46484375e-01,\n",
       "        9.52148438e-02, -6.81152344e-02,  5.19409180e-02,  9.78190079e-02,\n",
       "        1.62378943e-03, -2.13216141e-01, -2.36816406e-02,  4.06901026e-03,\n",
       "        1.02539062e-02, -2.38444004e-02, -4.01204415e-02,  6.62536621e-02,\n",
       "        1.02864586e-01, -8.51033553e-02, -8.67919922e-02, -3.09244785e-02,\n",
       "        9.21223983e-02,  7.93863907e-02, -1.30859375e-01,  2.66621914e-02,\n",
       "       -3.31827812e-02,  8.90299454e-02,  1.09863281e-03, -1.83919277e-02,\n",
       "       -5.97737618e-02, -8.22448730e-02,  7.12890625e-02, -1.79036462e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([wv_embeddings[i] for i in \"this is question\".split()]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    if question == \"\":\n",
    "        return np.zeros(dim)\n",
    "    t = np.array([embeddings[i]\n",
    "                  for i in question.split() if i in embeddings])\n",
    "    if len(t) == 0:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "    return(t.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_to_vec(\"a\",wv_embeddings,dim=100).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test_embeddings.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-27b1576aa1f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion2vec_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test_embeddings.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquestion2vec_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion2vec_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test_embeddings.tsv'"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, and topK($q_i$) is the top of the ranked sentences provided by our model.\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ DCG = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] We consider only the first place and *Hits@1 = 0*\n",
    "- [K = 2] We consider the first and the second places and *Hits@1 = 1*\n",
    "- [K = 4] We consider the whole list and *Hits@1 = 1*\n",
    "\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] *DCG = 0* because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] *DCG = $\\frac{1}{\\log_2{3}}$*, because $rank_{dup}$ = 2.\n",
    "- [K = 4] *DCG = $\\frac{1}{\\log_2{3}}$*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=np.array([3])\n",
    "ar<=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "\n",
    "    dup_ranks=(np.array(dup_ranks)<=k).astype(int).sum()/len(dup_ranks)\n",
    "\n",
    "    return dup_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    dup_ranks=np.array(dup_ranks)\n",
    "    return ((1/np.log2(1+dup_ranks)*((dup_ranks<=k).astype(int))).sum()/len(dup_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_score([1,2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus(\"/home/karen/Downloads/data/stackoverflow_qa/validation.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_questions*. The function should return a sorted list of pairs *(initial position in candidates list, question)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "\n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    question_e = [question_to_vec(question, embeddings)]\n",
    "    candidates_e = [question_to_vec(i, embeddings) for i in candidates]\n",
    "    similarity=cosine_similarity(question_e,candidates_e)\n",
    "\n",
    "    return [(pos,candidates[pos]) for pos in np.fliplr(np.argsort(similarity))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'C# create cookie from string and send it'),\n",
       " (0, 'Convert Google results object (pure js) to Python object'),\n",
       " (2, 'How to use jQuery AJAX for an outside domain?')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_candidates('converting string to list',['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'],wv_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation = [text_prepare(i) for i in line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file(\"/home/karen/Downloads/data/stackoverflow_qa/train.tsv\",\"/home/karen/Downloads/data/stackoverflow_qa/train_prep.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file(\"/home/karen/Downloads/data/stackoverflow_qa/test.tsv\",\"/home/karen/Downloads/data/stackoverflow_qa/test_prep.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep  = read_corpus(\"/home/karen/Downloads/data/stackoverflow_qa/test_prep.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. It should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = \"/home/karen/Downloads/data/stackoverflow_qa/test_prep.tsv\"\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace could not be run on Windows and we recommend to use provided\n",
    "docker container or other alternatives.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a nuber of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 1\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prep.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prep.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040000  loss: 0.007852  eta: 0h6m  tot: 0h1m43s  (20.0%)%  lr: 0.049279  loss: 0.024238  eta: 0h9m  tot: 0h0m7s  (1.2%)9.9%  lr: 0.048839  loss: 0.019177  eta: 0h9m  tot: 0h0m11s  (2.0%)12.7%  lr: 0.048408  loss: 0.017547  eta: 0h9m  tot: 0h0m15s  (2.5%)13.8%  lr: 0.048268  loss: 0.017024  eta: 0h9m  tot: 0h0m16s  (2.8%)15.6%  lr: 0.048118  loss: 0.016245  eta: 0h9m  tot: 0h0m18s  (3.1%)18.7%  lr: 0.047808  loss: 0.014928  eta: 0h9m  tot: 0h0m22s  (3.7%)25.6%  lr: 0.047037  loss: 0.013254  eta: 0h9m  tot: 0h0m30s  (5.1%)25.7%  lr: 0.047017  loss: 0.013213  eta: 0h9m  tot: 0h0m30s  (5.1%)31.1%  lr: 0.046386  loss: 0.012327  eta: 0h9m  tot: 0h0m36s  (6.2%)38.3%  lr: 0.045676  loss: 0.011295  eta: 0h8m  tot: 0h0m43s  (7.7%)38.4%  lr: 0.045676  loss: 0.011285  eta: 0h8m  tot: 0h0m43s  (7.7%)44.3%  lr: 0.045115  loss: 0.010673  eta: 0h8m  tot: 0h0m49s  (8.9%)45.2%  lr: 0.045015  loss: 0.010573  eta: 0h8m  tot: 0h0m50s  (9.0%)55.6%  lr: 0.043924  loss: 0.009779  eta: 0h7m  tot: 0h1m0s  (11.1%)55.8%  lr: 0.043894  loss: 0.009770  eta: 0h8m  tot: 0h1m0s  (11.2%)56.5%  lr: 0.043814  loss: 0.009732  eta: 0h8m  tot: 0h1m1s  (11.3%)57.1%  lr: 0.043664  loss: 0.009704  eta: 0h8m  tot: 0h1m1s  (11.4%)57.5%  lr: 0.043614  loss: 0.009672  eta: 0h8m  tot: 0h1m2s  (11.5%)58.0%  lr: 0.043574  loss: 0.009631  eta: 0h8m  tot: 0h1m3s  (11.6%)63.8%  lr: 0.042863  loss: 0.009277  eta: 0h7m  tot: 0h1m10s  (12.8%)64.5%  lr: 0.042843  loss: 0.009236  eta: 0h7m  tot: 0h1m10s  (12.9%)0h7m  tot: 0h1m11s  (13.0%)67.9%  lr: 0.042603  loss: 0.009034  eta: 0h7m  tot: 0h1m14s  (13.6%)75.2%  lr: 0.041652  loss: 0.008690  eta: 0h7m  tot: 0h1m22s  (15.0%)77.0%  lr: 0.041452  loss: 0.008614  eta: 0h7m  tot: 0h1m24s  (15.4%)79.0%  lr: 0.041221  loss: 0.008521  eta: 0h7m  tot: 0h1m26s  (15.8%)81.9%  lr: 0.040841  loss: 0.008427  eta: 0h7m  tot: 0h1m29s  (16.4%)82.1%  lr: 0.040801  loss: 0.008416  eta: 0h7m  tot: 0h1m29s  (16.4%)86.5%  lr: 0.040330  loss: 0.008260  eta: 0h7m  tot: 0h1m34s  (17.3%)88.0%  lr: 0.040210  loss: 0.008206  eta: 0h7m  tot: 0h1m36s  (17.6%)h6m  tot: 0h1m43s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.00831376 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030040  loss: 0.002291  eta: 0h6m  tot: 0h3m48s  (40.0%)5%  lr: 0.039930  loss: 0.001555  eta: 0h6m  tot: 0h1m43s  (20.1%)2.2%  lr: 0.039660  loss: 0.001614  eta: 0h7m  tot: 0h1m45s  (20.4%)3.5%  lr: 0.039520  loss: 0.001926  eta: 0h6m  tot: 0h1m47s  (20.7%)5.1%  lr: 0.039319  loss: 0.002073  eta: 0h7m  tot: 0h1m48s  (21.0%)5.3%  lr: 0.039279  loss: 0.002083  eta: 0h7m  tot: 0h1m49s  (21.1%)6.2%  lr: 0.039229  loss: 0.002020  eta: 0h8m  tot: 0h1m51s  (21.2%)12.2%  lr: 0.038519  loss: 0.002124  eta: 0h8m  tot: 0h1m59s  (22.4%)19.1%  lr: 0.037778  loss: 0.002182  eta: 0h8m  tot: 0h2m8s  (23.8%)21.8%  lr: 0.037568  loss: 0.002177  eta: 0h8m  tot: 0h2m11s  (24.4%)24.2%  lr: 0.037347  loss: 0.002204  eta: 0h8m  tot: 0h2m14s  (24.8%)25.0%  lr: 0.037257  loss: 0.002207  eta: 0h8m  tot: 0h2m15s  (25.0%)25.1%  lr: 0.037237  loss: 0.002205  eta: 0h8m  tot: 0h2m15s  (25.0%)26.2%  lr: 0.037137  loss: 0.002198  eta: 0h8m  tot: 0h2m17s  (25.2%)30.5%  lr: 0.036707  loss: 0.002218  eta: 0h8m  tot: 0h2m23s  (26.1%)35.6%  lr: 0.036206  loss: 0.002277  eta: 0h8m  tot: 0h2m31s  (27.1%)38.3%  lr: 0.035896  loss: 0.002284  eta: 0h8m  tot: 0h2m35s  (27.7%)38.4%  lr: 0.035886  loss: 0.002283  eta: 0h8m  tot: 0h2m35s  (27.7%)38.7%  lr: 0.035876  loss: 0.002284  eta: 0h8m  tot: 0h2m36s  (27.7%)41.0%  lr: 0.035656  loss: 0.002281  eta: 0h8m  tot: 0h2m38s  (28.2%)41.4%  lr: 0.035636  loss: 0.002284  eta: 0h8m  tot: 0h2m39s  (28.3%)53.4%  lr: 0.034455  loss: 0.002289  eta: 0h7m  tot: 0h2m53s  (30.7%)54.5%  lr: 0.034354  loss: 0.002278  eta: 0h7m  tot: 0h2m54s  (30.9%)56.4%  lr: 0.034224  loss: 0.002270  eta: 0h7m  tot: 0h2m56s  (31.3%)58.8%  lr: 0.033894  loss: 0.002268  eta: 0h7m  tot: 0h3m0s  (31.8%)63.0%  lr: 0.033624  loss: 0.002280  eta: 0h7m  tot: 0h3m4s  (32.6%)63.9%  lr: 0.033554  loss: 0.002278  eta: 0h7m  tot: 0h3m5s  (32.8%)66.0%  lr: 0.033323  loss: 0.002293  eta: 0h7m  tot: 0h3m8s  (33.2%)73.0%  lr: 0.032663  loss: 0.002305  eta: 0h7m  tot: 0h3m17s  (34.6%)73.2%  lr: 0.032653  loss: 0.002304  eta: 0h7m  tot: 0h3m17s  (34.6%)76.9%  lr: 0.032282  loss: 0.002290  eta: 0h6m  tot: 0h3m21s  (35.4%)78.3%  lr: 0.032142  loss: 0.002292  eta: 0h6m  tot: 0h3m23s  (35.7%)79.8%  lr: 0.031932  loss: 0.002297  eta: 0h6m  tot: 0h3m25s  (36.0%)80.3%  lr: 0.031892  loss: 0.002296  eta: 0h6m  tot: 0h3m26s  (36.1%)80.4%  lr: 0.031882  loss: 0.002298  eta: 0h6m  tot: 0h3m26s  (36.1%)81.4%  lr: 0.031812  loss: 0.002297  eta: 0h6m  tot: 0h3m27s  (36.3%)81.9%  lr: 0.031752  loss: 0.002296  eta: 0h6m  tot: 0h3m27s  (36.4%)85.7%  lr: 0.031351  loss: 0.002283  eta: 0h6m  tot: 0h3m33s  (37.1%)88.7%  lr: 0.031001  loss: 0.002281  eta: 0h6m  tot: 0h3m36s  (37.7%)93.4%  lr: 0.030571  loss: 0.002284  eta: 0h6m  tot: 0h3m41s  (38.7%)94.3%  lr: 0.030491  loss: 0.002277  eta: 0h6m  tot: 0h3m43s  (38.9%)95.6%  lr: 0.030340  loss: 0.002278  eta: 0h6m  tot: 0h3m44s  (39.1%)96.2%  lr: 0.030250  loss: 0.002278  eta: 0h6m  tot: 0h3m45s  (39.2%)96.3%  lr: 0.030240  loss: 0.002281  eta: 0h6m  tot: 0h3m45s  (39.3%)\n",
      " ---+++                Epoch    1 Train error : 0.00229162 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001576  eta: 0h4m  tot: 0h5m51s  (60.0%)4%  lr: 0.029980  loss: 0.001228  eta: 0h4m  tot: 0h3m50s  (40.1%)2.0%  lr: 0.029810  loss: 0.001370  eta: 0h6m  tot: 0h3m52s  (40.4%)0.029800  loss: 0.001377  eta: 0h7m  tot: 0h3m52s  (40.4%)%)9.1%  lr: 0.028999  loss: 0.001556  eta: 0h7m  tot: 0h4m3s  (41.8%)11.9%  lr: 0.028659  loss: 0.001507  eta: 0h6m  tot: 0h4m6s  (42.4%)12.0%  lr: 0.028649  loss: 0.001524  eta: 0h6m  tot: 0h4m7s  (42.4%)12.8%  lr: 0.028539  loss: 0.001492  eta: 0h6m  tot: 0h4m8s  (42.6%)14.7%  lr: 0.028318  loss: 0.001517  eta: 0h6m  tot: 0h4m10s  (42.9%)14.9%  lr: 0.028288  loss: 0.001529  eta: 0h6m  tot: 0h4m10s  (43.0%)18.4%  lr: 0.027888  loss: 0.001541  eta: 0h6m  tot: 0h4m15s  (43.7%)19.6%  lr: 0.027818  loss: 0.001555  eta: 0h6m  tot: 0h4m16s  (43.9%)20.9%  lr: 0.027708  loss: 0.001544  eta: 0h6m  tot: 0h4m17s  (44.2%)25.4%  lr: 0.027307  loss: 0.001563  eta: 0h6m  tot: 0h4m23s  (45.1%)31.4%  lr: 0.026637  loss: 0.001559  eta: 0h5m  tot: 0h4m30s  (46.3%)36.2%  lr: 0.026276  loss: 0.001551  eta: 0h5m  tot: 0h4m35s  (47.2%)36.8%  lr: 0.026176  loss: 0.001553  eta: 0h5m  tot: 0h4m36s  (47.4%)38.0%  lr: 0.026026  loss: 0.001547  eta: 0h5m  tot: 0h4m37s  (47.6%)38.4%  lr: 0.026006  loss: 0.001548  eta: 0h5m  tot: 0h4m37s  (47.7%)40.3%  lr: 0.025756  loss: 0.001554  eta: 0h5m  tot: 0h4m40s  (48.1%)41.4%  lr: 0.025666  loss: 0.001555  eta: 0h5m  tot: 0h4m41s  (48.3%)43.2%  lr: 0.025536  loss: 0.001543  eta: 0h5m  tot: 0h4m44s  (48.6%)46.6%  lr: 0.025235  loss: 0.001549  eta: 0h5m  tot: 0h4m48s  (49.3%)47.0%  lr: 0.025205  loss: 0.001548  eta: 0h5m  tot: 0h4m48s  (49.4%)49.2%  lr: 0.025015  loss: 0.001547  eta: 0h5m  tot: 0h4m51s  (49.8%)51.2%  lr: 0.024915  loss: 0.001570  eta: 0h5m  tot: 0h4m53s  (50.2%)51.5%  lr: 0.024895  loss: 0.001568  eta: 0h5m  tot: 0h4m54s  (50.3%)54.3%  lr: 0.024585  loss: 0.001569  eta: 0h5m  tot: 0h4m57s  (50.9%)54.6%  lr: 0.024585  loss: 0.001573  eta: 0h5m  tot: 0h4m57s  (50.9%)56.7%  lr: 0.024374  loss: 0.001571  eta: 0h5m  tot: 0h4m59s  (51.3%)58.8%  lr: 0.024124  loss: 0.001574  eta: 0h5m  tot: 0h5m3s  (51.8%)60.9%  lr: 0.023894  loss: 0.001579  eta: 0h4m  tot: 0h5m5s  (52.2%)65.4%  lr: 0.023373  loss: 0.001572  eta: 0h4m  tot: 0h5m10s  (53.1%)66.3%  lr: 0.023253  loss: 0.001568  eta: 0h4m  tot: 0h5m11s  (53.3%)69.5%  lr: 0.022943  loss: 0.001567  eta: 0h4m  tot: 0h5m15s  (53.9%)70.5%  lr: 0.022773  loss: 0.001568  eta: 0h4m  tot: 0h5m16s  (54.1%)71.3%  lr: 0.022743  loss: 0.001572  eta: 0h4m  tot: 0h5m18s  (54.3%)74.3%  lr: 0.022302  loss: 0.001583  eta: 0h4m  tot: 0h5m22s  (54.9%)74.4%  lr: 0.022272  loss: 0.001582  eta: 0h4m  tot: 0h5m22s  (54.9%)75.8%  lr: 0.022082  loss: 0.001586  eta: 0h4m  tot: 0h5m23s  (55.2%)76.7%  lr: 0.021952  loss: 0.001587  eta: 0h4m  tot: 0h5m24s  (55.3%)77.1%  lr: 0.021902  loss: 0.001587  eta: 0h4m  tot: 0h5m25s  (55.4%)79.7%  lr: 0.021642  loss: 0.001590  eta: 0h4m  tot: 0h5m29s  (55.9%)80.5%  lr: 0.021562  loss: 0.001587  eta: 0h4m  tot: 0h5m30s  (56.1%)87.2%  lr: 0.020961  loss: 0.001582  eta: 0h4m  tot: 0h5m37s  (57.4%)88.1%  lr: 0.020851  loss: 0.001585  eta: 0h4m  tot: 0h5m38s  (57.6%)90.3%  lr: 0.020671  loss: 0.001583  eta: 0h4m  tot: 0h5m41s  (58.1%)5m46s  (58.6%)0.020180  loss: 0.001578  eta: 0h4m  tot: 0h5m48s  (59.0%)\n",
      " ---+++                Epoch    2 Train error : 0.00157322 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001249  eta: 0h1m  tot: 0h7m44s  (80.0%) lr: 0.019700  loss: 0.001338  eta: 0h3m  tot: 0h5m54s  (60.5%)7.8%  lr: 0.019029  loss: 0.001355  eta: 0h4m  tot: 0h6m1s  (61.6%)9.8%  lr: 0.018849  loss: 0.001324  eta: 0h3m  tot: 0h6m4s  (62.0%)11.1%  lr: 0.018729  loss: 0.001321  eta: 0h3m  tot: 0h6m5s  (62.2%)12.2%  lr: 0.018619  loss: 0.001330  eta: 0h4m  tot: 0h6m7s  (62.4%)17.6%  lr: 0.017978  loss: 0.001306  eta: 0h3m  tot: 0h6m14s  (63.5%)19.0%  lr: 0.017818  loss: 0.001285  eta: 0h3m  tot: 0h6m16s  (63.8%)19.8%  lr: 0.017768  loss: 0.001263  eta: 0h3m  tot: 0h6m17s  (64.0%)22.2%  lr: 0.017548  loss: 0.001259  eta: 0h3m  tot: 0h6m19s  (64.4%)22.7%  lr: 0.017528  loss: 0.001255  eta: 0h3m  tot: 0h6m20s  (64.5%)24.0%  lr: 0.017427  loss: 0.001249  eta: 0h3m  tot: 0h6m21s  (64.8%)25.8%  lr: 0.017207  loss: 0.001249  eta: 0h3m  tot: 0h6m23s  (65.2%)32.4%  lr: 0.016336  loss: 0.001247  eta: 0h3m  tot: 0h6m31s  (66.5%)36.4%  lr: 0.015866  loss: 0.001265  eta: 0h3m  tot: 0h6m35s  (67.3%)37.5%  lr: 0.015716  loss: 0.001256  eta: 0h3m  tot: 0h6m37s  (67.5%)38.1%  lr: 0.015646  loss: 0.001260  eta: 0h3m  tot: 0h6m38s  (67.6%)40.0%  lr: 0.015466  loss: 0.001256  eta: 0h3m  tot: 0h6m41s  (68.0%)40.5%  lr: 0.015425  loss: 0.001256  eta: 0h3m  tot: 0h6m41s  (68.1%)41.5%  lr: 0.015305  loss: 0.001258  eta: 0h3m  tot: 0h6m43s  (68.3%)46.5%  lr: 0.014615  loss: 0.001256  eta: 0h3m  tot: 0h6m49s  (69.3%)48.5%  lr: 0.014445  loss: 0.001254  eta: 0h3m  tot: 0h6m52s  (69.7%)49.8%  lr: 0.014214  loss: 0.001262  eta: 0h3m  tot: 0h6m53s  (70.0%)51.0%  lr: 0.014034  loss: 0.001260  eta: 0h3m  tot: 0h6m55s  (70.2%)51.1%  lr: 0.013994  loss: 0.001259  eta: 0h3m  tot: 0h6m55s  (70.2%)%  lr: 0.013804  loss: 0.001260  eta: 0h3m  tot: 0h6m57s  (70.5%)52.5%  lr: 0.013794  loss: 0.001259  eta: 0h3m  tot: 0h6m57s  (70.5%)53.7%  lr: 0.013614  loss: 0.001255  eta: 0h3m  tot: 0h6m59s  (70.7%)54.3%  lr: 0.013534  loss: 0.001251  eta: 0h3m  tot: 0h6m59s  (70.9%)59.7%  lr: 0.012993  loss: 0.001272  eta: 0h2m  tot: 0h7m5s  (71.9%)60.4%  lr: 0.012873  loss: 0.001271  eta: 0h2m  tot: 0h7m6s  (72.1%)61.7%  lr: 0.012733  loss: 0.001267  eta: 0h2m  tot: 0h7m8s  (72.3%)61.9%  lr: 0.012713  loss: 0.001266  eta: 0h2m  tot: 0h7m8s  (72.4%)64.0%  lr: 0.012443  loss: 0.001270  eta: 0h2m  tot: 0h7m11s  (72.8%)64.2%  lr: 0.012433  loss: 0.001268  eta: 0h2m  tot: 0h7m12s  (72.8%)68.9%  lr: 0.011932  loss: 0.001267  eta: 0h2m  tot: 0h7m17s  (73.8%)69.0%  lr: 0.011922  loss: 0.001266  eta: 0h2m  tot: 0h7m18s  (73.8%)72.9%  lr: 0.011382  loss: 0.001270  eta: 0h2m  tot: 0h7m23s  (74.6%)75.1%  lr: 0.011101  loss: 0.001267  eta: 0h2m  tot: 0h7m26s  (75.0%)75.9%  lr: 0.011031  loss: 0.001265  eta: 0h2m  tot: 0h7m26s  (75.2%)76.5%  lr: 0.010991  loss: 0.001260  eta: 0h2m  tot: 0h7m27s  (75.3%)78.0%  lr: 0.010791  loss: 0.001260  eta: 0h2m  tot: 0h7m29s  (75.6%)78.9%  lr: 0.010671  loss: 0.001259  eta: 0h2m  tot: 0h7m31s  (75.8%)\n",
      " ---+++                Epoch    3 Train error : 0.00126753 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001107  eta: <1min   tot: 0h9m35s  (100.0%) lr: 0.009590  loss: 0.001139  eta: 0h1m  tot: 0h7m47s  (80.7%)3.7%  lr: 0.009570  loss: 0.001167  eta: 0h1m  tot: 0h7m48s  (80.7%)4.6%  lr: 0.009489  loss: 0.001147  eta: 0h1m  tot: 0h7m49s  (80.9%)6.9%  lr: 0.009059  loss: 0.001175  eta: 0h1m  tot: 0h7m51s  (81.4%)12.1%  lr: 0.008418  loss: 0.001071  eta: 0h1m  tot: 0h7m58s  (82.4%)12.3%  lr: 0.008378  loss: 0.001061  eta: 0h1m  tot: 0h7m59s  (82.5%)12.6%  lr: 0.008338  loss: 0.001068  eta: 0h1m  tot: 0h7m59s  (82.5%)13.5%  lr: 0.008158  loss: 0.001057  eta: 0h1m  tot: 0h8m0s  (82.7%)14.0%  lr: 0.008088  loss: 0.001055  eta: 0h1m  tot: 0h8m1s  (82.8%)14.4%  lr: 0.008038  loss: 0.001054  eta: 0h1m  tot: 0h8m2s  (82.9%)16.9%  lr: 0.007728  loss: 0.001046  eta: 0h1m  tot: 0h8m5s  (83.4%)17.6%  lr: 0.007698  loss: 0.001064  eta: 0h1m  tot: 0h8m6s  (83.5%)17.8%  lr: 0.007638  loss: 0.001058  eta: 0h1m  tot: 0h8m6s  (83.6%)19.7%  lr: 0.007447  loss: 0.001090  eta: 0h1m  tot: 0h8m9s  (83.9%)20.9%  lr: 0.007307  loss: 0.001075  eta: 0h1m  tot: 0h8m11s  (84.2%)23.1%  lr: 0.007047  loss: 0.001078  eta: 0h1m  tot: 0h8m14s  (84.6%)25.3%  lr: 0.006907  loss: 0.001072  eta: 0h1m  tot: 0h8m16s  (85.1%)25.4%  lr: 0.006877  loss: 0.001073  eta: 0h1m  tot: 0h8m16s  (85.1%)27.3%  lr: 0.006607  loss: 0.001076  eta: 0h1m  tot: 0h8m19s  (85.5%)27.9%  lr: 0.006487  loss: 0.001072  eta: 0h1m  tot: 0h8m20s  (85.6%)29.8%  lr: 0.006266  loss: 0.001075  eta: 0h1m  tot: 0h8m23s  (86.0%)29.9%  lr: 0.006256  loss: 0.001073  eta: 0h1m  tot: 0h8m23s  (86.0%)32.0%  lr: 0.005886  loss: 0.001085  eta: 0h1m  tot: 0h8m26s  (86.4%)40.5%  lr: 0.004765  loss: 0.001087  eta: 0h1m  tot: 0h8m37s  (88.1%)41.8%  lr: 0.004615  loss: 0.001090  eta: 0h1m  tot: 0h8m38s  (88.4%)43.0%  lr: 0.004485  loss: 0.001096  eta: 0h1m  tot: 0h8m40s  (88.6%)43.7%  lr: 0.004394  loss: 0.001097  eta: 0h1m  tot: 0h8m41s  (88.7%)46.4%  lr: 0.004154  loss: 0.001098  eta: 0h1m  tot: 0h8m44s  (89.3%)54.7%  lr: 0.003073  loss: 0.001107  eta: <1min   tot: 0h8m56s  (90.9%)56.0%  lr: 0.002863  loss: 0.001106  eta: <1min   tot: 0h8m57s  (91.2%)56.4%  lr: 0.002823  loss: 0.001108  eta: <1min   tot: 0h8m58s  (91.3%)57.3%  lr: 0.002733  loss: 0.001108  eta: <1min   tot: 0h9m0s  (91.5%)61.1%  lr: 0.002252  loss: 0.001107  eta: <1min   tot: 0h9m5s  (92.2%)69.8%  lr: 0.001251  loss: 0.001104  eta: <1min   tot: 0h9m15s  (94.0%)70.8%  lr: 0.001111  loss: 0.001109  eta: <1min   tot: 0h9m16s  (94.2%)77.9%  lr: 0.000651  loss: 0.001114  eta: <1min   tot: 0h9m23s  (95.6%)83.5%  lr: 0.000150  loss: 0.001103  eta: <1min   tot: 0h9m28s  (96.7%)\n",
      " ---+++                Epoch    4 Train error : 0.00111644 +++--- ���\n",
      "Saving model to file : starSpaceModel\n",
      "Saving model in tsv format : starSpaceModel.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile data/train_prep.tsv -trainMode 1 -adagrad  true -ngrams  1 -dim 100 -minCount  2 -verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05 -model starSpaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "embeds = pd.read_csv(\"starSpaceModel.tsv\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals=embeds.iloc[:,1:].values\n",
    "index=embeds.iloc[:,0].values\n",
    "starspace_embeddings= {i:j for i,j in zip(index,vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepared_validation=[prepared_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "\n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    question_e = [question_to_vec(question, embeddings,dim)]\n",
    "    candidates_e = [question_to_vec(i, embeddings,dim) for i in candidates]\n",
    "    similarity=cosine_similarity(question_e,candidates_e)\n",
    "  \n",
    "    return [(pos,candidates[pos]) for pos in np.fliplr(np.argsort(similarity))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 52\t63\t74\t46\t31\t88\t97\t32\t34\t80\t37\t3\t24\t72\t94\t95\t8\t58\t30\t79\t48\t42\t27\t87\t100\t39\t47\t60\t50\t12\t56\t36\t93\t91...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = \"/home/karen/Downloads/data/stackoverflow_qa/test_prep.tsv\"\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 52\t63\t74\t46\t31\t88\t97\t32\t34\t80\t37\t3\t24\t72\t94\t95\t8\t58\t30\t79\t48\t42\t27\t87\t100\t39\t47\t60\t50\t12\t56\t36\t93\t91...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"artirj@gmail.com\"\n",
    "STUDENT_TOKEN = \"dPHRPPN2l6jLw6s5\"\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
