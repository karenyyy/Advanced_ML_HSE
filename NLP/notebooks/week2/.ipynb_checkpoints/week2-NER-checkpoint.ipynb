{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week2/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd01f3ddc4f4e4ab7b500294f15d047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=849548), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab2fc095a9644eaa19740cb8c961082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=103771), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3e64fea28643ffb7f22d21b86bf4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=106837), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from download_utils import download_week2_resources\n",
    "\n",
    "download_week2_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Twitter Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "            if token.startswith(('http', 'https', 'ftp')):\n",
    "                token = '<URL>'\n",
    "            if token.startswith('@'):\n",
    "                token = '<USER>'\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['RT',\n",
       "  '<USER>',\n",
       "  ':',\n",
       "  'Online',\n",
       "  'ticket',\n",
       "  'sales',\n",
       "  'for',\n",
       "  'Ghostland',\n",
       "  'Observatory',\n",
       "  'extended',\n",
       "  'until',\n",
       "  '6',\n",
       "  'PM',\n",
       "  'EST',\n",
       "  'due',\n",
       "  'to',\n",
       "  'high',\n",
       "  'demand',\n",
       "  '.',\n",
       "  'Get',\n",
       "  'them',\n",
       "  'before',\n",
       "  'they',\n",
       "  'sell',\n",
       "  'out',\n",
       "  '...'],\n",
       " ['Apple',\n",
       "  'MacBook',\n",
       "  'Pro',\n",
       "  'A1278',\n",
       "  '13.3',\n",
       "  '\"',\n",
       "  'Laptop',\n",
       "  '-',\n",
       "  'MD101LL/A',\n",
       "  '(',\n",
       "  'June',\n",
       "  ',',\n",
       "  '2012',\n",
       "  ')',\n",
       "  '-',\n",
       "  'Full',\n",
       "  'read',\n",
       "  'by',\n",
       "  'eBay',\n",
       "  '<URL>',\n",
       "  '<URL>'],\n",
       " ['Happy',\n",
       "  'Birthday',\n",
       "  '<USER>',\n",
       "  '!',\n",
       "  'May',\n",
       "  'Allah',\n",
       "  's.w.t',\n",
       "  'bless',\n",
       "  'you',\n",
       "  'with',\n",
       "  'goodness',\n",
       "  'and',\n",
       "  'happiness',\n",
       "  '.'],\n",
       " ['<USER>',\n",
       "  'the',\n",
       "  'quest',\n",
       "  'line',\n",
       "  'im',\n",
       "  'assuming',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'same',\n",
       "  'way',\n",
       "  'with',\n",
       "  'awe',\n",
       "  'thur'],\n",
       " ['<USER>',\n",
       "  '<USER>',\n",
       "  '<USER>',\n",
       "  '<USER>',\n",
       "  '<USER>',\n",
       "  'still',\n",
       "  'perception',\n",
       "  ',',\n",
       "  'and',\n",
       "  'what',\n",
       "  'you',\n",
       "  'key',\n",
       "  'off',\n",
       "  'may',\n",
       "  'differ',\n",
       "  'from',\n",
       "  'me',\n",
       "  '.',\n",
       "  'Back',\n",
       "  'to',\n",
       "  'Selma',\n",
       "  'example',\n",
       "  ',',\n",
       "  'iow',\n",
       "  '.'],\n",
       " ['Amazon',\n",
       "  'Kindle',\n",
       "  'Fire',\n",
       "  '1st',\n",
       "  'Generation',\n",
       "  '8GB',\n",
       "  ',',\n",
       "  'Wi-Fi',\n",
       "  ',',\n",
       "  '7in',\n",
       "  '-',\n",
       "  'Black',\n",
       "  '-',\n",
       "  'Full',\n",
       "  'read',\n",
       "  'by',\n",
       "  'eBay',\n",
       "  '<URL>',\n",
       "  '<URL>'],\n",
       " ['<USER>', \"I'll\", 'call', 'u', 'after', 'class', 'pooky', ';D'],\n",
       " ['TL_jp', 'August', '29', ',', '2015', 'at', '01:25', 'AM', '<URL>'],\n",
       " ['<USER>',\n",
       "  'Wie',\n",
       "  'staat',\n",
       "  'daar',\n",
       "  'nou',\n",
       "  'met',\n",
       "  'de',\n",
       "  'microfoon',\n",
       "  'links',\n",
       "  ';-)',\n",
       "  '<URL>'],\n",
       " ['Why', 'do', 'I', 'have', 'an', 'exam', 'tomorrow', 'morning', '?']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USER>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USER>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
    "    # At first, add special tokens (or tags) to the dictionaries.\n",
    "    # The first special token must have index 0.\n",
    "    flattened_tokens_or_tags = [word for sentence in tokens_or_tags for word in sentence]\n",
    "    unk_token = special_tokens[0]\n",
    "    tokens_or_tags_set = [unk_token] + list(set(flattened_tokens_or_tags)|set(special_tokens) - set([unk_token]))\n",
    "    # Mapping tok2idx should contain each token or tag only once. \n",
    "    # To do so, you should:\n",
    "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
    "    #    occur in special_tokens (because they could have non-empty intersection)\n",
    "    # 2. index them (for example, you can add them into the list idx2tok\n",
    "    # 3. for each token/tag save the index into tok2idx).\n",
    "    tok2idx = {tok:idx for idx, tok in enumerate(tokens_or_tags_set)}\n",
    "    idx2tok = {idx:tok for idx, tok in enumerate(tokens_or_tags_set)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens + test_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'kita': 1,\n",
       " 'poguthu': 2,\n",
       " '5thflr': 3,\n",
       " 'glassy': 4,\n",
       " 're': 5,\n",
       " 'expectations': 6,\n",
       " 'Twas': 7,\n",
       " 'lbs': 8,\n",
       " 'Ripe': 9,\n",
       " 'Sandra': 10,\n",
       " '#Interstate': 11,\n",
       " 'KNOW': 12,\n",
       " 'ooooh': 13,\n",
       " 'cassie': 14,\n",
       " 'chihuahua': 15,\n",
       " 'ADIDAS': 16,\n",
       " 'sizes': 17,\n",
       " 'Thomson': 18,\n",
       " 'Sneakercon': 19,\n",
       " 'manque': 20,\n",
       " '#gifted': 21,\n",
       " 'spanning': 22,\n",
       " '2ND': 23,\n",
       " 'pronote': 24,\n",
       " 'asleep': 25,\n",
       " 'steve': 26,\n",
       " 'AGUANT': 27,\n",
       " 'poslije': 28,\n",
       " 'Mandaric': 29,\n",
       " 'Biro': 30,\n",
       " 'vfdt': 31,\n",
       " 'striker': 32,\n",
       " 'write-in': 33,\n",
       " 'outweigh': 34,\n",
       " 'machines': 35,\n",
       " 'kept': 36,\n",
       " 'doubtful': 37,\n",
       " 'est': 38,\n",
       " 'brat': 39,\n",
       " 'mutually': 40,\n",
       " '101.6': 41,\n",
       " 'Shoes': 42,\n",
       " 'sing': 43,\n",
       " \"j'adore\": 44,\n",
       " 'intense': 45,\n",
       " 'manages': 46,\n",
       " '30-31': 47,\n",
       " 'El': 48,\n",
       " 'sail': 49,\n",
       " 'losers': 50,\n",
       " 'Gadgets': 51,\n",
       " 'profound': 52,\n",
       " 'hole': 53,\n",
       " 'Metal': 54,\n",
       " 'JavaMonkeys': 55,\n",
       " '#69': 56,\n",
       " 'Minimum': 57,\n",
       " 'Physics1998': 58,\n",
       " 'Square': 59,\n",
       " 'total': 60,\n",
       " 'pimple': 61,\n",
       " 'yummy': 62,\n",
       " 'Mendez': 63,\n",
       " 'natation': 64,\n",
       " '#nowplaying-': 65,\n",
       " 'struggled': 66,\n",
       " '9:14': 67,\n",
       " 'Calvin': 68,\n",
       " 'B-DAY': 69,\n",
       " '!?': 70,\n",
       " '#Thirstdays': 71,\n",
       " 'ntlegs11': 72,\n",
       " 'Coin': 73,\n",
       " 'Padilla': 74,\n",
       " 'Entries': 75,\n",
       " 'starts': 76,\n",
       " 'City/Philadelphia': 77,\n",
       " 'staubt': 78,\n",
       " 'Sam': 79,\n",
       " 'Renee': 80,\n",
       " 'pitcher': 81,\n",
       " 'idk': 82,\n",
       " '#4lokos': 83,\n",
       " 'lunatic': 84,\n",
       " 'boyfriend': 85,\n",
       " '#BeatPikeview': 86,\n",
       " 'female': 87,\n",
       " 'hidden': 88,\n",
       " 'smells': 89,\n",
       " '4th-and-goal': 90,\n",
       " 'gifts': 91,\n",
       " 'Discount': 92,\n",
       " '04:32': 93,\n",
       " 'Active': 94,\n",
       " 'pobre': 95,\n",
       " '??!': 96,\n",
       " 'Whiteboard': 97,\n",
       " 'Stinson': 98,\n",
       " 'Maggi': 99,\n",
       " 'Walang': 100,\n",
       " 'Japan': 101,\n",
       " 'suportado': 102,\n",
       " 'Einstein': 103,\n",
       " '33': 104,\n",
       " 'Payson': 105,\n",
       " '$37K': 106,\n",
       " 'List': 107,\n",
       " 'Brother': 108,\n",
       " '1-3': 109,\n",
       " 'auditions': 110,\n",
       " 'pic.twitter.com/aL5Djt2F7m': 111,\n",
       " '*freakingoutkhsf*': 112,\n",
       " '630': 113,\n",
       " '#opm': 114,\n",
       " 'THING': 115,\n",
       " '~madi': 116,\n",
       " 'mid': 117,\n",
       " 'org': 118,\n",
       " 'did': 119,\n",
       " 'Pray': 120,\n",
       " 'law': 121,\n",
       " 'Newcastle': 122,\n",
       " 'Telecom': 123,\n",
       " '211': 124,\n",
       " 'haz': 125,\n",
       " 'lessons': 126,\n",
       " 'conclusions': 127,\n",
       " 'missing': 128,\n",
       " 'espagnol': 129,\n",
       " 'Adventist': 130,\n",
       " 'SJP': 131,\n",
       " '......': 132,\n",
       " 'exact': 133,\n",
       " 'Tennessee': 134,\n",
       " '#THURSDAY': 135,\n",
       " 'Only': 136,\n",
       " 'dette': 137,\n",
       " '#Maracay': 138,\n",
       " '#YaGunnersYa': 139,\n",
       " 'reason': 140,\n",
       " 'Loanbase': 141,\n",
       " 'YEARS': 142,\n",
       " 'maker': 143,\n",
       " 'Sacramentu': 144,\n",
       " 'Vanessa': 145,\n",
       " 'ODELL': 146,\n",
       " 'PSD': 147,\n",
       " 'Thru': 148,\n",
       " 'nigga': 149,\n",
       " 'STILL': 150,\n",
       " '#teamandy': 151,\n",
       " 'unattractive': 152,\n",
       " 'accused': 153,\n",
       " 'tries': 154,\n",
       " 'powder': 155,\n",
       " '4:17': 156,\n",
       " 'show-1': 157,\n",
       " 'Jets': 158,\n",
       " 'Alabang': 159,\n",
       " '#Boy_swag': 160,\n",
       " 'MA': 161,\n",
       " 'TheDeAndreWay': 162,\n",
       " 'RT': 163,\n",
       " '1245': 164,\n",
       " 'honey': 165,\n",
       " 'Homecoming': 166,\n",
       " 'coffee': 167,\n",
       " 'Vine': 168,\n",
       " 'Raises': 169,\n",
       " 'logged': 170,\n",
       " 'bertarungnya': 171,\n",
       " 'Again': 172,\n",
       " 'one-time': 173,\n",
       " 'hellish': 174,\n",
       " 'recuerdo': 175,\n",
       " 'Ave': 176,\n",
       " 'violence': 177,\n",
       " 'pair': 178,\n",
       " 'Conference': 179,\n",
       " 'kinds': 180,\n",
       " 'Lover': 181,\n",
       " 'rp': 182,\n",
       " 'Jessum': 183,\n",
       " 'tres': 184,\n",
       " '$20': 185,\n",
       " 'Pag': 186,\n",
       " 'WARRANTY': 187,\n",
       " '#GoldenGlobe': 188,\n",
       " '#HAPPYSa2015': 189,\n",
       " 'Allowing': 190,\n",
       " 'obituary': 191,\n",
       " 'depressed': 192,\n",
       " 'tulong': 193,\n",
       " '*PAG': 194,\n",
       " '#ecig': 195,\n",
       " 'Conv': 196,\n",
       " 'Villanueva': 197,\n",
       " 'foru': 198,\n",
       " '//': 199,\n",
       " '11alivepic.twitter.com/v2mlv7jveo': 200,\n",
       " '80+': 201,\n",
       " '#FindTheNearestExit': 202,\n",
       " 'mere': 203,\n",
       " 'Resets': 204,\n",
       " 'x6': 205,\n",
       " 'golfer': 206,\n",
       " 'hukhuk': 207,\n",
       " '#putterwatch2016': 208,\n",
       " 'sahabat': 209,\n",
       " 'page-notes': 210,\n",
       " 'map': 211,\n",
       " 'Sauipe': 212,\n",
       " 'spark': 213,\n",
       " 'faster': 214,\n",
       " 'Delirious': 215,\n",
       " 'Chari': 216,\n",
       " '8.908': 217,\n",
       " 'Swerdlow': 218,\n",
       " 'Faroes': 219,\n",
       " 'DRIVING': 220,\n",
       " '#bones': 221,\n",
       " 'KLH': 222,\n",
       " 'pretty': 223,\n",
       " 'anciennes': 224,\n",
       " '#chrisbrown': 225,\n",
       " 'U': 226,\n",
       " 'Probox': 227,\n",
       " 'Talking': 228,\n",
       " 'scrapbooking': 229,\n",
       " 'Identifies': 230,\n",
       " 'iyiyi': 231,\n",
       " 'Ohhhh': 232,\n",
       " 'Urban': 233,\n",
       " 'Staples': 234,\n",
       " 'HELL': 235,\n",
       " 'Belinelli': 236,\n",
       " 'less': 237,\n",
       " 'sasama': 238,\n",
       " 'Male': 239,\n",
       " 'galit': 240,\n",
       " 'Potros': 241,\n",
       " 'Kansas': 242,\n",
       " 'conduction': 243,\n",
       " 'ahi': 244,\n",
       " 'curvy': 245,\n",
       " 'realise': 246,\n",
       " '#ALDUBONLYYou': 247,\n",
       " 'kasama': 248,\n",
       " 'FT': 249,\n",
       " 'Western': 250,\n",
       " 'Nouvelle': 251,\n",
       " '#ThaHipHopHeadpic': 252,\n",
       " 'Donetsk': 253,\n",
       " '4D': 254,\n",
       " 'BEST': 255,\n",
       " 'Galesong': 256,\n",
       " 'ema': 257,\n",
       " 'given': 258,\n",
       " 'liked': 259,\n",
       " 'question': 260,\n",
       " 'winged': 261,\n",
       " 'Aqa': 262,\n",
       " '04:58': 263,\n",
       " 'Portland': 264,\n",
       " 'Amerika': 265,\n",
       " 'Love': 266,\n",
       " '#Duo': 267,\n",
       " 'Sundae': 268,\n",
       " 'susuko': 269,\n",
       " ':\"': 270,\n",
       " 'xxxxx': 271,\n",
       " 'Episodes': 272,\n",
       " 'XX': 273,\n",
       " 'lt': 274,\n",
       " 'consumed': 275,\n",
       " 'Featured': 276,\n",
       " '#SurvivorFinale': 277,\n",
       " 'twitpic': 278,\n",
       " 'Many': 279,\n",
       " 'khulay': 280,\n",
       " 'FAIR': 281,\n",
       " 'exempted': 282,\n",
       " '900': 283,\n",
       " '50': 284,\n",
       " 'yey': 285,\n",
       " '03:26': 286,\n",
       " 'scarey': 287,\n",
       " 'Zombies': 288,\n",
       " 'mort': 289,\n",
       " 'kang': 290,\n",
       " 'Wisconsin': 291,\n",
       " 'australia': 292,\n",
       " 'haters': 293,\n",
       " 'Imaginez': 294,\n",
       " 'Fryberg': 295,\n",
       " 'mahal': 296,\n",
       " 'strong': 297,\n",
       " 'Keganasan': 298,\n",
       " 'Promo': 299,\n",
       " '#BirthdayGift': 300,\n",
       " 'saturday': 301,\n",
       " 'stings': 302,\n",
       " 'LOVES': 303,\n",
       " 'Phone': 304,\n",
       " 'situation': 305,\n",
       " 'soph': 306,\n",
       " '06:38': 307,\n",
       " 'IMMA': 308,\n",
       " 'sunday': 309,\n",
       " 'pie': 310,\n",
       " 'Publisher': 311,\n",
       " 'Live': 312,\n",
       " 'salon': 313,\n",
       " 'research': 314,\n",
       " 'TAENA': 315,\n",
       " 'relay': 316,\n",
       " 'tenggelam': 317,\n",
       " 'seryoso': 318,\n",
       " 'KC': 319,\n",
       " 'Boxer': 320,\n",
       " 'Assessment': 321,\n",
       " 'Denver-area': 322,\n",
       " 'ASAP': 323,\n",
       " 'coin': 324,\n",
       " '#beerandpizza': 325,\n",
       " 'SOFT': 326,\n",
       " 'Embankment': 327,\n",
       " '\\\\\\\\Happy': 328,\n",
       " 'placed': 329,\n",
       " 'CountdownBegins': 330,\n",
       " 'MARIS': 331,\n",
       " 'kapuso': 332,\n",
       " 'surge': 333,\n",
       " 'suburban': 334,\n",
       " 'Rehab': 335,\n",
       " 'wAIT': 336,\n",
       " 'Chic': 337,\n",
       " '#NBATop10': 338,\n",
       " 'college': 339,\n",
       " 'tad': 340,\n",
       " 'ConservativeLA': 341,\n",
       " '*huffs': 342,\n",
       " 'listen': 343,\n",
       " 'Beast': 344,\n",
       " 'fallback': 345,\n",
       " 'church': 346,\n",
       " 'PG': 347,\n",
       " 'houses': 348,\n",
       " 'CLARK': 349,\n",
       " 'Inga': 350,\n",
       " '#TheRAC': 351,\n",
       " 'wth': 352,\n",
       " 'PSTBuy': 353,\n",
       " 'curren2956xiy': 354,\n",
       " '#noooooo': 355,\n",
       " 'SECTION': 356,\n",
       " 'DONATE': 357,\n",
       " 'premieres': 358,\n",
       " 'nationwide': 359,\n",
       " 'guilt': 360,\n",
       " 'mahulaan': 361,\n",
       " 'Margaret': 362,\n",
       " 'Beggers': 363,\n",
       " 'Elegante': 364,\n",
       " '#atk': 365,\n",
       " 'mark': 366,\n",
       " 'hain': 367,\n",
       " 'defence': 368,\n",
       " 'Tumilson': 369,\n",
       " '14:30': 370,\n",
       " 'Jadwal': 371,\n",
       " 'CANDY': 372,\n",
       " 'Bank': 373,\n",
       " 'rose': 374,\n",
       " 'Dolls': 375,\n",
       " 'sport': 376,\n",
       " 'humidity': 377,\n",
       " 'Reach': 378,\n",
       " 'technical': 379,\n",
       " 'vocal': 380,\n",
       " 'compliment': 381,\n",
       " 'yet(': 382,\n",
       " '#MLB': 383,\n",
       " 'disaster': 384,\n",
       " 'JUST': 385,\n",
       " '50.00': 386,\n",
       " 'ganna': 387,\n",
       " 'DING': 388,\n",
       " 'tos': 389,\n",
       " 'Shout': 390,\n",
       " 'cont': 391,\n",
       " 'sicker': 392,\n",
       " 'hospitals': 393,\n",
       " 'versa': 394,\n",
       " 'sorrow': 395,\n",
       " 'Wal-mart/swift': 396,\n",
       " 'kelas': 397,\n",
       " '#DreamLabRobot': 398,\n",
       " 'Commission': 399,\n",
       " 'Kuwait': 400,\n",
       " 'posts': 401,\n",
       " 'last': 402,\n",
       " 'teh': 403,\n",
       " 'watchdog': 404,\n",
       " 'suspenders': 405,\n",
       " 'LONG': 406,\n",
       " 'point': 407,\n",
       " 'Eyeopener': 408,\n",
       " '#datas': 409,\n",
       " 'navy': 410,\n",
       " 'Golf': 411,\n",
       " 'British…': 412,\n",
       " 'Stanford': 413,\n",
       " '#Tianjin': 414,\n",
       " 'dickt': 415,\n",
       " 'laws': 416,\n",
       " 'LIV': 417,\n",
       " '3:01': 418,\n",
       " 'borders': 419,\n",
       " 'Boris': 420,\n",
       " 'TOUR': 421,\n",
       " 'Analogy': 422,\n",
       " 'called': 423,\n",
       " '#007': 424,\n",
       " 'entertaining': 425,\n",
       " 'woaahh': 426,\n",
       " 'Phew': 427,\n",
       " 'investigating': 428,\n",
       " 'cuma': 429,\n",
       " 'mong': 430,\n",
       " '#ShowtimeTuLoyAngPasko': 431,\n",
       " 'kisses': 432,\n",
       " 'works': 433,\n",
       " 'croise': 434,\n",
       " 'Finally': 435,\n",
       " 'solid': 436,\n",
       " 'lola': 437,\n",
       " 'thirsty': 438,\n",
       " 'Celebration': 439,\n",
       " 'VPS': 440,\n",
       " '#Midvaleshooting': 441,\n",
       " 'Explorers': 442,\n",
       " 'HARLEM': 443,\n",
       " 'setiap': 444,\n",
       " 'series': 445,\n",
       " 'halls': 446,\n",
       " 'Certain': 447,\n",
       " 'Surrounded': 448,\n",
       " '#wineparty': 449,\n",
       " 'model': 450,\n",
       " '59': 451,\n",
       " 'SO': 452,\n",
       " 'Wwe': 453,\n",
       " '#baseball': 454,\n",
       " 'SAC': 455,\n",
       " 'sweetie': 456,\n",
       " 'Spoons': 457,\n",
       " 'Underground': 458,\n",
       " 'wolf': 459,\n",
       " 'Thursday': 460,\n",
       " 'H': 461,\n",
       " 'children': 462,\n",
       " 'looking': 463,\n",
       " '104': 464,\n",
       " 'FN': 465,\n",
       " 'blow': 466,\n",
       " 'fa': 467,\n",
       " 'back': 468,\n",
       " 'manage': 469,\n",
       " 'jsuis': 470,\n",
       " '179': 471,\n",
       " '#jobs': 472,\n",
       " 'Summer)': 473,\n",
       " '12.4': 474,\n",
       " '#neexpo': 475,\n",
       " 'webpage': 476,\n",
       " 'Bishop': 477,\n",
       " 'somebody': 478,\n",
       " 'Ask': 479,\n",
       " 'PS2': 480,\n",
       " 'chatting': 481,\n",
       " 'Lupita': 482,\n",
       " 'Whos': 483,\n",
       " 'faces': 484,\n",
       " 'Score': 485,\n",
       " 'rehearsal': 486,\n",
       " 'place': 487,\n",
       " 'wanna': 488,\n",
       " 'Infuriating': 489,\n",
       " 'shiny': 490,\n",
       " 'swole': 491,\n",
       " 'Replay': 492,\n",
       " 'Ladies': 493,\n",
       " 'tau': 494,\n",
       " 'flood': 495,\n",
       " '#Syria': 496,\n",
       " 'HA': 497,\n",
       " 'customers’': 498,\n",
       " 'Empire': 499,\n",
       " 'Newport': 500,\n",
       " 'BEFORE': 501,\n",
       " 'Courthouse': 502,\n",
       " 'Combo': 503,\n",
       " '#theroom': 504,\n",
       " 'ily*': 505,\n",
       " 'Surround': 506,\n",
       " 'backpackkk': 507,\n",
       " '09:31': 508,\n",
       " 'Utility': 509,\n",
       " 'unexpected': 510,\n",
       " 'inning': 511,\n",
       " 'RIP': 512,\n",
       " 'suprising': 513,\n",
       " 'consume': 514,\n",
       " 'Theta': 515,\n",
       " 'CNet': 516,\n",
       " 'mulu': 517,\n",
       " 'lolz': 518,\n",
       " 'Open': 519,\n",
       " 'Aabot': 520,\n",
       " 'Siaap': 521,\n",
       " 'measures': 522,\n",
       " 'TPCS': 523,\n",
       " 'support/safety': 524,\n",
       " 'because': 525,\n",
       " 'theatre': 526,\n",
       " '80s': 527,\n",
       " 'Hettinger': 528,\n",
       " '3hr': 529,\n",
       " 'silakan': 530,\n",
       " 'stunned': 531,\n",
       " 'Hilton': 532,\n",
       " 'Final': 533,\n",
       " 'original': 534,\n",
       " 'f': 535,\n",
       " 'TONIGHT': 536,\n",
       " 'Rohn': 537,\n",
       " 'Picchu': 538,\n",
       " 'videos': 539,\n",
       " 'Drop': 540,\n",
       " 'Hayes': 541,\n",
       " 'moving': 542,\n",
       " '9.13': 543,\n",
       " 'PA': 544,\n",
       " 'F-F-Friday': 545,\n",
       " 'belieber': 546,\n",
       " 'pic.twitter.com/cBfaHznxGU': 547,\n",
       " '9PM': 548,\n",
       " '70s': 549,\n",
       " 'Meijer': 550,\n",
       " 'initial': 551,\n",
       " 'entre': 552,\n",
       " 'Stickwitu': 553,\n",
       " 'SELLIN': 554,\n",
       " 'forces': 555,\n",
       " 'Barometric': 556,\n",
       " 'elderly': 557,\n",
       " '9|8c': 558,\n",
       " 'Mules': 559,\n",
       " 'snoop': 560,\n",
       " 'modern': 561,\n",
       " 'stubborn': 562,\n",
       " '12-26-14': 563,\n",
       " '09/27/2015': 564,\n",
       " '470': 565,\n",
       " 'debate': 566,\n",
       " 'thankk': 567,\n",
       " '#february': 568,\n",
       " 'World': 569,\n",
       " 'throat': 570,\n",
       " 'Terror': 571,\n",
       " 'NoThing': 572,\n",
       " 'Tip': 573,\n",
       " 'instead': 574,\n",
       " 'hihi': 575,\n",
       " 'Pol': 576,\n",
       " 'messing': 577,\n",
       " 'Level': 578,\n",
       " 'Caroline': 579,\n",
       " 'grumpy': 580,\n",
       " 'leader': 581,\n",
       " '#comcast': 582,\n",
       " 'tied': 583,\n",
       " 'PLAY': 584,\n",
       " 'earlier': 585,\n",
       " 'GR': 586,\n",
       " 'Awh': 587,\n",
       " 'Sorry': 588,\n",
       " '$6': 589,\n",
       " 'FOX': 590,\n",
       " '#nyc': 591,\n",
       " '#FortHood': 592,\n",
       " 'isr': 593,\n",
       " 'vie': 594,\n",
       " 'marketing': 595,\n",
       " 'Orchestrez': 596,\n",
       " 'yeeah': 597,\n",
       " 'avait': 598,\n",
       " 'standing': 599,\n",
       " 'securit': 600,\n",
       " '800': 601,\n",
       " 'WONDERFUL': 602,\n",
       " 'citizenry': 603,\n",
       " 'anxiety': 604,\n",
       " \"can't...\": 605,\n",
       " '09:29': 606,\n",
       " 'wd': 607,\n",
       " 'Enlarge)': 608,\n",
       " 'SILANG': 609,\n",
       " '1.4': 610,\n",
       " '#StraightOuttaCompton': 611,\n",
       " '1984': 612,\n",
       " 'sube': 613,\n",
       " 'solve': 614,\n",
       " 'aha': 615,\n",
       " 'Vodafone': 616,\n",
       " 'saved': 617,\n",
       " 'THISSS': 618,\n",
       " 'tattoo': 619,\n",
       " 'Corners': 620,\n",
       " '#Food': 621,\n",
       " 'Funny': 622,\n",
       " 'nope': 623,\n",
       " '2-4-1': 624,\n",
       " 'Terps-Illini': 625,\n",
       " 'Product': 626,\n",
       " 'collectif': 627,\n",
       " 'extension': 628,\n",
       " 'prospects': 629,\n",
       " 'HTML': 630,\n",
       " 'for': 631,\n",
       " 'surf': 632,\n",
       " 'daylight': 633,\n",
       " 'Saudi': 634,\n",
       " 'Midtown': 635,\n",
       " 'hogan': 636,\n",
       " '5': 637,\n",
       " 'Cullen': 638,\n",
       " 'lesson': 639,\n",
       " 'Ponderosa': 640,\n",
       " \"You'd\": 641,\n",
       " 'Due': 642,\n",
       " 'laughing': 643,\n",
       " 'Mbps': 644,\n",
       " 'Dortmund': 645,\n",
       " 'amat': 646,\n",
       " 'harbored': 647,\n",
       " 'Kalye': 648,\n",
       " 'sprints': 649,\n",
       " '599.00': 650,\n",
       " 'affecting': 651,\n",
       " 'kidddddd': 652,\n",
       " 'dating': 653,\n",
       " 'much': 654,\n",
       " 'Arthur': 655,\n",
       " 'Lifesitenews': 656,\n",
       " 'studio': 657,\n",
       " 'suffer': 658,\n",
       " 'Definitely': 659,\n",
       " 'sincerely': 660,\n",
       " '#law': 661,\n",
       " 'mini': 662,\n",
       " 'Stoppard': 663,\n",
       " 'bruder': 664,\n",
       " 'Mahal': 665,\n",
       " 'author': 666,\n",
       " 'Assange': 667,\n",
       " '#ReligionOfPeace': 668,\n",
       " 'Shane': 669,\n",
       " 'renew': 670,\n",
       " 'Flikken': 671,\n",
       " 'po': 672,\n",
       " 'Attackers': 673,\n",
       " 'Dzeko': 674,\n",
       " 'Gear】': 675,\n",
       " 'PERIOD': 676,\n",
       " 'hunting': 677,\n",
       " 'CW': 678,\n",
       " 'SUNDAY': 679,\n",
       " 'newborn': 680,\n",
       " 'UB': 681,\n",
       " 'Told': 682,\n",
       " '12.30': 683,\n",
       " '17-9-2010Tal': 684,\n",
       " 'neva': 685,\n",
       " 'ut': 686,\n",
       " 'jul': 687,\n",
       " 'definite': 688,\n",
       " '#KCA': 689,\n",
       " 'slowly': 690,\n",
       " 'DIANA': 691,\n",
       " 'Pac': 692,\n",
       " 'wala': 693,\n",
       " 'blond': 694,\n",
       " 'Wagner': 695,\n",
       " '#ACCESSTOHES': 696,\n",
       " 'Like': 697,\n",
       " 'Mariel': 698,\n",
       " 'Airmax': 699,\n",
       " '3PCS': 700,\n",
       " 'Dejavu': 701,\n",
       " 'Damn': 702,\n",
       " 'shoot': 703,\n",
       " 'FASTIN': 704,\n",
       " 'batukan': 705,\n",
       " 'Cyber-Ark': 706,\n",
       " 'Sungai': 707,\n",
       " '11/13/2015': 708,\n",
       " 'articles': 709,\n",
       " 'Anak': 710,\n",
       " 'fair': 711,\n",
       " 'mascara': 712,\n",
       " 'respond': 713,\n",
       " '490': 714,\n",
       " 'BISH': 715,\n",
       " 'Williams-Sonoma': 716,\n",
       " 'prepaid': 717,\n",
       " 'Codes': 718,\n",
       " 'Teen': 719,\n",
       " 'three-way': 720,\n",
       " 'Jasmin': 721,\n",
       " 'guestlist': 722,\n",
       " 'Charlie': 723,\n",
       " 'Gm': 724,\n",
       " 'witty': 725,\n",
       " '10K': 726,\n",
       " ':P': 727,\n",
       " 'Cliona': 728,\n",
       " 'fifth': 729,\n",
       " 'Vancouver': 730,\n",
       " '#amirite': 731,\n",
       " 'Selamat': 732,\n",
       " 'KNS': 733,\n",
       " 'XV': 734,\n",
       " 'mail': 735,\n",
       " 'iPads': 736,\n",
       " 'gumagawa': 737,\n",
       " '02:36': 738,\n",
       " '#cars': 739,\n",
       " 'Editor': 740,\n",
       " 'Nokia': 741,\n",
       " 'happier': 742,\n",
       " 'tF': 743,\n",
       " 'Id': 744,\n",
       " 'ahead': 745,\n",
       " 'must': 746,\n",
       " 'drapeau': 747,\n",
       " 'WATERPROOF': 748,\n",
       " '22h05m': 749,\n",
       " 'maliit': 750,\n",
       " 'kapag': 751,\n",
       " 'skating': 752,\n",
       " 'Lama': 753,\n",
       " '#FDI': 754,\n",
       " 'filling': 755,\n",
       " '205': 756,\n",
       " 'Facts': 757,\n",
       " 'Lent': 758,\n",
       " '#tips': 759,\n",
       " 'lightened': 760,\n",
       " \"d'humeur\": 761,\n",
       " 'quest': 762,\n",
       " 'mare': 763,\n",
       " '#EAv': 764,\n",
       " 'Yahoo': 765,\n",
       " 'This': 766,\n",
       " 'Dispatch': 767,\n",
       " 'taro': 768,\n",
       " 'UNC': 769,\n",
       " 'guys~': 770,\n",
       " 'franchise': 771,\n",
       " 'Update': 772,\n",
       " 'Vol': 773,\n",
       " 'spoil': 774,\n",
       " 'GS': 775,\n",
       " 'reolo.com': 776,\n",
       " 'Will': 777,\n",
       " 'external': 778,\n",
       " 'Depot': 779,\n",
       " 'kok': 780,\n",
       " '#GETEXCITED': 781,\n",
       " 'recalled': 782,\n",
       " '-And': 783,\n",
       " 'cedar': 784,\n",
       " 'udah': 785,\n",
       " 'discussion': 786,\n",
       " 'neighbours': 787,\n",
       " 'VR': 788,\n",
       " 'turnaround': 789,\n",
       " 'Haaaa': 790,\n",
       " '#i': 791,\n",
       " 'hoor': 792,\n",
       " '#Levels': 793,\n",
       " 'D-Day': 794,\n",
       " 'Kapag': 795,\n",
       " 'Cudi': 796,\n",
       " 'matsiar': 797,\n",
       " 'SPITE': 798,\n",
       " 'DVR': 799,\n",
       " '98-02': 800,\n",
       " '#Cobb': 801,\n",
       " 'SAMANA': 802,\n",
       " 'THREE-oh': 803,\n",
       " 'howitzer': 804,\n",
       " 'SAMPAI': 805,\n",
       " 'tbh': 806,\n",
       " 'Gets': 807,\n",
       " '#FourthTeenager': 808,\n",
       " 'progress': 809,\n",
       " 'stans': 810,\n",
       " 'caution': 811,\n",
       " '#DirtyWorkVideoTOMORROW': 812,\n",
       " 'Britain': 813,\n",
       " '#ENGJobs': 814,\n",
       " 'mins': 815,\n",
       " 'Shop': 816,\n",
       " 'BR': 817,\n",
       " 'Polar': 818,\n",
       " 'bright': 819,\n",
       " 'overestimated': 820,\n",
       " 'Israel': 821,\n",
       " 'nazi': 822,\n",
       " '$cat': 823,\n",
       " '2007': 824,\n",
       " 'individual': 825,\n",
       " 'Capricorn': 826,\n",
       " 'Da': 827,\n",
       " 'Wounded': 828,\n",
       " 'aming': 829,\n",
       " 'spencer': 830,\n",
       " 'M3': 831,\n",
       " 'China': 832,\n",
       " 'stupid': 833,\n",
       " '1pm': 834,\n",
       " 'driving': 835,\n",
       " '#wkndread': 836,\n",
       " 'nu': 837,\n",
       " '-Larry': 838,\n",
       " 'jeune': 839,\n",
       " '90s': 840,\n",
       " 'dominate': 841,\n",
       " 'COURSE': 842,\n",
       " 'Andrew': 843,\n",
       " 'Hollywood': 844,\n",
       " '01:00': 845,\n",
       " 'MCD': 846,\n",
       " 'McDonnell': 847,\n",
       " 'Slovenia': 848,\n",
       " 'park': 849,\n",
       " 'fully': 850,\n",
       " 'Scheduled': 851,\n",
       " 'orders': 852,\n",
       " 'MSN': 853,\n",
       " 'KAYO': 854,\n",
       " 'marshals': 855,\n",
       " 'aftern': 856,\n",
       " 'Liverpool': 857,\n",
       " 'Ada': 858,\n",
       " 'bungee': 859,\n",
       " 'COMPROMISED': 860,\n",
       " 'WWII': 861,\n",
       " 'connected': 862,\n",
       " '#Inlaws': 863,\n",
       " 'Philly': 864,\n",
       " '#ALDUBWayBackHome': 865,\n",
       " 'tina-type': 866,\n",
       " '<URL>': 867,\n",
       " '#ALDUBForTALKNTEXT': 868,\n",
       " \"C'mon\": 869,\n",
       " '1-2pm': 870,\n",
       " 'premiere': 871,\n",
       " 'many': 872,\n",
       " 'Million': 873,\n",
       " 'Interesting': 874,\n",
       " '14-Year-Old': 875,\n",
       " 'Trap': 876,\n",
       " 'Edin': 877,\n",
       " 'horses': 878,\n",
       " 'edits': 879,\n",
       " 'Gear': 880,\n",
       " 'nem': 881,\n",
       " 'dis': 882,\n",
       " '#bluelivesmatter': 883,\n",
       " 'baq': 884,\n",
       " \"n'est\": 885,\n",
       " 'khana': 886,\n",
       " 'korya': 887,\n",
       " 'KO': 888,\n",
       " 'Began': 889,\n",
       " 'tweaks': 890,\n",
       " '#REBOOT': 891,\n",
       " '#video': 892,\n",
       " 'impala': 893,\n",
       " 'LADY': 894,\n",
       " 'Motor': 895,\n",
       " 'thiiis': 896,\n",
       " 'rolling': 897,\n",
       " 'avec': 898,\n",
       " 'Bloomberg': 899,\n",
       " 'Hien': 900,\n",
       " 'Cry': 901,\n",
       " 'SaaS': 902,\n",
       " '#sanBrunofire': 903,\n",
       " 'Taylor': 904,\n",
       " 'Authentic': 905,\n",
       " '#Speech': 906,\n",
       " 'DIY': 907,\n",
       " 'Tweets': 908,\n",
       " 'Avengers': 909,\n",
       " 'ansley': 910,\n",
       " '#appfriday': 911,\n",
       " 'Tuesday-Fight': 912,\n",
       " 'Downs': 913,\n",
       " 'boundless': 914,\n",
       " 'receives': 915,\n",
       " '08:11': 916,\n",
       " '#samsungsocialtvdms': 917,\n",
       " 'Persuaders': 918,\n",
       " 'ustheduo': 919,\n",
       " 'Southern': 920,\n",
       " 'tthe': 921,\n",
       " 'library': 922,\n",
       " 'blacklisters': 923,\n",
       " 'result': 924,\n",
       " 'Marines': 925,\n",
       " 'selections': 926,\n",
       " 'ago': 927,\n",
       " \"TEACHER'S\": 928,\n",
       " 'Whatsap': 929,\n",
       " 'timepiece': 930,\n",
       " '2015)': 931,\n",
       " '*Danielle': 932,\n",
       " 'twist': 933,\n",
       " 'horan': 934,\n",
       " 'stressed': 935,\n",
       " '#SMM': 936,\n",
       " 'Popular': 937,\n",
       " 'Aye': 938,\n",
       " '#bricioz': 939,\n",
       " 'GMT-4': 940,\n",
       " 'FAIL': 941,\n",
       " 'those': 942,\n",
       " 'Rice': 943,\n",
       " 'Whiskey': 944,\n",
       " '#SharkNation': 945,\n",
       " 'fs': 946,\n",
       " 'participate': 947,\n",
       " '#Clubfitness': 948,\n",
       " 'Alejandro': 949,\n",
       " '#Infloodwetrust': 950,\n",
       " 'airing': 951,\n",
       " 'Cochrane': 952,\n",
       " 'Release': 953,\n",
       " 'ISSUES': 954,\n",
       " 'Bomb': 955,\n",
       " 'Anya': 956,\n",
       " '53+': 957,\n",
       " '#YomKippur': 958,\n",
       " 'Logging': 959,\n",
       " 'dogs': 960,\n",
       " 'TMA': 961,\n",
       " '#followingjob': 962,\n",
       " 'shootings': 963,\n",
       " '#PLL': 964,\n",
       " 'authorized': 965,\n",
       " 'NY': 966,\n",
       " 'Walsh': 967,\n",
       " 'shattered': 968,\n",
       " 'same': 969,\n",
       " 'fishes': 970,\n",
       " '#perfekterwochenstartundkaffe': 971,\n",
       " 'les': 972,\n",
       " 'DJ': 973,\n",
       " 'brutas': 974,\n",
       " 'Swish': 975,\n",
       " 'Came': 976,\n",
       " '#cybersecurityhttp': 977,\n",
       " 'of': 978,\n",
       " 'ICYMI': 979,\n",
       " \"d'espoir\": 980,\n",
       " 'position': 981,\n",
       " 'Magic': 982,\n",
       " 'Ilkay': 983,\n",
       " 'Brothers': 984,\n",
       " 'buat': 985,\n",
       " 'Potbelly': 986,\n",
       " 'stag': 987,\n",
       " 'mop': 988,\n",
       " 'Protests': 989,\n",
       " 'ahora': 990,\n",
       " 'dalam': 991,\n",
       " 'Next': 992,\n",
       " 'make': 993,\n",
       " 'Baldwin': 994,\n",
       " 'enga': 995,\n",
       " 'allegedly': 996,\n",
       " 'gunfire': 997,\n",
       " 'upload': 998,\n",
       " 'Had': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<UNK>',\n",
       " 1: 'kita',\n",
       " 2: 'poguthu',\n",
       " 3: '5thflr',\n",
       " 4: 'glassy',\n",
       " 5: 're',\n",
       " 6: 'expectations',\n",
       " 7: 'Twas',\n",
       " 8: 'lbs',\n",
       " 9: 'Ripe',\n",
       " 10: 'Sandra',\n",
       " 11: '#Interstate',\n",
       " 12: 'KNOW',\n",
       " 13: 'ooooh',\n",
       " 14: 'cassie',\n",
       " 15: 'chihuahua',\n",
       " 16: 'ADIDAS',\n",
       " 17: 'sizes',\n",
       " 18: 'Thomson',\n",
       " 19: 'Sneakercon',\n",
       " 20: 'manque',\n",
       " 21: '#gifted',\n",
       " 22: 'spanning',\n",
       " 23: '2ND',\n",
       " 24: 'pronote',\n",
       " 25: 'asleep',\n",
       " 26: 'steve',\n",
       " 27: 'AGUANT',\n",
       " 28: 'poslije',\n",
       " 29: 'Mandaric',\n",
       " 30: 'Biro',\n",
       " 31: 'vfdt',\n",
       " 32: 'striker',\n",
       " 33: 'write-in',\n",
       " 34: 'outweigh',\n",
       " 35: 'machines',\n",
       " 36: 'kept',\n",
       " 37: 'doubtful',\n",
       " 38: 'est',\n",
       " 39: 'brat',\n",
       " 40: 'mutually',\n",
       " 41: '101.6',\n",
       " 42: 'Shoes',\n",
       " 43: 'sing',\n",
       " 44: \"j'adore\",\n",
       " 45: 'intense',\n",
       " 46: 'manages',\n",
       " 47: '30-31',\n",
       " 48: 'El',\n",
       " 49: 'sail',\n",
       " 50: 'losers',\n",
       " 51: 'Gadgets',\n",
       " 52: 'profound',\n",
       " 53: 'hole',\n",
       " 54: 'Metal',\n",
       " 55: 'JavaMonkeys',\n",
       " 56: '#69',\n",
       " 57: 'Minimum',\n",
       " 58: 'Physics1998',\n",
       " 59: 'Square',\n",
       " 60: 'total',\n",
       " 61: 'pimple',\n",
       " 62: 'yummy',\n",
       " 63: 'Mendez',\n",
       " 64: 'natation',\n",
       " 65: '#nowplaying-',\n",
       " 66: 'struggled',\n",
       " 67: '9:14',\n",
       " 68: 'Calvin',\n",
       " 69: 'B-DAY',\n",
       " 70: '!?',\n",
       " 71: '#Thirstdays',\n",
       " 72: 'ntlegs11',\n",
       " 73: 'Coin',\n",
       " 74: 'Padilla',\n",
       " 75: 'Entries',\n",
       " 76: 'starts',\n",
       " 77: 'City/Philadelphia',\n",
       " 78: 'staubt',\n",
       " 79: 'Sam',\n",
       " 80: 'Renee',\n",
       " 81: 'pitcher',\n",
       " 82: 'idk',\n",
       " 83: '#4lokos',\n",
       " 84: 'lunatic',\n",
       " 85: 'boyfriend',\n",
       " 86: '#BeatPikeview',\n",
       " 87: 'female',\n",
       " 88: 'hidden',\n",
       " 89: 'smells',\n",
       " 90: '4th-and-goal',\n",
       " 91: 'gifts',\n",
       " 92: 'Discount',\n",
       " 93: '04:32',\n",
       " 94: 'Active',\n",
       " 95: 'pobre',\n",
       " 96: '??!',\n",
       " 97: 'Whiteboard',\n",
       " 98: 'Stinson',\n",
       " 99: 'Maggi',\n",
       " 100: 'Walang',\n",
       " 101: 'Japan',\n",
       " 102: 'suportado',\n",
       " 103: 'Einstein',\n",
       " 104: '33',\n",
       " 105: 'Payson',\n",
       " 106: '$37K',\n",
       " 107: 'List',\n",
       " 108: 'Brother',\n",
       " 109: '1-3',\n",
       " 110: 'auditions',\n",
       " 111: 'pic.twitter.com/aL5Djt2F7m',\n",
       " 112: '*freakingoutkhsf*',\n",
       " 113: '630',\n",
       " 114: '#opm',\n",
       " 115: 'THING',\n",
       " 116: '~madi',\n",
       " 117: 'mid',\n",
       " 118: 'org',\n",
       " 119: 'did',\n",
       " 120: 'Pray',\n",
       " 121: 'law',\n",
       " 122: 'Newcastle',\n",
       " 123: 'Telecom',\n",
       " 124: '211',\n",
       " 125: 'haz',\n",
       " 126: 'lessons',\n",
       " 127: 'conclusions',\n",
       " 128: 'missing',\n",
       " 129: 'espagnol',\n",
       " 130: 'Adventist',\n",
       " 131: 'SJP',\n",
       " 132: '......',\n",
       " 133: 'exact',\n",
       " 134: 'Tennessee',\n",
       " 135: '#THURSDAY',\n",
       " 136: 'Only',\n",
       " 137: 'dette',\n",
       " 138: '#Maracay',\n",
       " 139: '#YaGunnersYa',\n",
       " 140: 'reason',\n",
       " 141: 'Loanbase',\n",
       " 142: 'YEARS',\n",
       " 143: 'maker',\n",
       " 144: 'Sacramentu',\n",
       " 145: 'Vanessa',\n",
       " 146: 'ODELL',\n",
       " 147: 'PSD',\n",
       " 148: 'Thru',\n",
       " 149: 'nigga',\n",
       " 150: 'STILL',\n",
       " 151: '#teamandy',\n",
       " 152: 'unattractive',\n",
       " 153: 'accused',\n",
       " 154: 'tries',\n",
       " 155: 'powder',\n",
       " 156: '4:17',\n",
       " 157: 'show-1',\n",
       " 158: 'Jets',\n",
       " 159: 'Alabang',\n",
       " 160: '#Boy_swag',\n",
       " 161: 'MA',\n",
       " 162: 'TheDeAndreWay',\n",
       " 163: 'RT',\n",
       " 164: '1245',\n",
       " 165: 'honey',\n",
       " 166: 'Homecoming',\n",
       " 167: 'coffee',\n",
       " 168: 'Vine',\n",
       " 169: 'Raises',\n",
       " 170: 'logged',\n",
       " 171: 'bertarungnya',\n",
       " 172: 'Again',\n",
       " 173: 'one-time',\n",
       " 174: 'hellish',\n",
       " 175: 'recuerdo',\n",
       " 176: 'Ave',\n",
       " 177: 'violence',\n",
       " 178: 'pair',\n",
       " 179: 'Conference',\n",
       " 180: 'kinds',\n",
       " 181: 'Lover',\n",
       " 182: 'rp',\n",
       " 183: 'Jessum',\n",
       " 184: 'tres',\n",
       " 185: '$20',\n",
       " 186: 'Pag',\n",
       " 187: 'WARRANTY',\n",
       " 188: '#GoldenGlobe',\n",
       " 189: '#HAPPYSa2015',\n",
       " 190: 'Allowing',\n",
       " 191: 'obituary',\n",
       " 192: 'depressed',\n",
       " 193: 'tulong',\n",
       " 194: '*PAG',\n",
       " 195: '#ecig',\n",
       " 196: 'Conv',\n",
       " 197: 'Villanueva',\n",
       " 198: 'foru',\n",
       " 199: '//',\n",
       " 200: '11alivepic.twitter.com/v2mlv7jveo',\n",
       " 201: '80+',\n",
       " 202: '#FindTheNearestExit',\n",
       " 203: 'mere',\n",
       " 204: 'Resets',\n",
       " 205: 'x6',\n",
       " 206: 'golfer',\n",
       " 207: 'hukhuk',\n",
       " 208: '#putterwatch2016',\n",
       " 209: 'sahabat',\n",
       " 210: 'page-notes',\n",
       " 211: 'map',\n",
       " 212: 'Sauipe',\n",
       " 213: 'spark',\n",
       " 214: 'faster',\n",
       " 215: 'Delirious',\n",
       " 216: 'Chari',\n",
       " 217: '8.908',\n",
       " 218: 'Swerdlow',\n",
       " 219: 'Faroes',\n",
       " 220: 'DRIVING',\n",
       " 221: '#bones',\n",
       " 222: 'KLH',\n",
       " 223: 'pretty',\n",
       " 224: 'anciennes',\n",
       " 225: '#chrisbrown',\n",
       " 226: 'U',\n",
       " 227: 'Probox',\n",
       " 228: 'Talking',\n",
       " 229: 'scrapbooking',\n",
       " 230: 'Identifies',\n",
       " 231: 'iyiyi',\n",
       " 232: 'Ohhhh',\n",
       " 233: 'Urban',\n",
       " 234: 'Staples',\n",
       " 235: 'HELL',\n",
       " 236: 'Belinelli',\n",
       " 237: 'less',\n",
       " 238: 'sasama',\n",
       " 239: 'Male',\n",
       " 240: 'galit',\n",
       " 241: 'Potros',\n",
       " 242: 'Kansas',\n",
       " 243: 'conduction',\n",
       " 244: 'ahi',\n",
       " 245: 'curvy',\n",
       " 246: 'realise',\n",
       " 247: '#ALDUBONLYYou',\n",
       " 248: 'kasama',\n",
       " 249: 'FT',\n",
       " 250: 'Western',\n",
       " 251: 'Nouvelle',\n",
       " 252: '#ThaHipHopHeadpic',\n",
       " 253: 'Donetsk',\n",
       " 254: '4D',\n",
       " 255: 'BEST',\n",
       " 256: 'Galesong',\n",
       " 257: 'ema',\n",
       " 258: 'given',\n",
       " 259: 'liked',\n",
       " 260: 'question',\n",
       " 261: 'winged',\n",
       " 262: 'Aqa',\n",
       " 263: '04:58',\n",
       " 264: 'Portland',\n",
       " 265: 'Amerika',\n",
       " 266: 'Love',\n",
       " 267: '#Duo',\n",
       " 268: 'Sundae',\n",
       " 269: 'susuko',\n",
       " 270: ':\"',\n",
       " 271: 'xxxxx',\n",
       " 272: 'Episodes',\n",
       " 273: 'XX',\n",
       " 274: 'lt',\n",
       " 275: 'consumed',\n",
       " 276: 'Featured',\n",
       " 277: '#SurvivorFinale',\n",
       " 278: 'twitpic',\n",
       " 279: 'Many',\n",
       " 280: 'khulay',\n",
       " 281: 'FAIR',\n",
       " 282: 'exempted',\n",
       " 283: '900',\n",
       " 284: '50',\n",
       " 285: 'yey',\n",
       " 286: '03:26',\n",
       " 287: 'scarey',\n",
       " 288: 'Zombies',\n",
       " 289: 'mort',\n",
       " 290: 'kang',\n",
       " 291: 'Wisconsin',\n",
       " 292: 'australia',\n",
       " 293: 'haters',\n",
       " 294: 'Imaginez',\n",
       " 295: 'Fryberg',\n",
       " 296: 'mahal',\n",
       " 297: 'strong',\n",
       " 298: 'Keganasan',\n",
       " 299: 'Promo',\n",
       " 300: '#BirthdayGift',\n",
       " 301: 'saturday',\n",
       " 302: 'stings',\n",
       " 303: 'LOVES',\n",
       " 304: 'Phone',\n",
       " 305: 'situation',\n",
       " 306: 'soph',\n",
       " 307: '06:38',\n",
       " 308: 'IMMA',\n",
       " 309: 'sunday',\n",
       " 310: 'pie',\n",
       " 311: 'Publisher',\n",
       " 312: 'Live',\n",
       " 313: 'salon',\n",
       " 314: 'research',\n",
       " 315: 'TAENA',\n",
       " 316: 'relay',\n",
       " 317: 'tenggelam',\n",
       " 318: 'seryoso',\n",
       " 319: 'KC',\n",
       " 320: 'Boxer',\n",
       " 321: 'Assessment',\n",
       " 322: 'Denver-area',\n",
       " 323: 'ASAP',\n",
       " 324: 'coin',\n",
       " 325: '#beerandpizza',\n",
       " 326: 'SOFT',\n",
       " 327: 'Embankment',\n",
       " 328: '\\\\\\\\Happy',\n",
       " 329: 'placed',\n",
       " 330: 'CountdownBegins',\n",
       " 331: 'MARIS',\n",
       " 332: 'kapuso',\n",
       " 333: 'surge',\n",
       " 334: 'suburban',\n",
       " 335: 'Rehab',\n",
       " 336: 'wAIT',\n",
       " 337: 'Chic',\n",
       " 338: '#NBATop10',\n",
       " 339: 'college',\n",
       " 340: 'tad',\n",
       " 341: 'ConservativeLA',\n",
       " 342: '*huffs',\n",
       " 343: 'listen',\n",
       " 344: 'Beast',\n",
       " 345: 'fallback',\n",
       " 346: 'church',\n",
       " 347: 'PG',\n",
       " 348: 'houses',\n",
       " 349: 'CLARK',\n",
       " 350: 'Inga',\n",
       " 351: '#TheRAC',\n",
       " 352: 'wth',\n",
       " 353: 'PSTBuy',\n",
       " 354: 'curren2956xiy',\n",
       " 355: '#noooooo',\n",
       " 356: 'SECTION',\n",
       " 357: 'DONATE',\n",
       " 358: 'premieres',\n",
       " 359: 'nationwide',\n",
       " 360: 'guilt',\n",
       " 361: 'mahulaan',\n",
       " 362: 'Margaret',\n",
       " 363: 'Beggers',\n",
       " 364: 'Elegante',\n",
       " 365: '#atk',\n",
       " 366: 'mark',\n",
       " 367: 'hain',\n",
       " 368: 'defence',\n",
       " 369: 'Tumilson',\n",
       " 370: '14:30',\n",
       " 371: 'Jadwal',\n",
       " 372: 'CANDY',\n",
       " 373: 'Bank',\n",
       " 374: 'rose',\n",
       " 375: 'Dolls',\n",
       " 376: 'sport',\n",
       " 377: 'humidity',\n",
       " 378: 'Reach',\n",
       " 379: 'technical',\n",
       " 380: 'vocal',\n",
       " 381: 'compliment',\n",
       " 382: 'yet(',\n",
       " 383: '#MLB',\n",
       " 384: 'disaster',\n",
       " 385: 'JUST',\n",
       " 386: '50.00',\n",
       " 387: 'ganna',\n",
       " 388: 'DING',\n",
       " 389: 'tos',\n",
       " 390: 'Shout',\n",
       " 391: 'cont',\n",
       " 392: 'sicker',\n",
       " 393: 'hospitals',\n",
       " 394: 'versa',\n",
       " 395: 'sorrow',\n",
       " 396: 'Wal-mart/swift',\n",
       " 397: 'kelas',\n",
       " 398: '#DreamLabRobot',\n",
       " 399: 'Commission',\n",
       " 400: 'Kuwait',\n",
       " 401: 'posts',\n",
       " 402: 'last',\n",
       " 403: 'teh',\n",
       " 404: 'watchdog',\n",
       " 405: 'suspenders',\n",
       " 406: 'LONG',\n",
       " 407: 'point',\n",
       " 408: 'Eyeopener',\n",
       " 409: '#datas',\n",
       " 410: 'navy',\n",
       " 411: 'Golf',\n",
       " 412: 'British…',\n",
       " 413: 'Stanford',\n",
       " 414: '#Tianjin',\n",
       " 415: 'dickt',\n",
       " 416: 'laws',\n",
       " 417: 'LIV',\n",
       " 418: '3:01',\n",
       " 419: 'borders',\n",
       " 420: 'Boris',\n",
       " 421: 'TOUR',\n",
       " 422: 'Analogy',\n",
       " 423: 'called',\n",
       " 424: '#007',\n",
       " 425: 'entertaining',\n",
       " 426: 'woaahh',\n",
       " 427: 'Phew',\n",
       " 428: 'investigating',\n",
       " 429: 'cuma',\n",
       " 430: 'mong',\n",
       " 431: '#ShowtimeTuLoyAngPasko',\n",
       " 432: 'kisses',\n",
       " 433: 'works',\n",
       " 434: 'croise',\n",
       " 435: 'Finally',\n",
       " 436: 'solid',\n",
       " 437: 'lola',\n",
       " 438: 'thirsty',\n",
       " 439: 'Celebration',\n",
       " 440: 'VPS',\n",
       " 441: '#Midvaleshooting',\n",
       " 442: 'Explorers',\n",
       " 443: 'HARLEM',\n",
       " 444: 'setiap',\n",
       " 445: 'series',\n",
       " 446: 'halls',\n",
       " 447: 'Certain',\n",
       " 448: 'Surrounded',\n",
       " 449: '#wineparty',\n",
       " 450: 'model',\n",
       " 451: '59',\n",
       " 452: 'SO',\n",
       " 453: 'Wwe',\n",
       " 454: '#baseball',\n",
       " 455: 'SAC',\n",
       " 456: 'sweetie',\n",
       " 457: 'Spoons',\n",
       " 458: 'Underground',\n",
       " 459: 'wolf',\n",
       " 460: 'Thursday',\n",
       " 461: 'H',\n",
       " 462: 'children',\n",
       " 463: 'looking',\n",
       " 464: '104',\n",
       " 465: 'FN',\n",
       " 466: 'blow',\n",
       " 467: 'fa',\n",
       " 468: 'back',\n",
       " 469: 'manage',\n",
       " 470: 'jsuis',\n",
       " 471: '179',\n",
       " 472: '#jobs',\n",
       " 473: 'Summer)',\n",
       " 474: '12.4',\n",
       " 475: '#neexpo',\n",
       " 476: 'webpage',\n",
       " 477: 'Bishop',\n",
       " 478: 'somebody',\n",
       " 479: 'Ask',\n",
       " 480: 'PS2',\n",
       " 481: 'chatting',\n",
       " 482: 'Lupita',\n",
       " 483: 'Whos',\n",
       " 484: 'faces',\n",
       " 485: 'Score',\n",
       " 486: 'rehearsal',\n",
       " 487: 'place',\n",
       " 488: 'wanna',\n",
       " 489: 'Infuriating',\n",
       " 490: 'shiny',\n",
       " 491: 'swole',\n",
       " 492: 'Replay',\n",
       " 493: 'Ladies',\n",
       " 494: 'tau',\n",
       " 495: 'flood',\n",
       " 496: '#Syria',\n",
       " 497: 'HA',\n",
       " 498: 'customers’',\n",
       " 499: 'Empire',\n",
       " 500: 'Newport',\n",
       " 501: 'BEFORE',\n",
       " 502: 'Courthouse',\n",
       " 503: 'Combo',\n",
       " 504: '#theroom',\n",
       " 505: 'ily*',\n",
       " 506: 'Surround',\n",
       " 507: 'backpackkk',\n",
       " 508: '09:31',\n",
       " 509: 'Utility',\n",
       " 510: 'unexpected',\n",
       " 511: 'inning',\n",
       " 512: 'RIP',\n",
       " 513: 'suprising',\n",
       " 514: 'consume',\n",
       " 515: 'Theta',\n",
       " 516: 'CNet',\n",
       " 517: 'mulu',\n",
       " 518: 'lolz',\n",
       " 519: 'Open',\n",
       " 520: 'Aabot',\n",
       " 521: 'Siaap',\n",
       " 522: 'measures',\n",
       " 523: 'TPCS',\n",
       " 524: 'support/safety',\n",
       " 525: 'because',\n",
       " 526: 'theatre',\n",
       " 527: '80s',\n",
       " 528: 'Hettinger',\n",
       " 529: '3hr',\n",
       " 530: 'silakan',\n",
       " 531: 'stunned',\n",
       " 532: 'Hilton',\n",
       " 533: 'Final',\n",
       " 534: 'original',\n",
       " 535: 'f',\n",
       " 536: 'TONIGHT',\n",
       " 537: 'Rohn',\n",
       " 538: 'Picchu',\n",
       " 539: 'videos',\n",
       " 540: 'Drop',\n",
       " 541: 'Hayes',\n",
       " 542: 'moving',\n",
       " 543: '9.13',\n",
       " 544: 'PA',\n",
       " 545: 'F-F-Friday',\n",
       " 546: 'belieber',\n",
       " 547: 'pic.twitter.com/cBfaHznxGU',\n",
       " 548: '9PM',\n",
       " 549: '70s',\n",
       " 550: 'Meijer',\n",
       " 551: 'initial',\n",
       " 552: 'entre',\n",
       " 553: 'Stickwitu',\n",
       " 554: 'SELLIN',\n",
       " 555: 'forces',\n",
       " 556: 'Barometric',\n",
       " 557: 'elderly',\n",
       " 558: '9|8c',\n",
       " 559: 'Mules',\n",
       " 560: 'snoop',\n",
       " 561: 'modern',\n",
       " 562: 'stubborn',\n",
       " 563: '12-26-14',\n",
       " 564: '09/27/2015',\n",
       " 565: '470',\n",
       " 566: 'debate',\n",
       " 567: 'thankk',\n",
       " 568: '#february',\n",
       " 569: 'World',\n",
       " 570: 'throat',\n",
       " 571: 'Terror',\n",
       " 572: 'NoThing',\n",
       " 573: 'Tip',\n",
       " 574: 'instead',\n",
       " 575: 'hihi',\n",
       " 576: 'Pol',\n",
       " 577: 'messing',\n",
       " 578: 'Level',\n",
       " 579: 'Caroline',\n",
       " 580: 'grumpy',\n",
       " 581: 'leader',\n",
       " 582: '#comcast',\n",
       " 583: 'tied',\n",
       " 584: 'PLAY',\n",
       " 585: 'earlier',\n",
       " 586: 'GR',\n",
       " 587: 'Awh',\n",
       " 588: 'Sorry',\n",
       " 589: '$6',\n",
       " 590: 'FOX',\n",
       " 591: '#nyc',\n",
       " 592: '#FortHood',\n",
       " 593: 'isr',\n",
       " 594: 'vie',\n",
       " 595: 'marketing',\n",
       " 596: 'Orchestrez',\n",
       " 597: 'yeeah',\n",
       " 598: 'avait',\n",
       " 599: 'standing',\n",
       " 600: 'securit',\n",
       " 601: '800',\n",
       " 602: 'WONDERFUL',\n",
       " 603: 'citizenry',\n",
       " 604: 'anxiety',\n",
       " 605: \"can't...\",\n",
       " 606: '09:29',\n",
       " 607: 'wd',\n",
       " 608: 'Enlarge)',\n",
       " 609: 'SILANG',\n",
       " 610: '1.4',\n",
       " 611: '#StraightOuttaCompton',\n",
       " 612: '1984',\n",
       " 613: 'sube',\n",
       " 614: 'solve',\n",
       " 615: 'aha',\n",
       " 616: 'Vodafone',\n",
       " 617: 'saved',\n",
       " 618: 'THISSS',\n",
       " 619: 'tattoo',\n",
       " 620: 'Corners',\n",
       " 621: '#Food',\n",
       " 622: 'Funny',\n",
       " 623: 'nope',\n",
       " 624: '2-4-1',\n",
       " 625: 'Terps-Illini',\n",
       " 626: 'Product',\n",
       " 627: 'collectif',\n",
       " 628: 'extension',\n",
       " 629: 'prospects',\n",
       " 630: 'HTML',\n",
       " 631: 'for',\n",
       " 632: 'surf',\n",
       " 633: 'daylight',\n",
       " 634: 'Saudi',\n",
       " 635: 'Midtown',\n",
       " 636: 'hogan',\n",
       " 637: '5',\n",
       " 638: 'Cullen',\n",
       " 639: 'lesson',\n",
       " 640: 'Ponderosa',\n",
       " 641: \"You'd\",\n",
       " 642: 'Due',\n",
       " 643: 'laughing',\n",
       " 644: 'Mbps',\n",
       " 645: 'Dortmund',\n",
       " 646: 'amat',\n",
       " 647: 'harbored',\n",
       " 648: 'Kalye',\n",
       " 649: 'sprints',\n",
       " 650: '599.00',\n",
       " 651: 'affecting',\n",
       " 652: 'kidddddd',\n",
       " 653: 'dating',\n",
       " 654: 'much',\n",
       " 655: 'Arthur',\n",
       " 656: 'Lifesitenews',\n",
       " 657: 'studio',\n",
       " 658: 'suffer',\n",
       " 659: 'Definitely',\n",
       " 660: 'sincerely',\n",
       " 661: '#law',\n",
       " 662: 'mini',\n",
       " 663: 'Stoppard',\n",
       " 664: 'bruder',\n",
       " 665: 'Mahal',\n",
       " 666: 'author',\n",
       " 667: 'Assange',\n",
       " 668: '#ReligionOfPeace',\n",
       " 669: 'Shane',\n",
       " 670: 'renew',\n",
       " 671: 'Flikken',\n",
       " 672: 'po',\n",
       " 673: 'Attackers',\n",
       " 674: 'Dzeko',\n",
       " 675: 'Gear】',\n",
       " 676: 'PERIOD',\n",
       " 677: 'hunting',\n",
       " 678: 'CW',\n",
       " 679: 'SUNDAY',\n",
       " 680: 'newborn',\n",
       " 681: 'UB',\n",
       " 682: 'Told',\n",
       " 683: '12.30',\n",
       " 684: '17-9-2010Tal',\n",
       " 685: 'neva',\n",
       " 686: 'ut',\n",
       " 687: 'jul',\n",
       " 688: 'definite',\n",
       " 689: '#KCA',\n",
       " 690: 'slowly',\n",
       " 691: 'DIANA',\n",
       " 692: 'Pac',\n",
       " 693: 'wala',\n",
       " 694: 'blond',\n",
       " 695: 'Wagner',\n",
       " 696: '#ACCESSTOHES',\n",
       " 697: 'Like',\n",
       " 698: 'Mariel',\n",
       " 699: 'Airmax',\n",
       " 700: '3PCS',\n",
       " 701: 'Dejavu',\n",
       " 702: 'Damn',\n",
       " 703: 'shoot',\n",
       " 704: 'FASTIN',\n",
       " 705: 'batukan',\n",
       " 706: 'Cyber-Ark',\n",
       " 707: 'Sungai',\n",
       " 708: '11/13/2015',\n",
       " 709: 'articles',\n",
       " 710: 'Anak',\n",
       " 711: 'fair',\n",
       " 712: 'mascara',\n",
       " 713: 'respond',\n",
       " 714: '490',\n",
       " 715: 'BISH',\n",
       " 716: 'Williams-Sonoma',\n",
       " 717: 'prepaid',\n",
       " 718: 'Codes',\n",
       " 719: 'Teen',\n",
       " 720: 'three-way',\n",
       " 721: 'Jasmin',\n",
       " 722: 'guestlist',\n",
       " 723: 'Charlie',\n",
       " 724: 'Gm',\n",
       " 725: 'witty',\n",
       " 726: '10K',\n",
       " 727: ':P',\n",
       " 728: 'Cliona',\n",
       " 729: 'fifth',\n",
       " 730: 'Vancouver',\n",
       " 731: '#amirite',\n",
       " 732: 'Selamat',\n",
       " 733: 'KNS',\n",
       " 734: 'XV',\n",
       " 735: 'mail',\n",
       " 736: 'iPads',\n",
       " 737: 'gumagawa',\n",
       " 738: '02:36',\n",
       " 739: '#cars',\n",
       " 740: 'Editor',\n",
       " 741: 'Nokia',\n",
       " 742: 'happier',\n",
       " 743: 'tF',\n",
       " 744: 'Id',\n",
       " 745: 'ahead',\n",
       " 746: 'must',\n",
       " 747: 'drapeau',\n",
       " 748: 'WATERPROOF',\n",
       " 749: '22h05m',\n",
       " 750: 'maliit',\n",
       " 751: 'kapag',\n",
       " 752: 'skating',\n",
       " 753: 'Lama',\n",
       " 754: '#FDI',\n",
       " 755: 'filling',\n",
       " 756: '205',\n",
       " 757: 'Facts',\n",
       " 758: 'Lent',\n",
       " 759: '#tips',\n",
       " 760: 'lightened',\n",
       " 761: \"d'humeur\",\n",
       " 762: 'quest',\n",
       " 763: 'mare',\n",
       " 764: '#EAv',\n",
       " 765: 'Yahoo',\n",
       " 766: 'This',\n",
       " 767: 'Dispatch',\n",
       " 768: 'taro',\n",
       " 769: 'UNC',\n",
       " 770: 'guys~',\n",
       " 771: 'franchise',\n",
       " 772: 'Update',\n",
       " 773: 'Vol',\n",
       " 774: 'spoil',\n",
       " 775: 'GS',\n",
       " 776: 'reolo.com',\n",
       " 777: 'Will',\n",
       " 778: 'external',\n",
       " 779: 'Depot',\n",
       " 780: 'kok',\n",
       " 781: '#GETEXCITED',\n",
       " 782: 'recalled',\n",
       " 783: '-And',\n",
       " 784: 'cedar',\n",
       " 785: 'udah',\n",
       " 786: 'discussion',\n",
       " 787: 'neighbours',\n",
       " 788: 'VR',\n",
       " 789: 'turnaround',\n",
       " 790: 'Haaaa',\n",
       " 791: '#i',\n",
       " 792: 'hoor',\n",
       " 793: '#Levels',\n",
       " 794: 'D-Day',\n",
       " 795: 'Kapag',\n",
       " 796: 'Cudi',\n",
       " 797: 'matsiar',\n",
       " 798: 'SPITE',\n",
       " 799: 'DVR',\n",
       " 800: '98-02',\n",
       " 801: '#Cobb',\n",
       " 802: 'SAMANA',\n",
       " 803: 'THREE-oh',\n",
       " 804: 'howitzer',\n",
       " 805: 'SAMPAI',\n",
       " 806: 'tbh',\n",
       " 807: 'Gets',\n",
       " 808: '#FourthTeenager',\n",
       " 809: 'progress',\n",
       " 810: 'stans',\n",
       " 811: 'caution',\n",
       " 812: '#DirtyWorkVideoTOMORROW',\n",
       " 813: 'Britain',\n",
       " 814: '#ENGJobs',\n",
       " 815: 'mins',\n",
       " 816: 'Shop',\n",
       " 817: 'BR',\n",
       " 818: 'Polar',\n",
       " 819: 'bright',\n",
       " 820: 'overestimated',\n",
       " 821: 'Israel',\n",
       " 822: 'nazi',\n",
       " 823: '$cat',\n",
       " 824: '2007',\n",
       " 825: 'individual',\n",
       " 826: 'Capricorn',\n",
       " 827: 'Da',\n",
       " 828: 'Wounded',\n",
       " 829: 'aming',\n",
       " 830: 'spencer',\n",
       " 831: 'M3',\n",
       " 832: 'China',\n",
       " 833: 'stupid',\n",
       " 834: '1pm',\n",
       " 835: 'driving',\n",
       " 836: '#wkndread',\n",
       " 837: 'nu',\n",
       " 838: '-Larry',\n",
       " 839: 'jeune',\n",
       " 840: '90s',\n",
       " 841: 'dominate',\n",
       " 842: 'COURSE',\n",
       " 843: 'Andrew',\n",
       " 844: 'Hollywood',\n",
       " 845: '01:00',\n",
       " 846: 'MCD',\n",
       " 847: 'McDonnell',\n",
       " 848: 'Slovenia',\n",
       " 849: 'park',\n",
       " 850: 'fully',\n",
       " 851: 'Scheduled',\n",
       " 852: 'orders',\n",
       " 853: 'MSN',\n",
       " 854: 'KAYO',\n",
       " 855: 'marshals',\n",
       " 856: 'aftern',\n",
       " 857: 'Liverpool',\n",
       " 858: 'Ada',\n",
       " 859: 'bungee',\n",
       " 860: 'COMPROMISED',\n",
       " 861: 'WWII',\n",
       " 862: 'connected',\n",
       " 863: '#Inlaws',\n",
       " 864: 'Philly',\n",
       " 865: '#ALDUBWayBackHome',\n",
       " 866: 'tina-type',\n",
       " 867: '<URL>',\n",
       " 868: '#ALDUBForTALKNTEXT',\n",
       " 869: \"C'mon\",\n",
       " 870: '1-2pm',\n",
       " 871: 'premiere',\n",
       " 872: 'many',\n",
       " 873: 'Million',\n",
       " 874: 'Interesting',\n",
       " 875: '14-Year-Old',\n",
       " 876: 'Trap',\n",
       " 877: 'Edin',\n",
       " 878: 'horses',\n",
       " 879: 'edits',\n",
       " 880: 'Gear',\n",
       " 881: 'nem',\n",
       " 882: 'dis',\n",
       " 883: '#bluelivesmatter',\n",
       " 884: 'baq',\n",
       " 885: \"n'est\",\n",
       " 886: 'khana',\n",
       " 887: 'korya',\n",
       " 888: 'KO',\n",
       " 889: 'Began',\n",
       " 890: 'tweaks',\n",
       " 891: '#REBOOT',\n",
       " 892: '#video',\n",
       " 893: 'impala',\n",
       " 894: 'LADY',\n",
       " 895: 'Motor',\n",
       " 896: 'thiiis',\n",
       " 897: 'rolling',\n",
       " 898: 'avec',\n",
       " 899: 'Bloomberg',\n",
       " 900: 'Hien',\n",
       " 901: 'Cry',\n",
       " 902: 'SaaS',\n",
       " 903: '#sanBrunofire',\n",
       " 904: 'Taylor',\n",
       " 905: 'Authentic',\n",
       " 906: '#Speech',\n",
       " 907: 'DIY',\n",
       " 908: 'Tweets',\n",
       " 909: 'Avengers',\n",
       " 910: 'ansley',\n",
       " 911: '#appfriday',\n",
       " 912: 'Tuesday-Fight',\n",
       " 913: 'Downs',\n",
       " 914: 'boundless',\n",
       " 915: 'receives',\n",
       " 916: '08:11',\n",
       " 917: '#samsungsocialtvdms',\n",
       " 918: 'Persuaders',\n",
       " 919: 'ustheduo',\n",
       " 920: 'Southern',\n",
       " 921: 'tthe',\n",
       " 922: 'library',\n",
       " 923: 'blacklisters',\n",
       " 924: 'result',\n",
       " 925: 'Marines',\n",
       " 926: 'selections',\n",
       " 927: 'ago',\n",
       " 928: \"TEACHER'S\",\n",
       " 929: 'Whatsap',\n",
       " 930: 'timepiece',\n",
       " 931: '2015)',\n",
       " 932: '*Danielle',\n",
       " 933: 'twist',\n",
       " 934: 'horan',\n",
       " 935: 'stressed',\n",
       " 936: '#SMM',\n",
       " 937: 'Popular',\n",
       " 938: 'Aye',\n",
       " 939: '#bricioz',\n",
       " 940: 'GMT-4',\n",
       " 941: 'FAIL',\n",
       " 942: 'those',\n",
       " 943: 'Rice',\n",
       " 944: 'Whiskey',\n",
       " 945: '#SharkNation',\n",
       " 946: 'fs',\n",
       " 947: 'participate',\n",
       " 948: '#Clubfitness',\n",
       " 949: 'Alejandro',\n",
       " 950: '#Infloodwetrust',\n",
       " 951: 'airing',\n",
       " 952: 'Cochrane',\n",
       " 953: 'Release',\n",
       " 954: 'ISSUES',\n",
       " 955: 'Bomb',\n",
       " 956: 'Anya',\n",
       " 957: '53+',\n",
       " 958: '#YomKippur',\n",
       " 959: 'Logging',\n",
       " 960: 'dogs',\n",
       " 961: 'TMA',\n",
       " 962: '#followingjob',\n",
       " 963: 'shootings',\n",
       " 964: '#PLL',\n",
       " 965: 'authorized',\n",
       " 966: 'NY',\n",
       " 967: 'Walsh',\n",
       " 968: 'shattered',\n",
       " 969: 'same',\n",
       " 970: 'fishes',\n",
       " 971: '#perfekterwochenstartundkaffe',\n",
       " 972: 'les',\n",
       " 973: 'DJ',\n",
       " 974: 'brutas',\n",
       " 975: 'Swish',\n",
       " 976: 'Came',\n",
       " 977: '#cybersecurityhttp',\n",
       " 978: 'of',\n",
       " 979: 'ICYMI',\n",
       " 980: \"d'espoir\",\n",
       " 981: 'position',\n",
       " 982: 'Magic',\n",
       " 983: 'Ilkay',\n",
       " 984: 'Brothers',\n",
       " 985: 'buat',\n",
       " 986: 'Potbelly',\n",
       " 987: 'stag',\n",
       " 988: 'mop',\n",
       " 989: 'Protests',\n",
       " 990: 'ahora',\n",
       " 991: 'dalam',\n",
       " 992: 'Next',\n",
       " 993: 'make',\n",
       " 994: 'Baldwin',\n",
       " 995: 'enga',\n",
       " 996: 'allegedly',\n",
       " 997: 'gunfire',\n",
       " 998: 'upload',\n",
       " 999: 'Had',\n",
       " ...}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! __We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM).__ Dense layer will be used on top to perform tag classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__First, we need to create [placeholders](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder) to specify what data we are going to feed into the network during the execution time.__  For this task we will need the following placeholders:\n",
    " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
    " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
    "\n",
    "It could be noticed that we use *None* in the shapes in the declaration, which means that data of any size can be feeded. \n",
    "\n",
    "You need to complete the function *declare_placeholders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') # [batch_size, sequence_len]\n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "    \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to perform some __preparatory steps__: \n",
    " \n",
    "- Create __embeddings matrix__ with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable). Specify its name (*embeddings_matrix*), type  (*tf.float32*), and __initialize with random values__.\n",
    "- Create __forward and backward LSTM cells__. TensorFlow provides a number of RNN cells ready for you. We suggest that you use *LSTMCell*, but you can also experiment with other types, e.g. GRU cells. [This](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blogpost could be interesting if you want to learn more about the differences.\n",
    "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
    " \n",
    "After that, you can build the computation graph that transforms an input_batch:\n",
    "\n",
    "- [Look up](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) embeddings for an *input_batch* in the prepared *embedding_matrix*.\n",
    "- Pass the embeddings through [Bidirectional Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) with the specified forward and backward cells. Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
    "- Create a dense layer on top. Its output will be used directly in loss function.  \n",
    " \n",
    "Fill in the code below. In case you need to debug something, the easiest way is to check that tensor shapes of each step match the expected ones. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, dtype=tf.float32)\n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn),\n",
    "                                                 input_keep_prob=self.dropout_ph,\n",
    "                                                 output_keep_prob=self.dropout_ph,\n",
    "                                                 state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn),\n",
    "                                                 input_keep_prob=self.dropout_ph,\n",
    "                                                 output_keep_prob=self.dropout_ph,\n",
    "                                                 state_keep_prob=self.dropout_ph)\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings =  tf.nn.embedding_lookup(params=embedding_matrix_variable, ids=self.input_batch)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_cell, \n",
    "                                                                         cell_bw=backward_cell, \n",
    "                                                                         inputs=embeddings, \n",
    "                                                                         sequence_length=self.lengths, \n",
    "                                                                         dtype=tf.float32)\n",
    "                                                 \n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the actual predictions of the neural network, you need to apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    softmax_output = tf.nn.softmax(logits=self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(input=softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "During training we do not need predictions of the network, but we need a loss function. We will use [cross-entropy loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), efficiently implemented in TF as \n",
    "[cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2). Note that it should be applied to logits of the model (not to softmax probabilities!). Also note,  that we do not want to take into account loss terms coming from `<PAD>` tokens. So we need to mask them out, before computing [mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits_v2)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    # Be careful that the argument of tf.reduce_mean should be\n",
    "    # multiplication of mask and loss_tensor.\n",
    "    self.loss =  tf.reduce_mean(tf.cast(x=tf.boolean_mask(tensor=loss_tensor, mask=mask), dtype=tf.float32), keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to specify is how we want to optimize the loss. \n",
    "We suggest that you use [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) optimizer with a learning rate from the corresponding placeholder. \n",
    "You will also need to apply clipping to eliminate exploding gradients. It can be easily done with [clip_by_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars also contains variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(i[0],clip_norm), i[1]) for i in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations! You have specified all the parts of your network. You may have noticed, that we didn't deal with any real data yet, so what you have written is just recipes on how the network should function.\n",
    "Now we will put them to the constructor of our Bi-LSTM class to use it in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*, which was declared in *perform_optimization*. To predict tags, we just need to compute *self.predictions*. Anyway, we need to feed actual data through the placeholders that we defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *predict_for_batch* by initializing *feed_dict* with input *x_batch* and *lengths* and running the *session* for *self.predictions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    feed_dict = {self.input_batch: x_batch, self.lengths: lengths}\n",
    "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished with necessary methods of our BiLSTMModel model and almost ready to start experimenting.\n",
    "\n",
    "### Evaluation \n",
    "To simplify the evaluation process we provide two functions for you:\n",
    " - *predict_tags*: uses a model to get predictions and transforms indices to tokens and tags;\n",
    " - *eval_conll*: calculates precision, recall and F1 for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create *BiLSTMModel* model with the following parameters:\n",
    " - *vocabulary_size* — number of tokens;\n",
    " - *n_tags* — number of tags;\n",
    " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
    " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
    " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
    "\n",
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- 4 epochs;\n",
    "- starting value of *learning_rate*: 0.005\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n",
    "\n",
    "However, feel free to conduct more experiments to tune hyperparameters and earn extra points for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(n_tags=len(tag2idx), vocabulary_size=len(token2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 4\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = np.sqrt(2)\n",
    "dropout_keep_probability = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If got an error *\"Tensor conversion requested dtype float64 for Tensor with dtype float32\"* in this point, check if there are variables without dtype initialised. Set the value of dtype equals to *tf.float32* for such variables.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 68672 phrases; correct: 201.\n",
      "\n",
      "precision:  0.29%; recall:  4.48%; F1:  0.55\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 8303 phrases; correct: 22.\n",
      "\n",
      "precision:  0.26%; recall:  4.10%; F1:  0.50\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 3333 phrases; correct: 971.\n",
      "\n",
      "precision:  29.13%; recall:  21.63%; F1:  24.83\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 254 phrases; correct: 73.\n",
      "\n",
      "precision:  28.74%; recall:  13.59%; F1:  18.46\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4580 phrases; correct: 2868.\n",
      "\n",
      "precision:  62.62%; recall:  63.89%; F1:  63.25\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 346 phrases; correct: 171.\n",
      "\n",
      "precision:  49.42%; recall:  31.84%; F1:  38.73\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4797 phrases; correct: 3860.\n",
      "\n",
      "precision:  80.47%; recall:  85.99%; F1:  83.14\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 430 phrases; correct: 191.\n",
      "\n",
      "precision:  44.42%; recall:  35.57%; F1:  39.50\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-bb5f784bf4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Decaying the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-1aca3f5740ad>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_keep_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  self.lengths: lengths}\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see full quality reports for the final model on train, validation, and test sets. To give you a hint whether you have implemented everything correctly, you might expect F-score about 40% on the validation set.\n",
    "\n",
    "**The output of the cell below (as well as the output of all the other cells) should be present in the notebook for peer2peer review!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Could we say that our model is state of the art and the results are acceptable for the task? Definately, we can say so. Nowadays, Bi-LSTM is one of the state of the art approaches for solving NER problem and it outperforms other classical methods. Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definately small. However, the implemented model outperforms classical CRFs for this task. Even better results could be obtained by some combinations of several types of methods, e.g. see [this](https://arxiv.org/abs/1603.01354) paper if you are interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
