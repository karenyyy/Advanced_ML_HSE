{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTune with TF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "\n",
    "IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32)\n",
    "\n",
    "\n",
    "class ImageDataGenerator(object):\n",
    "    def __init__(self, txt_file, mode, batch_size, num_classes, shuffle=True,\n",
    "                 buffer_size=1000):\n",
    "        \n",
    "        self.txt_file = txt_file\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # retrieve the data from the text file\n",
    "        self._read_txt_file()\n",
    "\n",
    "        self.data_size = len(self.labels)\n",
    "\n",
    "        # initial shuffling of the file and label lists \n",
    "        if shuffle:\n",
    "            self._shuffle_lists()\n",
    "\n",
    "        # convert lists to TF tensor\n",
    "        self.img_paths = convert_to_tensor(self.img_paths, dtype=dtypes.string)\n",
    "        self.labels = convert_to_tensor(self.labels, dtype=dtypes.int32)\n",
    "\n",
    "        # create dataset\n",
    "        Dataset = tf.data.Dataset\n",
    "        data = Dataset.from_tensor_slices((self.img_paths, self.labels))\n",
    "\n",
    "        if mode == 'training':\n",
    "            data = data.map(self._parse_function_train)\n",
    "\n",
    "        elif mode == 'inference':\n",
    "            data = data.map(self._parse_function_inference)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode '%s'.\" % (mode))\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        data = data.batch(batch_size)\n",
    "        self.data = data\n",
    "\n",
    "    def _read_txt_file(self):\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        with open(self.txt_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                items = line.split(' ')\n",
    "                self.img_paths.append('/home/karen/Downloads/data/dogvscats/train/'+items[0])\n",
    "                self.labels.append(int(items[1]))\n",
    "\n",
    "    def _shuffle_lists(self):\n",
    "        path = self.img_paths\n",
    "        labels = self.labels\n",
    "        permutation = np.random.permutation(self.data_size)\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        for i in permutation:\n",
    "            self.img_paths.append(path[i])\n",
    "            self.labels.append(labels[i])\n",
    "\n",
    "    def _parse_function_train(self, filename, label):\n",
    "        one_hot = tf.one_hot(label, self.num_classes)\n",
    "        \n",
    "        # load and preprocess the image\n",
    "        img_string = tf.read_file(filename)\n",
    "        img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "        img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "\n",
    "        # RGB -> BGR\n",
    "        img_bgr = img_centered[:, :, ::-1]\n",
    "\n",
    "        return img_bgr, one_hot\n",
    "\n",
    "    def _parse_function_inference(self, filename, label):\n",
    "        # convert label number into one-hot-encoding\n",
    "        one_hot = tf.one_hot(label, self.num_classes)\n",
    "\n",
    "        # load and preprocess the image\n",
    "        img_string = tf.read_file(filename)\n",
    "        img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "        img_resized = tf.image.resize_images(img_decoded, [227, 227])\n",
    "        img_centered = tf.subtract(img_resized, IMAGENET_MEAN)\n",
    "\n",
    "        # RGB -> BGR\n",
    "        img_bgr = img_centered[:, :, ::-1]\n",
    "\n",
    "        return img_bgr, one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/karen/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from datagenerator import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class AlexNet(object):\n",
    "\n",
    "    def __init__(self, x, keep_prob, num_classes, skip_layer,\n",
    "                 weights_path='DEFAULT'):\n",
    "\n",
    "        # Parse input arguments into class variables\n",
    "        self.X = x\n",
    "        self.num_classes = num_classes\n",
    "        self.keep_prob = keep_prob\n",
    "        self.skip_layer = skip_layer\n",
    "\n",
    "        if weights_path == 'DEFAULT':\n",
    "            self.weights_path = '/home/karen/Downloads/data/bvlc_alexnet.npy'\n",
    "        else:\n",
    "            self.weights_path = weights_path\n",
    "\n",
    "        # Call the create function to build the computational graph of AlexNet\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"Create the network graph.\"\"\"\n",
    "        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "        norm1 = lrn(conv1, 2, 1e-04, 0.75, name='norm1')\n",
    "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "\n",
    "        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n",
    "        norm2 = lrn(conv2, 2, 1e-04, 0.75, name='norm2')\n",
    "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "\n",
    "        # 3rd Layer: Conv (w ReLu)\n",
    "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n",
    "\n",
    "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n",
    "\n",
    "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "\n",
    "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "        flattened = tf.reshape(pool5, [-1, 6 * 6 * 256])\n",
    "        fc6 = fc(flattened, 6 * 6 * 256, 4096, name='fc6')\n",
    "        dropout6 = dropout(fc6, self.keep_prob)\n",
    "\n",
    "        # 7th Layer: FC (w ReLu) -> Dropout\n",
    "        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n",
    "        dropout7 = dropout(fc7, self.keep_prob)\n",
    "\n",
    "        # 8th Layer: FC and return unscaled activations\n",
    "        self.fc8 = fc(dropout7, 4096, self.num_classes, relu=False, name='fc8')\n",
    "\n",
    "    def load_initial_weights(self, session):\n",
    "        # Load the weights into memory\n",
    "        weights_dict = np.load(self.weights_path, encoding='bytes').item()\n",
    "\n",
    "        # Loop over all layer names stored in the weights dict\n",
    "        for op_name in weights_dict:\n",
    "\n",
    "            # freeze weights if not skipped\n",
    "            if op_name not in self.skip_layer:\n",
    "\n",
    "                with tf.variable_scope(op_name, reuse=True):\n",
    "\n",
    "                    # Assign weights/biases to their corresponding tf variable\n",
    "                    for data in weights_dict[op_name]:\n",
    "\n",
    "                        # Biases\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "                        # Weights\n",
    "                        else:\n",
    "                            print('data shape: ', data.shape)\n",
    "                            var = tf.get_variable('weights', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n",
    "         padding='SAME', groups=1):\n",
    "\n",
    "    # Get number of input channels\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k,\n",
    "                                         strides=[1, stride_y, stride_x, 1],\n",
    "                                         padding=padding)\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "        weights = tf.get_variable('weights', shape=[filter_height,\n",
    "                                                    filter_width,\n",
    "                                                    input_channels / groups,\n",
    "                                                    num_filters])\n",
    "        biases = tf.get_variable('biases', shape=[num_filters])\n",
    "\n",
    "    if groups == 1:\n",
    "        conv = convolve(x, weights)\n",
    "\n",
    "    # In the cases of multiple groups, split inputs & weights and\n",
    "    else:\n",
    "        # Split input and weights and convolve them separately\n",
    "        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n",
    "        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n",
    "                                 value=weights)\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n",
    "\n",
    "        # Concat the convolved output together again\n",
    "        conv = tf.concat(axis=3, values=output_groups)\n",
    "\n",
    "    # Add biases\n",
    "    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n",
    "\n",
    "    # Apply relu function\n",
    "    relu = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    return relu\n",
    "\n",
    "\n",
    "def fc(x, num_in, num_out, name, relu=True):\n",
    "    \"\"\"Create a fully connected layer.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "        # Create tf variables for the weights and biases\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out],\n",
    "                                  trainable=True)\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "    if relu:\n",
    "        # Apply ReLu non linearity\n",
    "        relu = tf.nn.relu(act)\n",
    "        return relu\n",
    "    else:\n",
    "        return act\n",
    "\n",
    "\n",
    "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n",
    "             padding='SAME'):\n",
    "    \"\"\"Create a max pooling layer.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
    "                          strides=[1, stride_y, stride_x, 1],\n",
    "                          padding=padding, name=name)\n",
    "\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "    \"\"\"Create a local response normalization layer.\"\"\"\n",
    "    return tf.nn.local_response_normalization(x, depth_radius=radius,\n",
    "                                              alpha=alpha, beta=beta,\n",
    "                                              bias=bias, name=name)\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    \"\"\"Create a dropout layer.\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Path to the textfiles for the trainings and validation set\n",
    "    train_file = 'train.txt'\n",
    "    val_file = 'val.txt'\n",
    "\n",
    "    # Learning params\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "\n",
    "    # Network params\n",
    "    dropout_rate = 0.5\n",
    "    num_classes = 2\n",
    "    train_layers = ['fc8', 'fc7', 'fc6']\n",
    "\n",
    "    # How often we want to write the tf.summary data to disk\n",
    "    display_step = 20\n",
    "\n",
    "    # Path for tf.summary.FileWriter and to store model checkpoints\n",
    "    filewriter_path = \"tensorboard\"\n",
    "    checkpoint_path = \"checkpoints\"\n",
    "\n",
    "    \"\"\"\n",
    "    Main Part of the finetuning Script.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create parent path if it doesn't exist\n",
    "    if not os.path.isdir(checkpoint_path):\n",
    "        os.mkdir(checkpoint_path)\n",
    "\n",
    "    # Place data loading and preprocessing on the cpu\n",
    "    with tf.device('/cpu:0'):\n",
    "        tr_data = ImageDataGenerator(train_file,\n",
    "                                     mode='training',\n",
    "                                     batch_size=batch_size,\n",
    "                                     num_classes=num_classes,\n",
    "                                     shuffle=True)\n",
    "        val_data = ImageDataGenerator(val_file,\n",
    "                                      mode='inference',\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_classes=num_classes,\n",
    "                                      shuffle=False)\n",
    "\n",
    "        # create an reinitializable iterator given the dataset structure\n",
    "        Iterator = tf.data.Iterator\n",
    "        iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "        next_batch = iterator.get_next()\n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op = iterator.make_initializer(val_data.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Initialize model\n",
    "    model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "    # Link variable to model output\n",
    "    score = model.fc8\n",
    "\n",
    "    # List of trainable variables of the layers we want to train\n",
    "    var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score,\n",
    "                                                                      labels=y))\n",
    "\n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        # Get gradients of all trainable variables\n",
    "        gradients = tf.gradients(loss, var_list)\n",
    "        gradients = list(zip(gradients, var_list))\n",
    "\n",
    "        # Create optimizer and apply gradient descent to the trainable variables\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "    # Add gradients to summary\n",
    "    # for gradient, var in gradients:\n",
    "    #     tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "    # Add the variables we train to the summary\n",
    "    # for var in var_list:\n",
    "    #     tf.summary.histogram(var.name, var)\n",
    "\n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    # Evaluation op: Accuracy of the model\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
    "\n",
    "    # Start Tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Add the model graph to TensorBoard\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        # Load the pretrained weights into the non-trainable layer\n",
    "        model.load_initial_weights(sess)\n",
    "\n",
    "        print(\"{} Start training...\".format(datetime.now()))\n",
    "        print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),\n",
    "                                                          filewriter_path))\n",
    "\n",
    "        # Loop over number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            print(\"{} Epoch number: {}\".format(datetime.now(), epoch + 1))\n",
    "\n",
    "            # Initialize iterator with the training dataset\n",
    "            sess.run(training_init_op)\n",
    "\n",
    "            for step in range(train_batches_per_epoch):\n",
    "\n",
    "                # get next batch of data\n",
    "                img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "                # And run the training op\n",
    "                sess.run(train_op, feed_dict={x: img_batch,\n",
    "                                              y: label_batch,\n",
    "                                              keep_prob: dropout_rate})\n",
    "\n",
    "                # Generate summary with the current batch of data and write to file\n",
    "                if step % display_step == 0:\n",
    "                    s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                            y: label_batch,\n",
    "                                                            keep_prob: 1.})\n",
    "\n",
    "                    writer.add_summary(s, epoch * train_batches_per_epoch + step)\n",
    "\n",
    "            # Validate the model on the entire validation set\n",
    "            print(\"{} Start validation\".format(datetime.now()))\n",
    "            sess.run(validation_init_op)\n",
    "            test_acc = 0.\n",
    "            test_count = 0\n",
    "            for _ in range(val_batches_per_epoch):\n",
    "                img_batch, label_batch = sess.run(next_batch)\n",
    "                acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                    y: label_batch,\n",
    "                                                    keep_prob: 1.})\n",
    "                test_acc += acc\n",
    "                test_count += 1\n",
    "            test_acc /= test_count\n",
    "            print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                           test_acc))\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch' + str(epoch + 1) + '.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
