{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Motivation for recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/35.png)\n",
    "\n",
    "![](../../images/36.png)\n",
    "\n",
    "> Question 1\n",
    "\n",
    "How many weights are there in the first layer of the MLP? Do not forget about the bias vector!\n",
    "\n",
    "hidden neurons: 100\n",
    "\n",
    "window width: 100\n",
    "\n",
    "word embeddings size: 100\n",
    "\n",
    "\n",
    "- 10 100\n",
    "\n",
    "- 10 000\n",
    "\n",
    "- __1 000 100__\n",
    "\n",
    "__There are 100*100 inputs to the first layer of the MLP: 100 elements for each word in the window. Therefore the weight matrix contains 1M parameters. Additionally, the bias vector contains 100 parameters.__\n",
    "\n",
    "- 1 000 000\n",
    "\n",
    "\n",
    "![](../../images/37.png)\n",
    "\n",
    "![](../../images/38.png)\n",
    "\n",
    "\n",
    "> Question 2\n",
    "\n",
    "How many weights are there in the first layer of the MLP? Do not forget about the bias vector.\n",
    "\n",
    "hidden neurons: 100\n",
    "\n",
    "word embeddings size: 100\n",
    "\n",
    "we transfer hidden neurons of the first layer between time steps\n",
    "\n",
    "- __20 100__\n",
    "\n",
    "__There are 200 inputs to the first layer of the MLP: 100 from the current input and 100 from the previous hidden state. Therefore the weight matrix contains 200*100 parameters. Additionally, the bias vector contains 100 parameters.__\n",
    "\n",
    "- 10 100\n",
    "\n",
    "- 10 200\n",
    "\n",
    "- 20 200\n",
    "\n",
    "\n",
    "\n",
    "Takeaways:\n",
    "\n",
    "![](../../images/39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple RNN and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../../images/40.png)\n",
    "\n",
    "![](../../images/41.png)\n",
    "\n",
    "![](../../images/42.png)\n",
    "\n",
    "\n",
    "> Note:\n",
    "\n",
    "All weights are shared across time steps\n",
    "\n",
    "\n",
    "![](../../images/43.png)\n",
    "\n",
    "![](../../images/44.png)\n",
    "\n",
    "\n",
    "![](../../images/45.png)\n",
    "\n",
    "\n",
    "![](../../images/46.png)\n",
    "\n",
    "![](../../images/47.png)\n",
    "\n",
    "\n",
    "![](../../images/48.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The training of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### To train an RNN we need to backprop through layers and time\n",
    "\n",
    "\n",
    "![](../../images/49.png)\n",
    "\n",
    "\n",
    "__The more steps between the time moments k and t, the more elements are in this product__\n",
    "\n",
    "> Gradient Vanishing and Exploding !!\n",
    "\n",
    "- Values of these Jacobian matrices have particularly severe impact on the contribution from faraway steps\n",
    "\n",
    "![](../../images/50.png)\n",
    "\n",
    "![](../../images/51.png)\n",
    "\n",
    "![](../../images/52.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dealing with vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "### Gradient Exploding\n",
    "\n",
    "![](../../images/53.png)\n",
    "\n",
    "![](../../images/54.png)\n",
    "\n",
    "![](../../images/55.png)\n",
    "\n",
    "### Gradient Vanishing\n",
    "\n",
    "![](../../images/56.png)\n",
    "\n",
    "![](../../images/57.png)\n",
    "\n",
    "\n",
    "> How can we deal with gradient vanishing and exploding?\n",
    "\n",
    "- LSTM, GRU\n",
    "- RELU activation function\n",
    "- Initialization of the recurrent weight matrix\n",
    "- Skip connections\n",
    "\n",
    "\n",
    "\n",
    "#### Property of orthogonal matrix\n",
    "\n",
    "![](../../images/58.png)\n",
    "\n",
    "So no matter how many times we perform repeated matrix multiplication, the resulting matrix does not explode or vanish.\n",
    "\n",
    "\n",
    "- Initialize W with an orthogonal matrix\n",
    "- Use orthogonal W through the whole training\n",
    "\n",
    "----\n",
    "\n",
    "In this way the second part of the Jacobian doesn't cause the vanishing gradient problem, at least on the first iterations of the training.\n",
    "\n",
    "\n",
    "\n",
    "__In this case, the network has a chance to find long range dependencies in the data.__\n",
    "\n",
    "----\n",
    "\n",
    "There are some approaches that utilize the properties of orthogonal matrices, not just for a proper initialization, but also for the parameterization of the weights for the whole training process.\n",
    "\n",
    "----\n",
    "__Takeaways:__\n",
    "\n",
    "- Exploding gradients are easy to detect but it is not clear how to detect vanishing gradients\n",
    "- Exploding gradients: \n",
    "    - gradient clipping \n",
    "    - truncated BPTT\n",
    "- Vanishing gradients:\n",
    "    - ReLU nonlinearity\n",
    "    - orthogonal initialization of the recurrent weights\n",
    "    - skip connection\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern RNNs: LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../images/59.png)\n",
    "\n",
    "When we do a back-prop on such layer, __gradient needs to go through non-linearity__ and __for multiplication by weight matrix W__, both can cause `gradient vanishing`\n",
    "\n",
    "\n",
    "### So the main idea is to create a short way for the gradients result any non-linearities or multiplications.\n",
    "\n",
    "\n",
    "- All sorts of LSTM propose to do it by adding a new separately way through recurrent layer --> __Interal memory C__ (__which other layers on the network do not have access to__)\n",
    "\n",
    "\n",
    "- At each step of LSTM, we compute not only the vector of hidden units H but also a vector of memory c on the same dimension\n",
    "\n",
    "As the result, we have two ways through such layer:\n",
    "\n",
    "- one between $H_{t-1}$ and $H_t$\n",
    "- second between the memory cells $C_{t-1}$ and $C_t$\n",
    "\n",
    "![](../../images/60.png)\n",
    "\n",
    "\n",
    "\n",
    "![](../../images/61.png)\n",
    "\n",
    "\n",
    "![](../../images/62.png)\n",
    "\n",
    "![](../../images/63.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
