{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week 1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "7PZ1hid5AV4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear model for classfication"
      ]
    },
    {
      "metadata": {
        "id": "Qg1fzNNZ-bRH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Binary classification $(y \\in \\{-1, 1\\})$:\n",
        "\n",
        "$$a(x) = \\sigma (w^Tx)$$\n",
        "\n",
        "### multi classification $(y \\in \\{1, ..., K\\})$\n",
        "\n",
        "$$a(x) = \\arg \\max_{k \\in \\{1,...,K\\}}(w_k^Tx)$$\n",
        "\n",
        "\n",
        "\n",
        "### Classification loss\n",
        "\n",
        "#### Accuracy as loss:\n",
        "\n",
        "$$\\frac{1}{l} \\sum^l_{i=1} [a(x_i)=y_i]$$\n",
        "\n",
        "- not differentiable\n",
        "- don't access model confidence\n",
        "\n",
        "[P] : __Iverson bracket:__\n",
        "\n",
        "$$[p] = 1 \\text{ when P is true}; 0 \\text{ where P is false}$$\n",
        "\n",
        "\n",
        "\n",
        "Two major disadvantages:\n",
        "\n",
        "- Accuracy doesn'y have gradient to optimize loss function effectively, so can not learn the model through accuracy score;\n",
        "- Does not take into accounts of the confidence of the model in the prediction\n",
        "  - two ways of using the confidence information in evaluating how well the points were classified.\n",
        "    - Precision/recall \n",
        "    - the AUC/ROC score \n",
        "\n",
        "\n",
        "__it's known from machine learning that models with greater confidence generalize better__\n",
        "\n",
        "\n",
        "#### Logistic Regression\n",
        "\n",
        "$$z=(w^Tx, )$$\n",
        "\n",
        "$$\\downarrow$$\n",
        "\n",
        "\n",
        "$$(e^{z_1},...e^{z_k})$$\n",
        "\n",
        "$$\\downarrow$$\n",
        "\n",
        "$$\\sigma(z) = (\\frac{e^{z_1}}{\\sum^K_{k=1}e^{z_k}}, ..., \\frac{e^{z_K}}{\\sum^K_{k=1}e^{z_k}})$$\n",
        "\n",
        "\n",
        "Target values for class probabilities:\n",
        "\n",
        "$$p = ([y=1], ..., [y=k])$$\n",
        "\n",
        "\n",
        "\n",
        "__Similarity between z and p (measure the distance) can be measured by the cross-entropy__\n",
        "\n",
        "\n",
        "$$-\\sum^K_{k=1} [y=k] \\log  \\frac{e^{z_K}}{\\sum^K_{k=1}e^{z_k}} = - \\log  \\frac{e^{z_y}}{\\sum^K_{k=1}e^{z_k}}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note:\n",
        "\n",
        "Consider a 3-class problem and an example from 1st class. Which vector of predicted probabilities will get lower cross-entropy?\n",
        "\n",
        "- (1,0,0) __Correct__ \n",
        "\n",
        "- (0.5, 0.25, 0.25)\n",
        "\n",
        "- (0, 1, 0)\n",
        "\n",
        "\n",
        "\n",
        "Suppose K=3 and y=1:\n",
        "\n",
        "$$(-1)\\log(1)-(0)\\log(0)-(0)\\log(0) = 0$$\n",
        "\n",
        "- correct model is supposed to produce 0 cross-entropy\n",
        "- with the mode getting worse, cross-entropy gets larger\n",
        "\n",
        "\n",
        "$$(-1)\\log(0.5)-(0)\\log(0.25)-(0)\\log(0.25) \\approx 0.693$$\n",
        "$$(-1)\\log(0)-(0)\\log(1)-(0)\\log(0) = + \\infty$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6cy1eUy-3P09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "ufSry0YO4oja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimization problem\n",
        "\n",
        "$$L(w) \\rightarrow \\min_w$$\n",
        "\n",
        "$$w^0 \\rightarrow initialization$$\n",
        "\n",
        "\n",
        "$$\\bigtriangledown L(w^0) = (\\frac{\\partial L(w^0)}{\\partial w_1}, ...,\\frac{\\partial L(w^0)}{\\partial w_n}) \\rightarrow \\text{ gradient vector}$$\n",
        "\n",
        "\n",
        "- points in the direction of the steepest slope at $w^0$\n",
        "- the functijon has fastest decrease rate in the direction of negative\n",
        "gradient\n",
        "\n",
        "\n",
        "$$w^1 = w^0 - n_1 \\bigtriangledown L(w^0) \\text{ gradient step}$$\n",
        "\n",
        "$$\\text{while True:}$$\n",
        "\n",
        "$$w^t = w^{t-1} - n_t \\bigtriangledown L(w^{t-1}) $$\n",
        "\n",
        "$$\\text{ If } ||w^t - w^{t-1}|| < \\epsilon, \\text{ then break}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__Lots of heuristics:__\n",
        "\n",
        "- how to initialize $w^0$\n",
        "- how to select step size $n_t$\n",
        "- when to stop\n",
        "  - $||w^t - w^{t-1}|| < \\epsilon$\n",
        "  - __maxit__: predetermined maximum number of iterations\n",
        "  - stop when the function gets \"close enough\" to target\n",
        "  - stop when the improvement drops below a threshold\n",
        "- how to approximate gradient $\\bigtriangledown L(w^{t-1})$\n",
        "\n",
        "\n",
        "\n",
        "### Gradient descent for MSE\n",
        "\n",
        "#### Linear regression and MSE:\n",
        "\n",
        "$$L(w) = \\frac{1}{l} ||Xw-y||^2$$\n",
        "\n",
        "#### Derivatives:\n",
        "\n",
        "$$\\bigtriangledown L_w(w) = \\frac{2}{l} X^T(Xw-y)$$\n",
        "\n",
        "\n",
        "\n",
        "Analytical solution for MSE: \n",
        "\n",
        "$$w = (X^TX)^{-1}X^Ty$$\n",
        "\n",
        "__But it is inferior to GD because GD is:__\n",
        "\n",
        "- easy to implement\n",
        "- very general, can be applied to any differentiable loss function\n",
        "- require less memory and computations (for stochastic methods)\n",
        "\n",
        "\n",
        "__Note:__\n",
        "\n",
        "There are some cases when analytical solution for linear model and MSE loss can be effective. Select such cases.\n",
        "\n",
        "\n",
        "- Small size of training sample and large number of features\n",
        "\n",
        "\n",
        "- Large training sample and large number of features\n",
        "\n",
        "\n",
        "- Small number of features (about 100 or less)\n",
        "\n",
        "__Correct__\n",
        "\n",
        "__In such cases matrix $X^T X$ is not very large and can be easily inverted__\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "H6MrTYHwDLjS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gfXykZVmDQ61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Overfitting problem and model validation\n",
        "\n",
        "#### Cross-Validation\n",
        "\n",
        "- small testing set\n",
        "  - training set is representative\n",
        "  - testing set has high variance\n",
        "- large testing set\n",
        "  - testing set quality has low variance\n",
        "  - testing set has high bias\n",
        "  \n",
        "  \n",
        "\n",
        "Overfitted model will have large weights while good models actually have not very large weights to solve the problem of overfitting\n",
        "\n",
        "- Good model weights: (0.634, 0.918, -0.626)\n",
        "\n",
        "- Overfitted model weights: (130.0, -525.8, ..., 102.6)\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "### Weight penalty\n",
        "\n",
        "$$L_{reg}(w) = L(w) +\\lambda R(w) \\rightarrow \\min_w$$\n",
        "\n",
        "\n",
        "- $L(w)$: loss function (MSE, log-loss, etc)\n",
        "- $R(w)$: regularizer (e.g. penalizes large weights)\n",
        "- $\\lambda$: regularization strength\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "#### L2 penalty\n",
        "\n",
        "$$\\text{when } R(w) = ||w||^2, \\text{ where } ||w||^2 = \\sum^d_{j=1} w^2_j$$\n",
        "\n",
        "\n",
        "$$L_{reg}(w) = L(w) +\\lambda ||w||^2 \\rightarrow \\min_w$$\n",
        "  \n",
        "\n",
        "- Drives all weights __closer__ to zero\n",
        "- Can be optimized with gradient methods (convex problem, differentiable)\n",
        "\n",
        "\n",
        "\n",
        "#### L1 penalty\n",
        "\n",
        "$$\\text{when } R(w) = ||w||_1$$\n",
        "\n",
        "\n",
        "$$L_{reg}(w) = L(w) +\\lambda ||w||_1 \\rightarrow \\min_w$$\n",
        "\n",
        "\n",
        "\n",
        "- Drives some weights __exactly__ to zero\n",
        "- __Learn sparse models__\n",
        "- Cannot be optimized with simple gradient methods\n",
        "\n",
        "\n",
        "__Note:__\n",
        "\n",
        "Sparse models are really useful when the target value really depends only on some subset of features. But can they be used in the cases described below?\n",
        "\n",
        "\n",
        "- All features are useful, but we want to construct a small number of new features that contain all information from original features.\n",
        "\n",
        "\n",
        "\n",
        "- __All features are useful, but we should find some small subset of features most important for the problem.__\n",
        "\n",
        "\n",
        "#### Other regularization techniques\n",
        "\n",
        "- Dimensionality reduction\n",
        "  - remove redundant features \n",
        "  - PCA\n",
        "  - T-SNE\n",
        "- Data augmentation\n",
        "- Dropout\n",
        "- Early stopping\n",
        "- Collect more data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "YQCNsW6DLJlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stochastic Methods for Optimization\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h7sVDGxrLMAY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "\n",
        "Since SGD is on only one example, leads to very noisy approximations. \n",
        "\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/karenyyy/Advanced_ML_HSE/master/Introduction-To-Deep-Learning/images/1.png)\n",
        "\n",
        "\n",
        "\n",
        "- Noisy updates lead to fluctuations\n",
        "- Needs only one example on each step\n",
        "- Can be used in online setting\n",
        "- Reduces the variance of gradient approximations\n",
        "- Learning rate $n_t$ should be chosen very carefully\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__Notes on online learning__\n",
        "\n",
        "目的是正确预测数据的类别，并且在每次预测后，该结果用来更新修正预测模型，用于对以后数据进行更好的预测。\n",
        "\n",
        "\n",
        "对于一个序列进行一系列处理可以分为三步：\n",
        "\n",
        "- 算法获得一个训练实例；\n",
        "\n",
        "- 算法预测训练实例的类别；\n",
        "\n",
        "- 算法获得正确类别，并根据预测类别与正确类别更新模型假设。\n",
        "\n",
        "\n",
        "Online learning 很好的应用在训练整个数据集在计算上不可行的情况，和一些要求算法动态适应型模式的情况\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "对应的online learning 有两种通用的模型：\n",
        "\n",
        "- statistical learning 模型\n",
        "  - 数据样本是独立同分布的随机变量且不随时间变化，算法只是对数据的有限访问，即不是对整个数据集的计算\n",
        "- adversarial 模型\n",
        "  - 看作是两个玩家之间的游戏(学习者和数据生成器)，在这个模型中对手（数据生成器）能动态的适应该算法输出产生而产生变化，例如在垃圾邮件过滤中，垃圾邮件的产生者能基于过滤器的表现来生成新的垃圾邮件。\n",
        "\n",
        "____\n",
        "### Mini-batch gradient descent\n",
        "\n",
        "#### Optimization problem:\n",
        "\n",
        "$$L(w) = \\sum^l_{i=1} L(w;x_i, y_i) \\rightarrow \\min_w$$\n",
        "\n",
        "$$w_0 \\rightarrow \\text{ initialization }$$\n",
        "\n",
        "$$\\text{ while True: }$$\n",
        "\n",
        "$$i_1,...,i_m=\\text{ random indices between 1 and l}$$\n",
        "\n",
        "\n",
        "$$w^t = w^{t-1} -n_t\\frac{1}{m} \\sum^m_{j=1} \\bigtriangledown L(w^{t-1}; x_{ij};y_{ij})$$\n",
        "\n",
        "\n",
        "\n",
        "- Gradient descent is infeasible for large training sets\n",
        "- Stochastic and mini-batch descents use gradient approximations to speed up computations\n",
        "- Learning rate is hard to select\n",
        "- Methods can be optimized for difficult functions\n",
        "\n",
        "____\n",
        "\n",
        "### Momentum\n",
        "\n",
        "$$h_t = \\alpha h_{t-1} +n_tg_t$$\n",
        "\n",
        "__actually $h_t$is just a weighted sum of gradients from all previous iteration and from this iteration__\n",
        "\n",
        "\n",
        "\n",
        "$$w^t = w^{t-1} - h_t$$\n",
        "\n",
        "\n",
        "- Tends to move in the same direction as on previous steps\n",
        "- $h_t$ accumulates values along dimensions where gradients have the same sign\n",
        "- Usually $\\alpha = 0.9$\n",
        "\n",
        "____\n",
        "\n",
        "what's the point of this optimization method?\n",
        "\n",
        "- suppose that we have some function, it'll make gradient descent, and maybe for some coordinates of our parameter vector, gradients always have the same sign, so they lead us to the minimum\n",
        "\n",
        "- and for some coordinates, the sign of the gradient changes from iteration to iteration\n",
        "\n",
        "- so, vector $h_t$ would be large for component where gradients have the same sign on every iteration, and will make large steps by this coordinates\n",
        "\n",
        "- for coordinates that change sign, they will just cancel each other and $h_t$ will be close to zero\n",
        "\n",
        "\n",
        "- __So, $h_t$ cancels some coordinates that lead to  oscillation of gradients__\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "### Nesterov Momentum\n",
        "\n",
        "\n",
        "$$h_t = \\alpha h_{t-1} +n_t \\bigtriangledown L(w^{t-1} - \\alpha h_{t-1})$$\n",
        "\n",
        "\n",
        "$$w^t = w^{t-1} -h_t$$\n",
        "\n",
        "\n",
        "- momentum method and Nesterov momentum method work better with difficult functions with complex level sets, but they still require to choose learning rate, sine they're very sensitive to it\n",
        "\n",
        "- so we shall use some other optimization methods that choose learning rate adaptively, so we do not have to choose it by ourselves\n",
        "\n",
        "\n",
        "$$\\downarrow$$\n",
        "\n",
        "____\n",
        "\n",
        "### AdaGrad\n",
        "\n",
        "$$G_j^t = G_j^{t-1} +g_{tj}^2$$\n",
        "\n",
        "__essentially, G is a sum of squares of gradients from all previous iterations__\n",
        "\n",
        "$$w_j^t = w_j^{t-1} - \\frac{n_t}{\\sqrt{G^t_j+\\epsilon}}g_{tj}$$\n",
        "\n",
        "\n",
        "- $g_{tj}$: gradient with respect to j-th parameter\n",
        "- __separate learning rates for each dimension__\n",
        "  - __Example:__\n",
        "    - suppose that we are analyzing texts, and each feature from our sample corresponds to one word\n",
        "    - so for some frequent word that we see in every document, we have gradient updates on every step, and we'll make smaller steps.\n",
        "    - and for some rare words that we can met only in one of a thousand or 10 thousand documents, we'll make large updates, because they are rare, we don't need them very often, and we need to move faster in the direction of these words\n",
        "    \n",
        "- suits for sparse data\n",
        "- learning rate can be fixed: $n_t = 0.01$\n",
        "- $G^t_j$ always increases, leads to early stops\n",
        "\n",
        "\n",
        "__Disadvantage:__\n",
        "\n",
        "- the auxiliary parameter G, accumulates squares of gradient, and at certain step G might get too large, dividing learning rate by G will end up with 0, stopping weight updating\n",
        "\n",
        "- so we need some other better methods than AdaGrad\n",
        "\n",
        "$$\\downarrow$$\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "\n",
        "### RMSprop\n",
        "\n",
        "$$g_{tj} = \\frac{1}{m} \\sum^m_{j=1} \\bigtriangledown L(w^{t-1}; x_{ij};y_{ij})$$\n",
        "\n",
        "$$G_j^t = \\alpha G_j^{t-1} + (1-\\alpha)g_{tj}^2$$\n",
        "\n",
        "$$w_j^t = w_j^{t-1} - \\frac{n_t}{\\sqrt{G_j^t+\\epsilon}}g_{tj}$$\n",
        "\n",
        "\n",
        "- $\\alpha$ is about 0.9\n",
        "- learning rate adapts to latest gradient steps\n",
        "\n",
        "\n",
        "$$\\text{if we take RMSprop and slightly modify it: } \\downarrow$$\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "### Adam\n",
        "\n",
        "\n",
        "\n",
        "$$m_j^t = \\frac{\\beta_1 m_j^{t-1} + (1-\\beta_1)g_{tj}}{1-\\beta^t_1}$$\n",
        "\n",
        "$$v_j^t = \\frac{\\beta_2 v_j^{t-1} + (1-\\beta_2)g_{tj}^2}{1-\\beta^t_2}$$\n",
        "\n",
        "$$w_j^t = w_j^{t-1} - \\frac{n_t}{\\sqrt{v^t_j}+\\epsilon}m_{j}^t$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- combines momentum and individual learning rates\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pjhAEkrPBsCt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}